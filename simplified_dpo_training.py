#!/usr/bin/env python3
"""
ç°¡æ˜“ç‰ˆ TinySwallow DPO Training
APIå•é¡Œã‚’å›é¿ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…
"""

import os
import json
import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments
)
from datasets import Dataset
from trl import DPOTrainer
import warnings
warnings.filterwarnings('ignore')

# ç’°å¢ƒè¨­å®š
os.environ["WANDB_DISABLED"] = "true"

def main():
    print("ğŸ¦† ç°¡æ˜“ç‰ˆ TinySwallow DPO Training")
    print("=" * 50)
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"ğŸ”§ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
    print("ğŸ“¥ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    model_name = "SakanaAI/TinySwallow-1.5B-Instruct"
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32,  # MPSç”¨
        trust_remote_code=True
    )
    
    if device == "mps":
        model = model.to(device)
    
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_name}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­...")
    dataset_list = []
    
    with open('data/dpo_dataset.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line.strip())
            dataset_list.append({
                'prompt': data['prompt'],
                'chosen': data['chosen'],
                'rejected': data['rejected']
            })
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    dataset = Dataset.from_list(dataset_list)
    train_test = dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = train_test['train']
    eval_dataset = train_test['test']
    
    print(f"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    print(f"ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(eval_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    print("âš™ï¸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š...")
    training_args = TrainingArguments(
        output_dir="./outputs/simple_dpo",
        num_train_epochs=1,
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=4,
        learning_rate=5e-7,
        warmup_steps=20,
        max_steps=100,  # çŸ­ç¸®ãƒ†ã‚¹ãƒˆ
        eval_steps=25,
        save_steps=50,
        logging_steps=5,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to=[],
        remove_unused_columns=False,
        dataloader_pin_memory=False,
        fp16=False,  # MPSç’°å¢ƒã§ã¯ç„¡åŠ¹
    )
    
    # DPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
    print("ğŸ¯ DPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–...")
    try:
        dpo_trainer = DPOTrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            beta=0.1,
            max_length=512,
            max_prompt_length=256,
        )
        print("âœ… DPOTraineråˆæœŸåŒ–æˆåŠŸ")
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        print("ğŸš€ DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
        train_result = dpo_trainer.train()
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
        dpo_trainer.save_model("./outputs/simple_dpo/final_model")
        tokenizer.save_pretrained("./outputs/simple_dpo/final_model")
        
        print("âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
        print(f"æœ€çµ‚loss: {train_result.training_loss:.4f}")
        
        # ãƒ†ã‚¹ãƒˆ
        print("\nğŸ§ª ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ")
        test_prompts = [
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®åˆ©ç‚¹ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
            "DMPã¨CDPã®é•ã„ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
        ]
        
        for prompt in test_prompts:
            print(f"\nğŸ” ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
            inputs = tokenizer.encode(prompt, return_tensors="pt")
            if device == "mps":
                inputs = inputs.to(device)
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_new_tokens=50,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response[len(prompt):].strip()
            print(f"ğŸ“ å›ç­”: {response}")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        print("åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
        
        # åŸºæœ¬ã®Trainerä½¿ç”¨
        from transformers import Trainer, DataCollatorForLanguageModeling
        
        def preprocess_function(examples):
            texts = [f"{prompt} {chosen}" for prompt, chosen in zip(examples['prompt'], examples['chosen'])]
            return tokenizer(texts, truncation=True, padding=True, max_length=512)
        
        tokenized_train = train_dataset.map(preprocess_function, batched=True)
        tokenized_eval = eval_dataset.map(preprocess_function, batched=True)
        
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_train,
            eval_dataset=tokenized_eval,
            data_collator=data_collator,
        )
        
        print("ğŸš€ åŸºæœ¬ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
        trainer.train()
        trainer.save_model("./outputs/simple_dpo/fallback_model")
        print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")

if __name__ == "__main__":
    main()
