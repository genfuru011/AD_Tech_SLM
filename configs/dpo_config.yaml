# DPO Training Configuration
# Optimized for MacBook Air M2 8GB

# Model settings
model_name: "gpt2"  # Basic GPT-2 for testing (no _lzma required)
max_length: 512  # Smaller for GPT-2
max_prompt_length: 256

# Training parameters
num_train_epochs: 3
per_device_train_batch_size: 1  # Small batch size for 8GB memory
per_device_eval_batch_size: 1
gradient_accumulation_steps: 4  # Effective batch size = 4
learning_rate: 5e-6
warmup_steps: 50
weight_decay: 0.01
beta: 0.1  # DPO beta parameter

# LoRA settings (optimized for GPT-2 architecture)
use_lora: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1
target_modules: ["c_attn", "c_proj", "c_fc"]

# Hardware settings
device: "mps"  # Metal Performance Shaders for M2
fp16: false  # Disabled for MPS compatibility
dataloader_num_workers: 2

# Logging and saving
logging_steps: 10
save_steps: 100
eval_steps: 100
save_total_limit: 3
output_dir: "./outputs"
logging_dir: "./outputs/logs"
report_to: []  # Disable tensorboard for now

# Dataset settings
dataset_path: "./data/dpo_dataset.jsonl"  # Updated to use actual dataset
train_split: 0.8
validation_split: 0.2

# Evaluation settings
eval_strategy: "steps"
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: false

# Optimization settings for M2
gradient_checkpointing: true
remove_unused_columns: true
dataloader_pin_memory: false  # Disable for MPS