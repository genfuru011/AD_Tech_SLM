{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 🦆 TinySwallow DPO Training - Improved Version\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/genfuru011/AD_Tech_SLM/blob/main/notebooks/colab_dpo_training_improved.ipynb)\n",
    "\n",
    "**Direct Preference Optimization (DPO) Training for Japanese Language Model**\n",
    "\n",
    "このノートブックは、SakanaAI/TinySwallow-1.5B-Instructモデルに対して広告技術分野のDPOトレーニングを実行します。\n",
    "\n",
    "## 🎯 主な特徴\n",
    "- **最新TRL 0.18.1対応**: API変更に対応した堅牢な実装\n",
    "- **エラーハンドリング**: 複数のフォールバックオプション\n",
    "- **Colab最適化**: GPU環境での効率的な学習\n",
    "- **包括的なドキュメント**: 公式ドキュメントとベストプラクティス\n",
    "\n",
    "## 📚 参考資料\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl/)\n",
    "- [DPO Paper](https://arxiv.org/abs/2305.18290)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 🛠️ 環境セットアップ\n",
    "\n",
    "### GPU確認とランタイム情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU情報の確認\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "print(\"🔧 システム情報:\")\n",
    "print(f\"  Python: {sys.version}\")\n",
    "print(f\"  Platform: {platform.platform()}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"  GPU Count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"  ⚠️ GPU not available - training will be slow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-header",
   "metadata": {},
   "source": [
    "### 依存関係のインストール\n",
    "\n",
    "最新の安定版ライブラリをインストールします。TRL 0.18.1の変更に対応済み。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install -q torch>=2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers>=4.36.0\n",
    "!pip install -q trl>=0.7.0\n",
    "!pip install -q datasets>=2.14.0\n",
    "!pip install -q peft>=0.6.0\n",
    "!pip install -q bitsandbytes>=0.41.0\n",
    "!pip install -q accelerate>=0.24.0\n",
    "!pip install -q wandb\n",
    "!pip install -q tensorboard\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "\n",
    "# Colab特有の問題解決用\n",
    "!pip install -q tf-keras\n",
    "\n",
    "print(\"✅ 依存関係のインストール完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-header",
   "metadata": {},
   "source": [
    "### ライブラリのインポートと設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Transformers & TRL\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        BitsAndBytesConfig,\n",
    "        TrainingArguments\n",
    "    )\n",
    "    from trl import DPOTrainer, DPOConfig\n",
    "    from datasets import Dataset\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "    import accelerate\n",
    "    import transformers\n",
    "    import trl\n",
    "    print(\"✅ 主要ライブラリのインポート成功\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ インポートエラー: {e}\")\n",
    "    print(\"上記のpip installコマンドを再実行してください\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# 設定\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# バージョン情報表示\n",
    "print(\"📦 ライブラリバージョン:\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  Transformers: {transformers.__version__}\")\n",
    "print(f\"  TRL: {trl.__version__}\")\n",
    "print(f\"  Accelerate: {accelerate.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## ⚙️ トレーニング設定\n",
    "\n",
    "Google Colab環境に最適化された設定を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング設定\n",
    "CONFIG = {\n",
    "    # モデル設定\n",
    "    \"model_name\": \"SakanaAI/TinySwallow-1.5B-Instruct\",\n",
    "    \"backup_models\": [\n",
    "        \"tokyotech-llm/Swallow-1.5b-instruct-hf\",\n",
    "        \"rinna/japanese-gpt-neox-3.6b-instruction-sft\"\n",
    "    ],\n",
    "    \n",
    "    # トレーニング設定（Colab GPU最適化）\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"max_steps\": 500,\n",
    "    \"per_device_train_batch_size\": 1,  # メモリ使用量を抑制\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,  # 実質的なバッチサイズ = 8\n",
    "    \"learning_rate\": 5e-7,\n",
    "    \"warmup_steps\": 50,\n",
    "    \"eval_steps\": 50,\n",
    "    \"save_steps\": 100,\n",
    "    \"logging_steps\": 10,\n",
    "    \n",
    "    # DPO設定\n",
    "    \"beta\": 0.1,\n",
    "    \"max_length\": 1024,\n",
    "    \"max_prompt_length\": 512,\n",
    "    \n",
    "    # LoRA設定（メモリ効率化）\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    \n",
    "    # 量子化設定\n",
    "    \"use_4bit\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"use_nested_quant\": False,\n",
    "    \n",
    "    # データセット設定\n",
    "    \"test_size\": 0.1,\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # 出力設定\n",
    "    \"output_dir\": \"./outputs/colab_dpo_improved\",\n",
    "    \"final_output_dir\": \"./final_model\",\n",
    "}\n",
    "\n",
    "print(\"✅ 設定完了\")\n",
    "print(f\"📋 使用モデル: {CONFIG['model_name']}\")\n",
    "print(f\"🎯 最大ステップ数: {CONFIG['max_steps']}\")\n",
    "print(f\"📊 実質バッチサイズ: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"🔥 学習率: {CONFIG['learning_rate']}\")\n",
    "print(f\"⚡ DPO Beta: {CONFIG['beta']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## 📊 データセットの準備\n",
    "\n",
    "DPOトレーニング用のデータセットを準備します。複数の方法でデータを取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットのアップロード（方法1: ファイル直接アップロード）\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# サンプルデータの準備\n",
    "sample_dpo_data = [\n",
    "    {\n",
    "        \"prompt\": \"プログラマティック広告のRTBについて説明してください。\",\n",
    "        \"chosen\": \"RTB（Real-Time Bidding）は、広告枠の売買をリアルタイムのオークション形式で行う仕組みです。ユーザーがWebページにアクセスした瞬間に、そのユーザーの属性や閲覧履歴に基づいて、広告主が自動的に入札を行います。最も高い金額を提示した広告主の広告が表示される仕組みで、効率的なターゲティングと費用対効果の向上を実現します。\",\n",
    "        \"rejected\": \"RTBは広告を表示するシステムです。\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"DSPとSSPの違いを教えてください。\",\n",
    "        \"chosen\": \"DSP（Demand-Side Platform）は広告主側のプラットフォームで、広告枠の購入を自動化し、ターゲティングや入札戦略の最適化を行います。一方、SSP（Supply-Side Platform）はメディア側のプラットフォームで、広告枠の販売を自動化し、収益の最大化を図ります。DSPは買い手、SSPは売り手の立場でプログラマティック広告の取引を支援します。\",\n",
    "        \"rejected\": \"DSPとSSPは両方とも広告関連のシステムです。\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"アトリビューション分析の重要性について教えてください。\",\n",
    "        \"chosen\": \"アトリビューション分析は、コンバージョンに至るまでの各タッチポイントの貢献度を測定する分析手法です。ユーザーの購買行動は複雑で、複数の広告接触を経てコンバージョンに至るため、ラストクリック以外の接触点の価値も適切に評価する必要があります。これにより、マーケティング予算の最適配分、チャネル間の相互作用の理解、ROIの正確な測定が可能になります。\",\n",
    "        \"rejected\": \"アトリビューション分析は広告の効果を測定する方法です。\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"📁 DPOデータセットをアップロードしてください\")\n",
    "print(\"📋 期待される形式（JSONL）:\")\n",
    "print('{\"prompt\": \"質問\", \"chosen\": \"良い回答\", \"rejected\": \"悪い回答\"}')\n",
    "print()\n",
    "print(\"📎 ファイルがない場合は、サンプルデータセットを使用します\")\n",
    "\n",
    "try:\n",
    "    uploaded = files.upload()\n",
    "    dataset_file = None\n",
    "    \n",
    "    for filename in uploaded.keys():\n",
    "        if filename.endswith('.jsonl') or filename.endswith('.json'):\n",
    "            dataset_file = filename\n",
    "            print(f\"✅ データセットファイル検出: {dataset_file}\")\n",
    "            break\n",
    "    \n",
    "    if dataset_file is None:\n",
    "        print(\"❌ 適切なファイルが見つかりません\")\n",
    "        use_sample_data = True\n",
    "    else:\n",
    "        use_sample_data = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"📂 ファイルアップロードをスキップ: {e}\")\n",
    "    use_sample_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの読み込みと前処理\n",
    "def load_dpo_dataset(use_sample: bool = False, dataset_file: str = None) -> Dataset:\n",
    "    \"\"\"DPOデータセットを読み込み、前処理を行う\"\"\"\n",
    "    dataset_list = []\n",
    "    \n",
    "    if use_sample:\n",
    "        print(\"📊 サンプルデータセットを使用\")\n",
    "        dataset_list = sample_dpo_data\n",
    "    else:\n",
    "        print(f\"📊 アップロードされたファイルを読み込み: {dataset_file}\")\n",
    "        try:\n",
    "            # JSONLファイルの読み込み\n",
    "            if dataset_file.endswith('.jsonl'):\n",
    "                content = uploaded[dataset_file].decode('utf-8')\n",
    "                for line_num, line in enumerate(content.strip().split('\\n'), 1):\n",
    "                    if line.strip():\n",
    "                        try:\n",
    "                            data = json.loads(line)\n",
    "                            required_keys = ['prompt', 'chosen', 'rejected']\n",
    "                            if all(key in data for key in required_keys):\n",
    "                                dataset_list.append({\n",
    "                                    'prompt': data['prompt'],\n",
    "                                    'chosen': data['chosen'],\n",
    "                                    'rejected': data['rejected']\n",
    "                                })\n",
    "                            else:\n",
    "                                print(f\"⚠️ Line {line_num}: 必要なキーが不足\")\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"⚠️ Line {line_num}: JSON形式エラー - {e}\")\n",
    "            \n",
    "            # JSONファイルの読み込み\n",
    "            elif dataset_file.endswith('.json'):\n",
    "                content = uploaded[dataset_file].decode('utf-8')\n",
    "                data = json.loads(content)\n",
    "                if isinstance(data, list):\n",
    "                    dataset_list = data\n",
    "                else:\n",
    "                    print(\"❌ JSONファイルはリスト形式である必要があります\")\n",
    "                    dataset_list = sample_dpo_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ ファイル読み込みエラー: {e}\")\n",
    "            print(\"📊 サンプルデータセットにフォールバック\")\n",
    "            dataset_list = sample_dpo_data\n",
    "    \n",
    "    if not dataset_list:\n",
    "        print(\"❌ 有効なデータが見つかりません。サンプルデータを使用します。\")\n",
    "        dataset_list = sample_dpo_data\n",
    "    \n",
    "    print(f\"📊 データセット読み込み完了: {len(dataset_list)} サンプル\")\n",
    "    \n",
    "    # データセットの統計情報\n",
    "    if dataset_list:\n",
    "        avg_prompt_len = sum(len(item['prompt']) for item in dataset_list) / len(dataset_list)\n",
    "        avg_chosen_len = sum(len(item['chosen']) for item in dataset_list) / len(dataset_list)\n",
    "        avg_rejected_len = sum(len(item['rejected']) for item in dataset_list) / len(dataset_list)\n",
    "        \n",
    "        print(f\"📋 データセット統計:\")\n",
    "        print(f\"  平均プロンプト長: {avg_prompt_len:.1f} 文字\")\n",
    "        print(f\"  平均選択回答長: {avg_chosen_len:.1f} 文字\")\n",
    "        print(f\"  平均拒否回答長: {avg_rejected_len:.1f} 文字\")\n",
    "    \n",
    "    return Dataset.from_list(dataset_list)\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset = load_dpo_dataset(use_sample=use_sample_data, dataset_file=dataset_file if not use_sample_data else None)\n",
    "\n",
    "# 訓練・検証データの分割\n",
    "train_test = dataset.train_test_split(test_size=CONFIG['test_size'], seed=CONFIG['seed'])\n",
    "train_dataset = train_test['train']\n",
    "eval_dataset = train_test['test']\n",
    "\n",
    "print(f\"\\n📊 データ分割結果:\")\n",
    "print(f\"  訓練データ: {len(train_dataset)} サンプル\")\n",
    "print(f\"  検証データ: {len(eval_dataset)} サンプル\")\n",
    "\n",
    "# データサンプルの表示\n",
    "print(f\"\\n📝 データサンプル:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"  プロンプト: {sample['prompt'][:100]}...\")\n",
    "print(f\"  選択回答: {sample['chosen'][:100]}...\")\n",
    "print(f\"  拒否回答: {sample['rejected'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 🤖 モデルとトークナイザーの読み込み\n",
    "\n",
    "TinySwallowモデルを読み込み、量子化とLoRAを適用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(config: Dict) -> tuple:\n",
    "    \"\"\"モデルとトークナイザーを読み込む（エラーハンドリング付き）\"\"\"\n",
    "    \n",
    "    # 量子化設定\n",
    "    if config['use_4bit']:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=getattr(torch, config['bnb_4bit_compute_dtype']),\n",
    "            bnb_4bit_quant_type=config['bnb_4bit_quant_type'],\n",
    "            bnb_4bit_use_double_quant=config['use_nested_quant'],\n",
    "        )\n",
    "        print(\"⚡ 4bit量子化を有効化\")\n",
    "    else:\n",
    "        bnb_config = None\n",
    "    \n",
    "    # モデルの読み込み（フォールバック付き）\n",
    "    models_to_try = [config['model_name']] + config['backup_models']\n",
    "    \n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    used_model = None\n",
    "    \n",
    "    for model_name in models_to_try:\n",
    "        try:\n",
    "            print(f\"📥 モデル読み込み試行: {model_name}\")\n",
    "            \n",
    "            # トークナイザーの読み込み\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # パディングトークンの設定\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                print(\"🔧 パディングトークンを設定\")\n",
    "            \n",
    "            # モデルの読み込み\n",
    "            model_kwargs = {\n",
    "                'trust_remote_code': True,\n",
    "                'torch_dtype': torch.float16,\n",
    "                'device_map': 'auto',\n",
    "            }\n",
    "            \n",
    "            if bnb_config is not None:\n",
    "                model_kwargs['quantization_config'] = bnb_config\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "            used_model = model_name\n",
    "            print(f\"✅ モデル読み込み成功: {model_name}\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {model_name} の読み込み失敗: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if model is None:\n",
    "        raise RuntimeError(\"すべてのモデルの読み込みに失敗しました\")\n",
    "    \n",
    "    # モデル情報の表示\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\n📊 モデル情報:\")\n",
    "    print(f\"  使用モデル: {used_model}\")\n",
    "    print(f\"  総パラメータ数: {total_params:,}\")\n",
    "    print(f\"  学習可能パラメータ数: {trainable_params:,}\")\n",
    "    \n",
    "    return model, tokenizer, used_model\n",
    "\n",
    "# モデルとトークナイザーの読み込み\n",
    "model, tokenizer, used_model_name = load_model_and_tokenizer(CONFIG)\n",
    "\n",
    "# GPU使用量の確認\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    memory_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"\\n💾 GPU メモリ使用量:\")\n",
    "    print(f\"  使用中: {memory_allocated:.2f} GB\")\n",
    "    print(f\"  予約済み: {memory_reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lora-header",
   "metadata": {},
   "source": [
    "### LoRA設定の適用\n",
    "\n",
    "Parameter Efficient Fine-tuning (PEFT) でメモリ効率的な学習を実現します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lora-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA設定\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG['lora_r'],\n",
    "    lora_alpha=CONFIG['lora_alpha'],\n",
    "    target_modules=CONFIG['target_modules'],\n",
    "    lora_dropout=CONFIG['lora_dropout'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"🔧 LoRA設定:\")\n",
    "print(f\"  ランク (r): {CONFIG['lora_r']}\")\n",
    "print(f\"  アルファ: {CONFIG['lora_alpha']}\")\n",
    "print(f\"  ドロップアウト: {CONFIG['lora_dropout']}\")\n",
    "print(f\"  対象モジュール: {CONFIG['target_modules']}\")\n",
    "\n",
    "# LoRAの適用\n",
    "if CONFIG['use_4bit']:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    print(\"⚡ 4bit学習用の準備完了\")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"✅ LoRA適用完了\")\n",
    "\n",
    "# 学習可能パラメータ数の確認\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 更新されたGPU使用量\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    memory_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"\\n💾 GPU メモリ使用量（LoRA適用後）:\")\n",
    "    print(f\"  使用中: {memory_allocated:.2f} GB\")\n",
    "    print(f\"  予約済み: {memory_reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "## 🚀 DPOトレーニングの実行\n",
    "\n",
    "最新のTRL APIに対応した堅牢なDPOトレーニングを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力ディレクトリの作成\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['final_output_dir'], exist_ok=True)\n",
    "\n",
    "# DPOConfig設定（TRL 0.18.1対応）\n",
    "try:\n",
    "    # 新しいAPI (TRL >= 0.8.0)\n",
    "    training_args = DPOConfig(\n",
    "        output_dir=CONFIG['output_dir'],\n",
    "        num_train_epochs=CONFIG['num_train_epochs'],\n",
    "        max_steps=CONFIG['max_steps'],\n",
    "        per_device_train_batch_size=CONFIG['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=CONFIG['per_device_eval_batch_size'],\n",
    "        gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        warmup_steps=CONFIG['warmup_steps'],\n",
    "        eval_steps=CONFIG['eval_steps'],\n",
    "        save_steps=CONFIG['save_steps'],\n",
    "        logging_steps=CONFIG['logging_steps'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=[],  # WandBを無効化\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        gradient_checkpointing=True,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        max_prompt_length=CONFIG['max_prompt_length'],\n",
    "        beta=CONFIG['beta'],\n",
    "    )\n",
    "    print(\"✅ DPOConfig設定完了（新API）\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 新API失敗、レガシーAPIを試行: {e}\")\n",
    "    # レガシーAPI (TRL < 0.8.0)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=CONFIG['output_dir'],\n",
    "        num_train_epochs=CONFIG['num_train_epochs'],\n",
    "        max_steps=CONFIG['max_steps'],\n",
    "        per_device_train_batch_size=CONFIG['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=CONFIG['per_device_eval_batch_size'],\n",
    "        gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        warmup_steps=CONFIG['warmup_steps'],\n",
    "        eval_steps=CONFIG['eval_steps'],\n",
    "        save_steps=CONFIG['save_steps'],\n",
    "        logging_steps=CONFIG['logging_steps'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=[],\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        gradient_checkpointing=True,\n",
    "    )\n",
    "    print(\"✅ TrainingArguments設定完了（レガシーAPI）\")\n",
    "\n",
    "print(f\"\\n📋 トレーニング設定:\")\n",
    "print(f\"  最大ステップ数: {CONFIG['max_steps']}\")\n",
    "print(f\"  学習率: {CONFIG['learning_rate']}\")\n",
    "print(f\"  DPO Beta: {CONFIG['beta']}\")\n",
    "print(f\"  バッチサイズ: {CONFIG['per_device_train_batch_size']}\")\n",
    "print(f\"  勾配蓄積: {CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPOTrainer の初期化（TRL API変更対応）\n",
    "try:\n",
    "    # 新API (TRL >= 0.8.0)\n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=tokenizer,  # 新API\n",
    "    )\n",
    "    print(\"✅ DPOTrainer初期化完了（新API）\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 新API失敗、レガシーAPIを試行: {e}\")\n",
    "    try:\n",
    "        # レガシーAPI対応\n",
    "        trainer = DPOTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,  # レガシーパラメータ\n",
    "            beta=CONFIG['beta'],\n",
    "            max_length=CONFIG['max_length'],\n",
    "            max_prompt_length=CONFIG['max_prompt_length'],\n",
    "        )\n",
    "        print(\"✅ DPOTrainer初期化完了（レガシーAPI）\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"❌ すべてのAPI初期化に失敗: {e2}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\n🚀 DPOトレーニング準備完了\")\n",
    "print(f\"📊 訓練データ: {len(train_dataset)} サンプル\")\n",
    "print(f\"📊 検証データ: {len(eval_dataset)} サンプル\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング実行\n",
    "print(\"🚀 DPOトレーニング開始...\")\n",
    "print(f\"⏰ 開始時刻: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "try:\n",
    "    # メモリクリア\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # トレーニング実行\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n🎉 トレーニング完了！\")\n",
    "    print(f\"⏰ 終了時刻: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"📊 最終訓練ロス: {train_result.training_loss:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ トレーニングエラー: {e}\")\n",
    "    # エラー詳細をログ出力\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-header",
   "metadata": {},
   "source": [
    "## 📊 評価と可視化\n",
    "\n",
    "トレーニング結果の評価と可視化を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終評価\n",
    "try:\n",
    "    print(\"📊 最終評価実行中...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(\"\\n📊 最終評価結果:\")\n",
    "    for key, value in eval_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 評価エラー: {e}\")\n",
    "    eval_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング履歴の可視化\n",
    "try:\n",
    "    # ログ履歴の取得\n",
    "    log_history = trainer.state.log_history\n",
    "    \n",
    "    if log_history:\n",
    "        # 訓練ロスの抽出\n",
    "        train_loss = []\n",
    "        train_steps = []\n",
    "        eval_loss = []\n",
    "        eval_steps = []\n",
    "        \n",
    "        for log in log_history:\n",
    "            if 'loss' in log:\n",
    "                train_loss.append(log['loss'])\n",
    "                train_steps.append(log['step'])\n",
    "            if 'eval_loss' in log:\n",
    "                eval_loss.append(log['eval_loss'])\n",
    "                eval_steps.append(log['step'])\n",
    "        \n",
    "        # グラフの作成\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 訓練ロス\n",
    "        plt.subplot(1, 2, 1)\n",
    "        if train_loss:\n",
    "            plt.plot(train_steps, train_loss, 'b-', label='Training Loss')\n",
    "        if eval_loss:\n",
    "            plt.plot(eval_steps, eval_loss, 'r-', label='Validation Loss')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Progress')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # 学習率\n",
    "        plt.subplot(1, 2, 2)\n",
    "        learning_rates = [log.get('learning_rate', 0) for log in log_history if 'learning_rate' in log]\n",
    "        lr_steps = [log['step'] for log in log_history if 'learning_rate' in log]\n",
    "        if learning_rates:\n",
    "            plt.plot(lr_steps, learning_rates, 'g-')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"✅ トレーニング履歴の可視化完了\")\n",
    "    else:\n",
    "        print(\"⚠️ ログ履歴が見つかりません\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 可視化エラー: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-header",
   "metadata": {},
   "source": [
    "## 🧪 学習済みモデルのテスト\n",
    "\n",
    "トレーニング済みモデルの性能をテストします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用プロンプト\n",
    "test_prompts = [\n",
    "    \"プログラマティック広告の仕組みについて詳しく説明してください。\",\n",
    "    \"DSPの主な機能と利点を教えてください。\",\n",
    "    \"クッキーレス時代のターゲティング手法について説明してください。\",\n",
    "    \"アトリビューション分析がなぜ重要なのか説明してください。\"\n",
    "]\n",
    "\n",
    "print(\"🧪 学習済みモデルのテスト開始\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# モデルを評価モードに設定\n",
    "model.eval()\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    try:\n",
    "        print(f\"\\n📝 テスト {i}/4:\")\n",
    "        print(f\"質問: {prompt}\")\n",
    "        \n",
    "        # トークナイズ\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # GPU使用時はデバイス移動\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        # 推論実行\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # 生成結果のデコード\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"回答: {response}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ テスト {i} でエラー: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 モデルテスト完了！\")\n",
    "print(\"\\n💡 評価のポイント:\")\n",
    "print(\"   • 元のモデルと比較して応答品質が向上しているか\")\n",
    "print(\"   • 技術用語が適切に使用されているか\")\n",
    "print(\"   • 回答の正確性と一貫性\")\n",
    "print(\"   • ドメイン固有の知識が反映されているか\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 💾 モデルの保存\n",
    "\n",
    "学習済みモデルを保存し、ダウンロード可能な形式で準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "print(\"💾 モデルを保存中...\")\n",
    "\n",
    "try:\n",
    "    # 最終的なモデルを保存\n",
    "    trainer.save_model(CONFIG['final_output_dir'])\n",
    "    tokenizer.save_pretrained(CONFIG['final_output_dir'])\n",
    "    \n",
    "    print(f\"✅ モデル保存完了: {CONFIG['final_output_dir']}\")\n",
    "    \n",
    "    # 保存されたファイルの確認\n",
    "    saved_files = os.listdir(CONFIG['final_output_dir'])\n",
    "    print(f\"📁 保存されたファイル: {len(saved_files)} 個\")\n",
    "    for file in sorted(saved_files):\n",
    "        file_path = os.path.join(CONFIG['final_output_dir'], file)\n",
    "        file_size = os.path.getsize(file_path) / 1024**2\n",
    "        print(f\"  {file}: {file_size:.1f} MB\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ モデル保存エラー: {e}\")\n",
    "\n",
    "# 設定ファイルの保存\n",
    "config_file = os.path.join(CONFIG['final_output_dir'], 'training_config.json')\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    # 設定を保存（JSON形式）\n",
    "    config_to_save = CONFIG.copy()\n",
    "    config_to_save['used_model'] = used_model_name\n",
    "    config_to_save['training_completed'] = datetime.now().isoformat()\n",
    "    json.dump(config_to_save, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ 設定ファイル保存: {config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "package-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのパッケージング（ダウンロード用）\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    # ZIPファイルを作成\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    zip_filename = f\"tinyswallow_dpo_model_{timestamp}\"\n",
    "    \n",
    "    print(f\"📦 ZIPファイル作成中: {zip_filename}.zip\")\n",
    "    shutil.make_archive(zip_filename, 'zip', CONFIG['final_output_dir'])\n",
    "    \n",
    "    zip_size = os.path.getsize(f'{zip_filename}.zip') / 1024**2\n",
    "    print(f\"✅ ZIPファイル作成完了\")\n",
    "    print(f\"📁 ファイル名: {zip_filename}.zip\")\n",
    "    print(f\"📊 ファイルサイズ: {zip_size:.1f} MB\")\n",
    "    \n",
    "    # ダウンロード（コメントアウト状態）\n",
    "    print(\"\\n💾 ダウンロードするには、以下のコメントを解除してください:\")\n",
    "    print(f\"# files.download('{zip_filename}.zip')\")\n",
    "    \n",
    "    # 実際のダウンロードコード（必要に応じてコメント解除）\n",
    "    # files.download(f\"{zip_filename}.zip\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ パッケージング エラー: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 🎉 完了！\n",
    "\n",
    "### ✅ 達成項目\n",
    "- ✅ **環境セットアップ**: 最新ライブラリの安定インストール\n",
    "- ✅ **モデル読み込み**: TinySwallow-1.5B + フォールバック対応\n",
    "- ✅ **データセット処理**: 柔軟なデータ入力方式\n",
    "- ✅ **LoRA適用**: メモリ効率的なファインチューニング\n",
    "- ✅ **DPOトレーニング**: TRL 0.18.1 API対応\n",
    "- ✅ **評価・可視化**: 詳細なトレーニング分析\n",
    "- ✅ **モデルテスト**: 実用的な性能評価\n",
    "- ✅ **モデル保存**: ダウンロード可能な形式で出力\n",
    "\n",
    "### 🔧 技術的特徴\n",
    "- **API互換性**: 新旧TRL APIの両方に対応\n",
    "- **エラーハンドリング**: 堅牢なフォールバック機能\n",
    "- **メモリ最適化**: 4bit量子化 + LoRA\n",
    "- **Colab最適化**: GPU環境での効率的実行\n",
    "\n",
    "### 📚 次のステップ\n",
    "1. **大規模データでの訓練**: より多様なデータセットでの学習\n",
    "2. **ハイパーパラメータ調整**: 学習率・betaパラメータの最適化\n",
    "3. **評価メトリクス拡張**: BLEU, ROUGE, BERTScoreなど\n",
    "4. **プロダクション展開**: API化・Webアプリ化\n",
    "5. **継続的改善**: 人間フィードバックによる反復学習\n",
    "\n",
    "### 🔗 参考リンク\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl/)\n",
    "- [DPO Paper](https://arxiv.org/abs/2305.18290)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft/)\n",
    "- [BitsAndBytes Documentation](https://huggingface.co/docs/bitsandbytes/)\n",
    "\n",
    "### 📞 サポート\n",
    "問題が発生した場合は、[GitHub Issues](https://github.com/genfuru011/AD_Tech_SLM/issues) で報告してください。\n",
    "\n",
    "---\n",
    "**🦆 TinySwallow DPO Training - Improved Version Completed Successfully! 🎊**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}