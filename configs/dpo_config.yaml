# DPO Training Configuration
# Optimized for MacBook Air M2 8GB

# Model settings
model_name: "google/gemma-2b-it"  # Smaller model for M2 8GB
max_length: 512
max_prompt_length: 256

# Training parameters
num_train_epochs: 3
per_device_train_batch_size: 1  # Small batch size for 8GB memory
per_device_eval_batch_size: 1
gradient_accumulation_steps: 4  # Effective batch size = 4
learning_rate: 5e-6
warmup_steps: 50
weight_decay: 0.01
beta: 0.1  # DPO beta parameter

# LoRA settings
use_lora: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1
target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

# Hardware settings
device: "mps"  # Metal Performance Shaders for M2
fp16: true  # Mixed precision for memory efficiency
dataloader_num_workers: 2

# Logging and saving
logging_steps: 10
save_steps: 100
eval_steps: 100
save_total_limit: 3
output_dir: "./outputs"
logging_dir: "./outputs/logs"
report_to: "tensorboard"

# Dataset settings
dataset_path: "./data/sample_dpo_dataset.jsonl"
train_split: 0.8
validation_split: 0.2

# Evaluation settings
eval_strategy: "steps"
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: false

# Optimization settings for M2
gradient_checkpointing: true
remove_unused_columns: true
dataloader_pin_memory: false  # Disable for MPS