#!/usr/bin/env python3
"""
TinySwallow DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ç‰ˆ
é€²è¡ŒçŠ¶æ³ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å‡ºåŠ›ã™ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³
"""

import os
import sys
import json
import logging
import torch
import yaml
from datetime import datetime
from pathlib import Path
from datasets import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments,
    set_seed
)
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig, get_peft_model, TaskType
import warnings
warnings.filterwarnings("ignore")

def setup_logging():
    """ãƒ­ã‚°è¨­å®š"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(f'outputs/logs/dpo_realtime_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
        ]
    )
    return logging.getLogger(__name__)

def load_config(config_path: str) -> dict:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def create_custom_callback():
    """ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¯ãƒ©ã‚¹"""
    from transformers import TrainerCallback
    
    class RealtimeProgressCallback(TrainerCallback):
        def __init__(self):
            self.last_log_time = datetime.now()
            
        def on_step_end(self, args, state, control, **kwargs):
            current_time = datetime.now()
            if (current_time - self.last_log_time).seconds >= 30:  # 30ç§’ã”ã¨
                print(f"\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ— {state.global_step}/{args.max_steps} "
                      f"({state.global_step/args.max_steps*100:.1f}%) "
                      f"- ã‚¨ãƒãƒƒã‚¯ {state.epoch:.3f}")
                self.last_log_time = current_time
                
        def on_log(self, args, state, control, logs=None, **kwargs):
            if logs:
                print(f"ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ— {state.global_step}: "
                      f"loss={logs.get('loss', 'N/A'):.4f}, "
                      f"lr={logs.get('learning_rate', 'N/A'):.2e}")
                
        def on_save(self, args, state, control, **kwargs):
            print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: ã‚¹ãƒ†ãƒƒãƒ— {state.global_step}")
    
    return RealtimeProgressCallback()

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    logger = setup_logging()
    logger.info("ğŸ¦† TinySwallow DPO ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config("configs/tiny_swallow_config.yaml")
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    logger.info(f"ğŸ”§ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
    model_name = config['model']['name']
    logger.info(f"ğŸ“¥ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32 if device == "mps" else torch.float16,
        trust_remote_code=True
    )
    
    # LoRAè¨­å®š
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    model = get_peft_model(model, lora_config)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿")
    with open(config['dataset']['train_file'], 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²
    train_size = int(len(data) * 0.9)
    train_data = data[:train_size]
    eval_data = data[train_size:]
    
    train_dataset = Dataset.from_list(train_data)
    eval_dataset = Dataset.from_list(eval_data)
    
    logger.info(f"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    logger.info(f"ğŸ“Š è©•ä¾¡ãƒ‡ãƒ¼ã‚¿: {len(eval_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    training_config = config['training']
    
    # DPOConfigä½¿ç”¨
    try:
        training_args = DPOConfig(
            output_dir=training_config['output_dir'],
            num_train_epochs=training_config['num_train_epochs'],
            max_steps=training_config['max_steps'],
            per_device_train_batch_size=training_config['per_device_train_batch_size'],
            per_device_eval_batch_size=training_config['per_device_eval_batch_size'],
            gradient_accumulation_steps=training_config['gradient_accumulation_steps'],
            learning_rate=training_config['learning_rate'],
            warmup_steps=training_config['warmup_steps'],
            eval_steps=training_config['eval_steps'],
            save_steps=training_config['save_steps'],
            logging_steps=training_config['logging_steps'],
            eval_strategy=training_config['eval_strategy'],
            save_strategy=training_config['save_strategy'],
            load_best_model_at_end=training_config['load_best_model_at_end'],
            metric_for_best_model=training_config['metric_for_best_model'],
            fp16=training_config['fp16'],
            dataloader_pin_memory=training_config['dataloader_pin_memory'],
            remove_unused_columns=training_config['remove_unused_columns'],
            report_to=training_config['report_to'],
            beta=config['dpo']['beta'],
            max_length=config['dpo']['max_length'],
            max_prompt_length=config['dpo']['max_prompt_length']
        )
        logger.info("âœ… DPOConfigä½¿ç”¨")
    except Exception as e:
        logger.warning(f"DPOConfigå¤±æ•—ã€TrainingArgumentsä½¿ç”¨: {e}")
        training_args = TrainingArguments(**training_config)
    
    # DPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
    try:
        trainer = DPOTrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            processing_class=tokenizer,
            callbacks=[create_custom_callback()]
        )
        logger.info("âœ… DPOTraineråˆæœŸåŒ–å®Œäº†ï¼ˆTRL 0.18.1 APIï¼‰")
    except Exception as e:
        logger.warning(f"æ–°APIå¤±æ•—ã€ãƒ¬ã‚¬ã‚·ãƒ¼APIä½¿ç”¨: {e}")
        trainer = DPOTrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            callbacks=[create_custom_callback()]
        )
        logger.info("âœ… DPOTraineråˆæœŸåŒ–å®Œäº†ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼APIï¼‰")
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹
    logger.info("ğŸš€ DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ï¼‰")
    print("=" * 60)
    print("ğŸš€ DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²è¡Œè¡¨ç¤º")
    print(f"ğŸ“Š ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {training_config['max_steps']}")
    print(f"ğŸ’¾ ä¿å­˜é–“éš”: {training_config['save_steps']} ã‚¹ãƒ†ãƒƒãƒ—")
    print(f"ğŸ“ˆ è©•ä¾¡é–“éš”: {training_config['eval_steps']} ã‚¹ãƒ†ãƒƒãƒ—")
    print("=" * 60)
    
    try:
        trainer.train()
        logger.info("âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
        print("\nğŸ‰ DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
        
        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        final_model_path = f"{training_config['output_dir']}/final_model"
        trainer.save_model(final_model_path)
        logger.info(f"ğŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {final_model_path}")
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
        print("\nâ¹ï¸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"âŒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")

if __name__ == "__main__":
    main()
