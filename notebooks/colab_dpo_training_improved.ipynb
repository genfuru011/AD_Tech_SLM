{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸ¦† TinySwallow DPO Training - Improved Version\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/genfuru011/AD_Tech_SLM/blob/main/notebooks/colab_dpo_training_improved.ipynb)\n",
    "\n",
    "**Direct Preference Optimization (DPO) Training for Japanese Language Model**\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€SakanaAI/TinySwallow-1.5B-Instructãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦åºƒå‘ŠæŠ€è¡“åˆ†é‡ã®DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ¯ ä¸»ãªç‰¹å¾´\n",
    "- **æœ€æ–°TRL 0.18.1å¯¾å¿œ**: APIå¤‰æ›´ã«å¯¾å¿œã—ãŸå …ç‰¢ãªå®Ÿè£…\n",
    "- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: è¤‡æ•°ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
    "- **Colabæœ€é©åŒ–**: GPUç’°å¢ƒã§ã®åŠ¹ç‡çš„ãªå­¦ç¿’\n",
    "- **åŒ…æ‹¬çš„ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹\n",
    "\n",
    "## ğŸ“š å‚è€ƒè³‡æ–™\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl/)\n",
    "- [DPO Paper](https://arxiv.org/abs/2305.18290)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "### GPUç¢ºèªã¨ãƒ©ãƒ³ã‚¿ã‚¤ãƒ æƒ…å ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUæƒ…å ±ã®ç¢ºèª\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "print(\"ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:\")\n",
    "print(f\"  Python: {sys.version}\")\n",
    "print(f\"  Platform: {platform.platform()}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"  GPU Count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"  âš ï¸ GPU not available - training will be slow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-header",
   "metadata": {},
   "source": [
    "### ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "\n",
    "æœ€æ–°ã®å®‰å®šç‰ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚TRL 0.18.1ã®å¤‰æ›´ã«å¯¾å¿œæ¸ˆã¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -q torch>=2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers>=4.36.0\n",
    "!pip install -q trl>=0.7.0\n",
    "!pip install -q datasets>=2.14.0\n",
    "!pip install -q peft>=0.6.0\n",
    "!pip install -q bitsandbytes>=0.41.0\n",
    "!pip install -q accelerate>=0.24.0\n",
    "!pip install -q wandb\n",
    "!pip install -q tensorboard\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "\n",
    "# Colabç‰¹æœ‰ã®å•é¡Œè§£æ±ºç”¨\n",
    "!pip install -q tf-keras\n",
    "\n",
    "print(\"âœ… ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-header",
   "metadata": {},
   "source": [
    "### ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Transformers & TRL\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        BitsAndBytesConfig,\n",
    "        TrainingArguments\n",
    "    )\n",
    "    from trl import DPOTrainer, DPOConfig\n",
    "    from datasets import Dataset\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "    import accelerate\n",
    "    import transformers\n",
    "    import trl\n",
    "    print(\"âœ… ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"ä¸Šè¨˜ã®pip installã‚³ãƒãƒ³ãƒ‰ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# è¨­å®š\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±è¡¨ç¤º\n",
    "print(\"ğŸ“¦ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒãƒ¼ã‚¸ãƒ§ãƒ³:\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  Transformers: {transformers.__version__}\")\n",
    "print(f\"  TRL: {trl.__version__}\")\n",
    "print(f\"  Accelerate: {accelerate.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## âš™ï¸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\n",
    "\n",
    "Google Colabç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\n",
    "CONFIG = {\n",
    "    # ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "    \"model_name\": \"SakanaAI/TinySwallow-1.5B-Instruct\",\n",
    "    \"backup_models\": [\n",
    "        \"tokyotech-llm/Swallow-1.5b-instruct-hf\",\n",
    "        \"rinna/japanese-gpt-neox-3.6b-instruction-sft\"\n",
    "    ],\n",
    "    \n",
    "    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šï¼ˆColab GPUæœ€é©åŒ–ï¼‰\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"max_steps\": 500,\n",
    "    \"per_device_train_batch_size\": 1,  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æŠ‘åˆ¶\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,  # å®Ÿè³ªçš„ãªãƒãƒƒãƒã‚µã‚¤ã‚º = 8\n",
    "    \"learning_rate\": 5e-7,\n",
    "    \"warmup_steps\": 50,\n",
    "    \"eval_steps\": 50,\n",
    "    \"save_steps\": 100,\n",
    "    \"logging_steps\": 10,\n",
    "    \n",
    "    # DPOè¨­å®š\n",
    "    \"beta\": 0.1,\n",
    "    \"max_length\": 1024,\n",
    "    \"max_prompt_length\": 512,\n",
    "    \n",
    "    # LoRAè¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    \n",
    "    # é‡å­åŒ–è¨­å®š\n",
    "    \"use_4bit\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"use_nested_quant\": False,\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š\n",
    "    \"test_size\": 0.1,\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # å‡ºåŠ›è¨­å®š\n",
    "    \"output_dir\": \"./outputs/colab_dpo_improved\",\n",
    "    \"final_output_dir\": \"./final_model\",\n",
    "}\n",
    "\n",
    "print(\"âœ… è¨­å®šå®Œäº†\")\n",
    "print(f\"ğŸ“‹ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {CONFIG['model_name']}\")\n",
    "print(f\"ğŸ¯ æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°: {CONFIG['max_steps']}\")\n",
    "print(f\"ğŸ“Š å®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"ğŸ”¥ å­¦ç¿’ç‡: {CONFIG['learning_rate']}\")\n",
    "print(f\"âš¡ DPO Beta: {CONFIG['beta']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\n",
    "\n",
    "DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã—ã¾ã™ã€‚è¤‡æ•°ã®æ–¹æ³•ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-upload",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆæ–¹æ³•1: ãƒ•ã‚¡ã‚¤ãƒ«ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "sample_dpo_data = [\n",
    "    {\n",
    "        \"prompt\": \"ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®RTBã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n",
    "        \"chosen\": \"RTBï¼ˆReal-Time Biddingï¼‰ã¯ã€åºƒå‘Šæ ã®å£²è²·ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³å½¢å¼ã§è¡Œã†ä»•çµ„ã¿ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒWebãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ãŸç¬é–“ã«ã€ãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å±æ€§ã‚„é–²è¦§å±¥æ­´ã«åŸºã¥ã„ã¦ã€åºƒå‘Šä¸»ãŒè‡ªå‹•çš„ã«å…¥æœ­ã‚’è¡Œã„ã¾ã™ã€‚æœ€ã‚‚é«˜ã„é‡‘é¡ã‚’æç¤ºã—ãŸåºƒå‘Šä¸»ã®åºƒå‘ŠãŒè¡¨ç¤ºã•ã‚Œã‚‹ä»•çµ„ã¿ã§ã€åŠ¹ç‡çš„ãªã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°ã¨è²»ç”¨å¯¾åŠ¹æœã®å‘ä¸Šã‚’å®Ÿç¾ã—ã¾ã™ã€‚\",\n",
    "        \"rejected\": \"RTBã¯åºƒå‘Šã‚’è¡¨ç¤ºã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"DSPã¨SSPã®é•ã„ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\",\n",
    "        \"chosen\": \"DSPï¼ˆDemand-Side Platformï¼‰ã¯åºƒå‘Šä¸»å´ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã€åºƒå‘Šæ ã®è³¼å…¥ã‚’è‡ªå‹•åŒ–ã—ã€ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°ã‚„å…¥æœ­æˆ¦ç•¥ã®æœ€é©åŒ–ã‚’è¡Œã„ã¾ã™ã€‚ä¸€æ–¹ã€SSPï¼ˆSupply-Side Platformï¼‰ã¯ãƒ¡ãƒ‡ã‚£ã‚¢å´ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã€åºƒå‘Šæ ã®è²©å£²ã‚’è‡ªå‹•åŒ–ã—ã€åç›Šã®æœ€å¤§åŒ–ã‚’å›³ã‚Šã¾ã™ã€‚DSPã¯è²·ã„æ‰‹ã€SSPã¯å£²ã‚Šæ‰‹ã®ç«‹å ´ã§ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®å–å¼•ã‚’æ”¯æ´ã—ã¾ã™ã€‚\",\n",
    "        \"rejected\": \"DSPã¨SSPã¯ä¸¡æ–¹ã¨ã‚‚åºƒå‘Šé–¢é€£ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"ã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æã®é‡è¦æ€§ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚\",\n",
    "        \"chosen\": \"ã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æã¯ã€ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«è‡³ã‚‹ã¾ã§ã®å„ã‚¿ãƒƒãƒãƒã‚¤ãƒ³ãƒˆã®è²¢çŒ®åº¦ã‚’æ¸¬å®šã™ã‚‹åˆ†ææ‰‹æ³•ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³¼è²·è¡Œå‹•ã¯è¤‡é›‘ã§ã€è¤‡æ•°ã®åºƒå‘Šæ¥è§¦ã‚’çµŒã¦ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«è‡³ã‚‹ãŸã‚ã€ãƒ©ã‚¹ãƒˆã‚¯ãƒªãƒƒã‚¯ä»¥å¤–ã®æ¥è§¦ç‚¹ã®ä¾¡å€¤ã‚‚é©åˆ‡ã«è©•ä¾¡ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°äºˆç®—ã®æœ€é©é…åˆ†ã€ãƒãƒ£ãƒãƒ«é–“ã®ç›¸äº’ä½œç”¨ã®ç†è§£ã€ROIã®æ­£ç¢ºãªæ¸¬å®šãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚\",\n",
    "        \"rejected\": \"ã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æã¯åºƒå‘Šã®åŠ¹æœã‚’æ¸¬å®šã™ã‚‹æ–¹æ³•ã§ã™ã€‚\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ DPOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\")\n",
    "print(\"ğŸ“‹ æœŸå¾…ã•ã‚Œã‚‹å½¢å¼ï¼ˆJSONLï¼‰:\")\n",
    "print('{\"prompt\": \"è³ªå•\", \"chosen\": \"è‰¯ã„å›ç­”\", \"rejected\": \"æ‚ªã„å›ç­”\"}')\n",
    "print()\n",
    "print(\"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯ã€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¾ã™\")\n",
    "\n",
    "try:\n",
    "    uploaded = files.upload()\n",
    "    dataset_file = None\n",
    "    \n",
    "    for filename in uploaded.keys():\n",
    "        if filename.endswith('.jsonl') or filename.endswith('.json'):\n",
    "            dataset_file = filename\n",
    "            print(f\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {dataset_file}\")\n",
    "            break\n",
    "    \n",
    "    if dataset_file is None:\n",
    "        print(\"âŒ é©åˆ‡ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        use_sample_data = True\n",
    "    else:\n",
    "        use_sample_data = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}\")\n",
    "    use_sample_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\n",
    "def load_dpo_dataset(use_sample: bool = False, dataset_file: str = None) -> Dataset:\n",
    "    \"\"\"DPOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã‚’è¡Œã†\"\"\"\n",
    "    dataset_list = []\n",
    "    \n",
    "    if use_sample:\n",
    "        print(\"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨\")\n",
    "        dataset_list = sample_dpo_data\n",
    "    else:\n",
    "        print(f\"ğŸ“Š ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿: {dataset_file}\")\n",
    "        try:\n",
    "            # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "            if dataset_file.endswith('.jsonl'):\n",
    "                content = uploaded[dataset_file].decode('utf-8')\n",
    "                for line_num, line in enumerate(content.strip().split('\\n'), 1):\n",
    "                    if line.strip():\n",
    "                        try:\n",
    "                            data = json.loads(line)\n",
    "                            required_keys = ['prompt', 'chosen', 'rejected']\n",
    "                            if all(key in data for key in required_keys):\n",
    "                                dataset_list.append({\n",
    "                                    'prompt': data['prompt'],\n",
    "                                    'chosen': data['chosen'],\n",
    "                                    'rejected': data['rejected']\n",
    "                                })\n",
    "                            else:\n",
    "                                print(f\"âš ï¸ Line {line_num}: å¿…è¦ãªã‚­ãƒ¼ãŒä¸è¶³\")\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"âš ï¸ Line {line_num}: JSONå½¢å¼ã‚¨ãƒ©ãƒ¼ - {e}\")\n",
    "            \n",
    "            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "            elif dataset_file.endswith('.json'):\n",
    "                content = uploaded[dataset_file].decode('utf-8')\n",
    "                data = json.loads(content)\n",
    "                if isinstance(data, list):\n",
    "                    dataset_list = data\n",
    "                else:\n",
    "                    print(\"âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒªã‚¹ãƒˆå½¢å¼ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\")\n",
    "                    dataset_list = sample_dpo_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            print(\"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\")\n",
    "            dataset_list = sample_dpo_data\n",
    "    \n",
    "    if not dataset_list:\n",
    "        print(\"âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\")\n",
    "        dataset_list = sample_dpo_data\n",
    "    \n",
    "    print(f\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(dataset_list)} ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆæƒ…å ±\n",
    "    if dataset_list:\n",
    "        avg_prompt_len = sum(len(item['prompt']) for item in dataset_list) / len(dataset_list)\n",
    "        avg_chosen_len = sum(len(item['chosen']) for item in dataset_list) / len(dataset_list)\n",
    "        avg_rejected_len = sum(len(item['rejected']) for item in dataset_list) / len(dataset_list)\n",
    "        \n",
    "        print(f\"ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ:\")\n",
    "        print(f\"  å¹³å‡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {avg_prompt_len:.1f} æ–‡å­—\")\n",
    "        print(f\"  å¹³å‡é¸æŠå›ç­”é•·: {avg_chosen_len:.1f} æ–‡å­—\")\n",
    "        print(f\"  å¹³å‡æ‹’å¦å›ç­”é•·: {avg_rejected_len:.1f} æ–‡å­—\")\n",
    "    \n",
    "    return Dataset.from_list(dataset_list)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
    "dataset = load_dpo_dataset(use_sample=use_sample_data, dataset_file=dataset_file if not use_sample_data else None)\n",
    "\n",
    "# è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²\n",
    "train_test = dataset.train_test_split(test_size=CONFIG['test_size'], seed=CONFIG['seed'])\n",
    "train_dataset = train_test['train']\n",
    "eval_dataset = train_test['test']\n",
    "\n",
    "print(f\"\\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ:\")\n",
    "print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "print(f\"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(eval_dataset)} ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã®è¡¨ç¤º\n",
    "print(f\"\\nğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {sample['prompt'][:100]}...\")\n",
    "print(f\"  é¸æŠå›ç­”: {sample['chosen'][:100]}...\")\n",
    "print(f\"  æ‹’å¦å›ç­”: {sample['rejected'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
    "\n",
    "TinySwallowãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€é‡å­åŒ–ã¨LoRAã‚’é©ç”¨ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(config: Dict) -> tuple:\n",
    "    \"\"\"ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰\"\"\"\n",
    "    \n",
    "    # é‡å­åŒ–è¨­å®š\n",
    "    if config['use_4bit']:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=getattr(torch, config['bnb_4bit_compute_dtype']),\n",
    "            bnb_4bit_quant_type=config['bnb_4bit_quant_type'],\n",
    "            bnb_4bit_use_double_quant=config['use_nested_quant'],\n",
    "        )\n",
    "        print(\"âš¡ 4bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–\")\n",
    "    else:\n",
    "        bnb_config = None\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰\n",
    "    models_to_try = [config['model_name']] + config['backup_models']\n",
    "    \n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    used_model = None\n",
    "    \n",
    "    for model_name in models_to_try:\n",
    "        try:\n",
    "            print(f\"ğŸ“¥ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿è©¦è¡Œ: {model_name}\")\n",
    "            \n",
    "            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                print(\"ğŸ”§ ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š\")\n",
    "            \n",
    "            # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "            model_kwargs = {\n",
    "                'trust_remote_code': True,\n",
    "                'torch_dtype': torch.float16,\n",
    "                'device_map': 'auto',\n",
    "            }\n",
    "            \n",
    "            if bnb_config is not None:\n",
    "                model_kwargs['quantization_config'] = bnb_config\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "            used_model = model_name\n",
    "            print(f\"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {model_name}\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {model_name} ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if model is None:\n",
    "        raise RuntimeError(\"ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:\")\n",
    "    print(f\"  ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {used_model}\")\n",
    "    print(f\"  ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
    "    print(f\"  å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}\")\n",
    "    \n",
    "    return model, tokenizer, used_model\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
    "model, tokenizer, used_model_name = load_model_and_tokenizer(CONFIG)\n",
    "\n",
    "# GPUä½¿ç”¨é‡ã®ç¢ºèª\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    memory_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"\\nğŸ’¾ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:\")\n",
    "    print(f\"  ä½¿ç”¨ä¸­: {memory_allocated:.2f} GB\")\n",
    "    print(f\"  äºˆç´„æ¸ˆã¿: {memory_reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lora-header",
   "metadata": {},
   "source": [
    "### LoRAè¨­å®šã®é©ç”¨\n",
    "\n",
    "Parameter Efficient Fine-tuning (PEFT) ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå­¦ç¿’ã‚’å®Ÿç¾ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lora-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRAè¨­å®š\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG['lora_r'],\n",
    "    lora_alpha=CONFIG['lora_alpha'],\n",
    "    target_modules=CONFIG['target_modules'],\n",
    "    lora_dropout=CONFIG['lora_dropout'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"ğŸ”§ LoRAè¨­å®š:\")\n",
    "print(f\"  ãƒ©ãƒ³ã‚¯ (r): {CONFIG['lora_r']}\")\n",
    "print(f\"  ã‚¢ãƒ«ãƒ•ã‚¡: {CONFIG['lora_alpha']}\")\n",
    "print(f\"  ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ: {CONFIG['lora_dropout']}\")\n",
    "print(f\"  å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: {CONFIG['target_modules']}\")\n",
    "\n",
    "# LoRAã®é©ç”¨\n",
    "if CONFIG['use_4bit']:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    print(\"âš¡ 4bitå­¦ç¿’ç”¨ã®æº–å‚™å®Œäº†\")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"âœ… LoRAé©ç”¨å®Œäº†\")\n",
    "\n",
    "# å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ç¢ºèª\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# æ›´æ–°ã•ã‚ŒãŸGPUä½¿ç”¨é‡\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    memory_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"\\nğŸ’¾ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆLoRAé©ç”¨å¾Œï¼‰:\")\n",
    "    print(f\"  ä½¿ç”¨ä¸­: {memory_allocated:.2f} GB\")\n",
    "    print(f\"  äºˆç´„æ¸ˆã¿: {memory_reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "## ğŸš€ DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ\n",
    "\n",
    "æœ€æ–°ã®TRL APIã«å¯¾å¿œã—ãŸå …ç‰¢ãªDPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['final_output_dir'], exist_ok=True)\n",
    "\n",
    "# DPOConfigè¨­å®šï¼ˆTRL 0.18.1å¯¾å¿œï¼‰\n",
    "try:\n",
    "    # æ–°ã—ã„API (TRL >= 0.8.0)\n",
    "    training_args = DPOConfig(\n",
    "        output_dir=CONFIG['output_dir'],\n",
    "        num_train_epochs=CONFIG['num_train_epochs'],\n",
    "        max_steps=CONFIG['max_steps'],\n",
    "        per_device_train_batch_size=CONFIG['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=CONFIG['per_device_eval_batch_size'],\n",
    "        gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        warmup_steps=CONFIG['warmup_steps'],\n",
    "        eval_steps=CONFIG['eval_steps'],\n",
    "        save_steps=CONFIG['save_steps'],\n",
    "        logging_steps=CONFIG['logging_steps'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=[],  # WandBã‚’ç„¡åŠ¹åŒ–\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        gradient_checkpointing=True,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        max_prompt_length=CONFIG['max_prompt_length'],\n",
    "        beta=CONFIG['beta'],\n",
    "    )\n",
    "    print(\"âœ… DPOConfigè¨­å®šå®Œäº†ï¼ˆæ–°APIï¼‰\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ æ–°APIå¤±æ•—ã€ãƒ¬ã‚¬ã‚·ãƒ¼APIã‚’è©¦è¡Œ: {e}\")\n",
    "    # ãƒ¬ã‚¬ã‚·ãƒ¼API (TRL < 0.8.0)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=CONFIG['output_dir'],\n",
    "        num_train_epochs=CONFIG['num_train_epochs'],\n",
    "        max_steps=CONFIG['max_steps'],\n",
    "        per_device_train_batch_size=CONFIG['per_device_train_batch_size'],\n",
    "        per_device_eval_batch_size=CONFIG['per_device_eval_batch_size'],\n",
    "        gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        warmup_steps=CONFIG['warmup_steps'],\n",
    "        eval_steps=CONFIG['eval_steps'],\n",
    "        save_steps=CONFIG['save_steps'],\n",
    "        logging_steps=CONFIG['logging_steps'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=[],\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        gradient_checkpointing=True,\n",
    "    )\n",
    "    print(\"âœ… TrainingArgumentsè¨­å®šå®Œäº†ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼APIï¼‰\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š:\")\n",
    "print(f\"  æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°: {CONFIG['max_steps']}\")\n",
    "print(f\"  å­¦ç¿’ç‡: {CONFIG['learning_rate']}\")\n",
    "print(f\"  DPO Beta: {CONFIG['beta']}\")\n",
    "print(f\"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {CONFIG['per_device_train_batch_size']}\")\n",
    "print(f\"  å‹¾é…è“„ç©: {CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPOTrainer ã®åˆæœŸåŒ–ï¼ˆTRL APIå¤‰æ›´å¯¾å¿œï¼‰\n",
    "try:\n",
    "    # æ–°API (TRL >= 0.8.0)\n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=tokenizer,  # æ–°API\n",
    "    )\n",
    "    print(\"âœ… DPOTraineråˆæœŸåŒ–å®Œäº†ï¼ˆæ–°APIï¼‰\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ æ–°APIå¤±æ•—ã€ãƒ¬ã‚¬ã‚·ãƒ¼APIã‚’è©¦è¡Œ: {e}\")\n",
    "    try:\n",
    "        # ãƒ¬ã‚¬ã‚·ãƒ¼APIå¯¾å¿œ\n",
    "        trainer = DPOTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,  # ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            beta=CONFIG['beta'],\n",
    "            max_length=CONFIG['max_length'],\n",
    "            max_prompt_length=CONFIG['max_prompt_length'],\n",
    "        )\n",
    "        print(\"âœ… DPOTraineråˆæœŸåŒ–å®Œäº†ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼APIï¼‰\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ ã™ã¹ã¦ã®APIåˆæœŸåŒ–ã«å¤±æ•—: {e2}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nğŸš€ DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æº–å‚™å®Œäº†\")\n",
    "print(f\"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "print(f\"ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(eval_dataset)} ã‚µãƒ³ãƒ—ãƒ«\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
    "print(\"ğŸš€ DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...\")\n",
    "print(f\"â° é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "try:\n",
    "    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\nğŸ‰ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼\")\n",
    "    print(f\"â° çµ‚äº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"ğŸ“Š æœ€çµ‚è¨“ç·´ãƒ­ã‚¹: {train_result.training_loss:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-header",
   "metadata": {},
   "source": [
    "## ğŸ“Š è©•ä¾¡ã¨å¯è¦–åŒ–\n",
    "\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®è©•ä¾¡ã¨å¯è¦–åŒ–ã‚’è¡Œã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€çµ‚è©•ä¾¡\n",
    "try:\n",
    "    print(\"ğŸ“Š æœ€çµ‚è©•ä¾¡å®Ÿè¡Œä¸­...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(\"\\nğŸ“Š æœ€çµ‚è©•ä¾¡çµæœ:\")\n",
    "    for key, value in eval_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    eval_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å±¥æ­´ã®å¯è¦–åŒ–\n",
    "try:\n",
    "    # ãƒ­ã‚°å±¥æ­´ã®å–å¾—\n",
    "    log_history = trainer.state.log_history\n",
    "    \n",
    "    if log_history:\n",
    "        # è¨“ç·´ãƒ­ã‚¹ã®æŠ½å‡º\n",
    "        train_loss = []\n",
    "        train_steps = []\n",
    "        eval_loss = []\n",
    "        eval_steps = []\n",
    "        \n",
    "        for log in log_history:\n",
    "            if 'loss' in log:\n",
    "                train_loss.append(log['loss'])\n",
    "                train_steps.append(log['step'])\n",
    "            if 'eval_loss' in log:\n",
    "                eval_loss.append(log['eval_loss'])\n",
    "                eval_steps.append(log['step'])\n",
    "        \n",
    "        # ã‚°ãƒ©ãƒ•ã®ä½œæˆ\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # è¨“ç·´ãƒ­ã‚¹\n",
    "        plt.subplot(1, 2, 1)\n",
    "        if train_loss:\n",
    "            plt.plot(train_steps, train_loss, 'b-', label='Training Loss')\n",
    "        if eval_loss:\n",
    "            plt.plot(eval_steps, eval_loss, 'r-', label='Validation Loss')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Progress')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # å­¦ç¿’ç‡\n",
    "        plt.subplot(1, 2, 2)\n",
    "        learning_rates = [log.get('learning_rate', 0) for log in log_history if 'learning_rate' in log]\n",
    "        lr_steps = [log['step'] for log in log_history if 'learning_rate' in log]\n",
    "        if learning_rates:\n",
    "            plt.plot(lr_steps, learning_rates, 'g-')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å±¥æ­´ã®å¯è¦–åŒ–å®Œäº†\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ãƒ­ã‚°å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-header",
   "metadata": {},
   "source": [
    "## ğŸ§ª å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "test_prompts = [\n",
    "    \"ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®ä»•çµ„ã¿ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n",
    "    \"DSPã®ä¸»ãªæ©Ÿèƒ½ã¨åˆ©ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\",\n",
    "    \"ã‚¯ãƒƒã‚­ãƒ¼ãƒ¬ã‚¹æ™‚ä»£ã®ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°æ‰‹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n",
    "    \"ã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æãŒãªãœé‡è¦ãªã®ã‹èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆé–‹å§‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "model.eval()\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    try:\n",
    "        print(f\"\\nğŸ“ ãƒ†ã‚¹ãƒˆ {i}/4:\")\n",
    "        print(f\"è³ªå•: {prompt}\")\n",
    "        \n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # GPUä½¿ç”¨æ™‚ã¯ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        # æ¨è«–å®Ÿè¡Œ\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # ç”Ÿæˆçµæœã®ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"å›ç­”: {response}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ†ã‚¹ãƒˆ {i} ã§ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆå®Œäº†ï¼\")\n",
    "print(\"\\nğŸ’¡ è©•ä¾¡ã®ãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "print(\"   â€¢ å…ƒã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦å¿œç­”å“è³ªãŒå‘ä¸Šã—ã¦ã„ã‚‹ã‹\")\n",
    "print(\"   â€¢ æŠ€è¡“ç”¨èªãŒé©åˆ‡ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹\")\n",
    "print(\"   â€¢ å›ç­”ã®æ­£ç¢ºæ€§ã¨ä¸€è²«æ€§\")\n",
    "print(\"   â€¢ ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®çŸ¥è­˜ãŒåæ˜ ã•ã‚Œã¦ã„ã‚‹ã‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "\n",
    "å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªå½¢å¼ã§æº–å‚™ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "print(\"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...\")\n",
    "\n",
    "try:\n",
    "    # æœ€çµ‚çš„ãªãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "    trainer.save_model(CONFIG['final_output_dir'])\n",
    "    tokenizer.save_pretrained(CONFIG['final_output_dir'])\n",
    "    \n",
    "    print(f\"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {CONFIG['final_output_dir']}\")\n",
    "    \n",
    "    # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "    saved_files = os.listdir(CONFIG['final_output_dir'])\n",
    "    print(f\"ğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«: {len(saved_files)} å€‹\")\n",
    "    for file in sorted(saved_files):\n",
    "        file_path = os.path.join(CONFIG['final_output_dir'], file)\n",
    "        file_size = os.path.getsize(file_path) / 1024**2\n",
    "        print(f\"  {file}: {file_size:.1f} MB\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜\n",
    "config_file = os.path.join(CONFIG['final_output_dir'], 'training_config.json')\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    # è¨­å®šã‚’ä¿å­˜ï¼ˆJSONå½¢å¼ï¼‰\n",
    "    config_to_save = CONFIG.copy()\n",
    "    config_to_save['used_model'] = used_model_name\n",
    "    config_to_save['training_completed'] = datetime.now().isoformat()\n",
    "    json.dump(config_to_save, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "package-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ï¼‰\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    zip_filename = f\"tinyswallow_dpo_model_{timestamp}\"\n",
    "    \n",
    "    print(f\"ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­: {zip_filename}.zip\")\n",
    "    shutil.make_archive(zip_filename, 'zip', CONFIG['final_output_dir'])\n",
    "    \n",
    "    zip_size = os.path.getsize(f'{zip_filename}.zip') / 1024**2\n",
    "    print(f\"âœ… ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†\")\n",
    "    print(f\"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å: {zip_filename}.zip\")\n",
    "    print(f\"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {zip_size:.1f} MB\")\n",
    "    \n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆçŠ¶æ…‹ï¼‰\n",
    "    print(\"\\nğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’è§£é™¤ã—ã¦ãã ã•ã„:\")\n",
    "    print(f\"# files.download('{zip_filename}.zip')\")\n",
    "    \n",
    "    # å®Ÿéš›ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ï¼ˆå¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤ï¼‰\n",
    "    # files.download(f\"{zip_filename}.zip\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚° ã‚¨ãƒ©ãƒ¼: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## ğŸ‰ å®Œäº†ï¼\n",
    "\n",
    "### âœ… é”æˆé …ç›®\n",
    "- âœ… **ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: æœ€æ–°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å®‰å®šã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "- âœ… **ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿**: TinySwallow-1.5B + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œ\n",
    "- âœ… **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†**: æŸ”è»Ÿãªãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ–¹å¼\n",
    "- âœ… **LoRAé©ç”¨**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- âœ… **DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°**: TRL 0.18.1 APIå¯¾å¿œ\n",
    "- âœ… **è©•ä¾¡ãƒ»å¯è¦–åŒ–**: è©³ç´°ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åˆ†æ\n",
    "- âœ… **ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ**: å®Ÿç”¨çš„ãªæ€§èƒ½è©•ä¾¡\n",
    "- âœ… **ãƒ¢ãƒ‡ãƒ«ä¿å­˜**: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªå½¢å¼ã§å‡ºåŠ›\n",
    "\n",
    "### ğŸ”§ æŠ€è¡“çš„ç‰¹å¾´\n",
    "- **APIäº’æ›æ€§**: æ–°æ—§TRL APIã®ä¸¡æ–¹ã«å¯¾å¿œ\n",
    "- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å …ç‰¢ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½\n",
    "- **ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**: 4bité‡å­åŒ– + LoRA\n",
    "- **Colabæœ€é©åŒ–**: GPUç’°å¢ƒã§ã®åŠ¹ç‡çš„å®Ÿè¡Œ\n",
    "\n",
    "### ğŸ“š æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "1. **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®è¨“ç·´**: ã‚ˆã‚Šå¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å­¦ç¿’\n",
    "2. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**: å­¦ç¿’ç‡ãƒ»betaãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–\n",
    "3. **è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ‹¡å¼µ**: BLEU, ROUGE, BERTScoreãªã©\n",
    "4. **ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å±•é–‹**: APIåŒ–ãƒ»Webã‚¢ãƒ—ãƒªåŒ–\n",
    "5. **ç¶™ç¶šçš„æ”¹å–„**: äººé–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã‚ˆã‚‹åå¾©å­¦ç¿’\n",
    "\n",
    "### ğŸ”— å‚è€ƒãƒªãƒ³ã‚¯\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl/)\n",
    "- [DPO Paper](https://arxiv.org/abs/2305.18290)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft/)\n",
    "- [BitsAndBytes Documentation](https://huggingface.co/docs/bitsandbytes/)\n",
    "\n",
    "### ğŸ“ ã‚µãƒãƒ¼ãƒˆ\n",
    "å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€[GitHub Issues](https://github.com/genfuru011/AD_Tech_SLM/issues) ã§å ±å‘Šã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "---\n",
    "**ğŸ¦† TinySwallow DPO Training - Improved Version Completed Successfully! ğŸŠ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}