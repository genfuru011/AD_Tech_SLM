{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7df24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š æ–¹æ³•1: GitHubçµŒç”±ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒGitHubã«ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "\n",
    "USE_GITHUB = False  # â† Trueã«å¤‰æ›´ã—ã¦GitHubãƒªãƒã‚¸ãƒˆãƒªURLã‚’è¨­å®š\n",
    "GITHUB_REPO_URL = \"\"  # â† ã‚ãªãŸã®GitHubãƒªãƒã‚¸ãƒˆãƒªURLã‚’å…¥åŠ›\n",
    "\n",
    "if USE_GITHUB:\n",
    "    import os\n",
    "    \n",
    "    # GitHubã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "    repo_name = GITHUB_REPO_URL.split('/')[-1].replace('.git', '')\n",
    "    \n",
    "    if not os.path.exists(repo_name):\n",
    "        !git clone {GITHUB_REPO_URL}\n",
    "        print(f\"âœ… Repository cloned: {repo_name}\")\n",
    "    else:\n",
    "        print(f\"ğŸ“ Repository already exists: {repo_name}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "    dataset_path = f\"{repo_name}/data/dpo_dataset.jsonl\"\n",
    "    \n",
    "    if os.path.exists(dataset_path):\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¡¨ç¤º\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        print(f\"ğŸ“Š Dataset found: {len(lines)} samples\")\n",
    "        print(f\"ğŸ“ Dataset path: {dataset_path}\")\n",
    "        \n",
    "        # æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤º\n",
    "        import json\n",
    "        print(\"\\nğŸ” First sample:\")\n",
    "        sample = json.loads(lines[0])\n",
    "        for key, value in sample.items():\n",
    "            if isinstance(value, str) and len(value) > 100:\n",
    "                print(f\"{key}: {value[:100]}...\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(f\"âŒ Dataset not found at {dataset_path}\")\n",
    "        print(\"Please check your repository structure or use another method.\")\n",
    "else:\n",
    "    print(\"ğŸ“ GitHub method not enabled. Set USE_GITHUB = True if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7cfd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š æ–¹æ³•2: Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½¿ç”¨\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰ä½¿ç”¨\n",
    "\n",
    "USE_GOOGLE_DRIVE = False  # â† Trueã«å¤‰æ›´ã—ã¦Google Driveã‚’ä½¿ç”¨\n",
    "\n",
    "if USE_GOOGLE_DRIVE:\n",
    "    from google.colab import drive, files\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    # Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"âœ… Google Drive mounted\")\n",
    "    \n",
    "    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "    print(\"\\nğŸ“¤ Option 1: Upload dataset file directly\")\n",
    "    print(\"Run the next cell to upload your dpo_dataset.jsonl file\")\n",
    "    \n",
    "    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³2: Google Driveã‹ã‚‰èª­ã¿è¾¼ã¿\n",
    "    print(\"\\nğŸ“ Option 2: Place file in Google Drive\")\n",
    "    print(\"1. Upload dpo_dataset.jsonl to your Google Drive\")\n",
    "    print(\"2. Update the path below to match your file location\")\n",
    "    \n",
    "    # Google Driveã®ãƒ‘ã‚¹ã‚’è¨­å®šï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒèª¿æ•´ï¼‰\n",
    "    drive_dataset_path = \"/content/drive/MyDrive/dpo_dataset.jsonl\"  # â† ãƒ‘ã‚¹ã‚’èª¿æ•´\n",
    "    \n",
    "    if os.path.exists(drive_dataset_path):\n",
    "        with open(drive_dataset_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        print(f\"ğŸ“Š Dataset found: {len(lines)} samples\")\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼\n",
    "        !cp \"{drive_dataset_path}\" ./dpo_dataset.jsonl\n",
    "        print(\"âœ… Dataset copied to working directory\")\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "        sample = json.loads(lines[0])\n",
    "        print(\"\\nğŸ” First sample:\")\n",
    "        for key, value in sample.items():\n",
    "            if isinstance(value, str) and len(value) > 100:\n",
    "                print(f\"{key}: {value[:100]}...\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(f\"âŒ Dataset not found at {drive_dataset_path}\")\n",
    "        print(\"Please check the file path or upload the file first.\")\n",
    "else:\n",
    "    print(\"ğŸ“ Google Drive method not enabled. Set USE_GOOGLE_DRIVE = True if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¤ Google Colabã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "# Google Driveã‚’ä½¿ã‚ãšã«ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "UPLOAD_FILE = False  # â† Trueã«å¤‰æ›´ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "if UPLOAD_FILE:\n",
    "    from google.colab import files\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    print(\"ğŸ“¤ Please select your dpo_dataset.jsonl file to upload...\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª\n",
    "    for filename in uploaded.keys():\n",
    "        print(f\"âœ… Uploaded: {filename} ({len(uploaded[filename])} bytes)\")\n",
    "        \n",
    "        if filename.endswith('.jsonl'):\n",
    "            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¡¨ç¤º\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            print(f\"ğŸ“Š Dataset loaded: {len(lines)} samples\")\n",
    "            \n",
    "            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¨™æº–åŒ–\n",
    "            if filename != 'dpo_dataset.jsonl':\n",
    "                os.rename(filename, 'dpo_dataset.jsonl')\n",
    "                print(\"âœ… File renamed to dpo_dataset.jsonl\")\n",
    "            \n",
    "            # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "            sample = json.loads(lines[0])\n",
    "            print(\"\\nğŸ” First sample:\")\n",
    "            for key, value in sample.items():\n",
    "                if isinstance(value, str) and len(value) > 100:\n",
    "                    print(f\"{key}: {value[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"{key}: {value}\")\n",
    "            break\n",
    "else:\n",
    "    print(\"ğŸ“ File upload not enabled. Set UPLOAD_FILE = True if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š æ–¹æ³•3: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç›´æ¥ä½œæˆï¼ˆå°ã•ãªã‚µãƒ³ãƒ—ãƒ«ç”¨ï¼‰\n",
    "# å°‘æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ†ã‚¹ãƒˆã™ã‚‹å ´åˆ\n",
    "\n",
    "CREATE_SAMPLE_DATASET = False  # â† Trueã«å¤‰æ›´ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\n",
    "\n",
    "if CREATE_SAMPLE_DATASET:\n",
    "    import json\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«DPOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆåºƒå‘ŠæŠ€è¡“åˆ†é‡ï¼‰\n",
    "    sample_dataset = [\n",
    "        {\n",
    "            \"prompt\": \"ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®ä¸»è¦ãªãƒ¡ãƒªãƒƒãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
    "            \"chosen\": \"ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®ä¸»è¦ãªãƒ¡ãƒªãƒƒãƒˆã«ã¯ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã®å…¥æœ­æœ€é©åŒ–ã€ç²¾å¯†ãªã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°ã€åŠ¹ç‡çš„ãªåºƒå‘Šè²»ã®æ´»ç”¨ã€ãã—ã¦å¤§è¦æ¨¡ãªã‚¤ãƒ³ãƒ™ãƒ³ãƒˆãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒå«ã¾ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åºƒå‘Šä¸»ã¯ã‚ˆã‚Šé–¢é€£æ€§ã®é«˜ã„ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹ã«ãƒªãƒ¼ãƒã—ã€ROIã‚’æœ€å¤§åŒ–ã§ãã¾ã™ã€‚\",\n",
    "            \"rejected\": \"ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã¯è‡ªå‹•åŒ–ã•ã‚ŒãŸåºƒå‘Šé…ä¿¡ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ä¾¿åˆ©ã§åŠ¹ç‡çš„ã§ã™ã€‚\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"RTBï¼ˆReal-Time Biddingï¼‰ã®ä»•çµ„ã¿ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n",
    "            \"chosen\": \"RTBã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã‚’è¨ªå•ã—ãŸç¬é–“ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§åºƒå‘Šæ ã®ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ãŒè¡Œã‚ã‚Œã‚‹ä»•çµ„ã¿ã§ã™ã€‚DSPï¼ˆDemand Side Platformï¼‰ãŒè‡ªå‹•çš„ã«å…¥æœ­ã‚’è¡Œã„ã€æœ€é«˜ä¾¡æ ¼ã‚’æç¤ºã—ãŸåºƒå‘Šä¸»ã®åºƒå‘ŠãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚ã“ã®éç¨‹ã¯é€šå¸¸100ãƒŸãƒªç§’ä»¥å†…ã§å®Œäº†ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã‚’æãªã†ã“ã¨ãªãæœ€é©ãªåºƒå‘Šãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿç¾ã—ã¾ã™ã€‚\",\n",
    "            \"rejected\": \"RTBã¯åºƒå‘Šã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è²·ã†æ–¹æ³•ã§ã™ã€‚ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³å½¢å¼ã§è¡Œã‚ã‚Œã¾ã™ã€‚\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"DMPã¨CDPã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
    "            \"chosen\": \"DMPï¼ˆData Management Platformï¼‰ã¯ä¸»ã«åŒ¿ååŒ–ã•ã‚ŒãŸã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ã‚’ç®¡ç†ã—ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«ç‰¹åŒ–ã—ãŸãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚ä¸€æ–¹ã€CDPï¼ˆCustomer Data Platformï¼‰ã¯ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒ‘ãƒ¼ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ã‚’ä¸­å¿ƒã¨ã—ãŸé¡§å®¢ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆãƒ»ç®¡ç†ã‚’è¡Œã„ã€å€‹äººãƒ¬ãƒ™ãƒ«ã§ã®é¡§å®¢ä½“é¨“æœ€é©åŒ–ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚CDPã¯ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªé¡§å®¢ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼é‡è¦–ã®è¨­è¨ˆãŒç‰¹å¾´ã§ã™ã€‚\",\n",
    "            \"rejected\": \"DMPã¨CDPã¯ã©ã¡ã‚‰ã‚‚ãƒ‡ãƒ¼ã‚¿ã‚’ç®¡ç†ã™ã‚‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚æ©Ÿèƒ½ã¯ä¼¼ã¦ã„ã¾ã™ã€‚\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\n",
    "    with open('dpo_dataset.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in sample_dataset:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"âœ… Sample dataset created: {len(sample_dataset)} samples\")\n",
    "    print(\"ğŸ“ File saved as: dpo_dataset.jsonl\")\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "    print(\"\\nğŸ” Sample data:\")\n",
    "    for i, item in enumerate(sample_dataset[:2], 1):\n",
    "        print(f\"\\nSample {i}:\")\n",
    "        print(f\"Prompt: {item['prompt'][:80]}...\")\n",
    "        print(f\"Chosen: {item['chosen'][:80]}...\")\n",
    "        print(f\"Rejected: {item['rejected'][:80]}...\")\n",
    "else:\n",
    "    print(\"ğŸ“ Sample dataset creation not enabled. Set CREATE_SAMPLE_DATASET = True if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æº–å‚™\n",
    "# ã©ã®æ–¹æ³•ã§ã‚‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæº–å‚™ã§ããŸã‚‰ã€ã“ã®ã‚»ãƒ«ã§æ¤œè¨¼ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æº–å‚™ã‚’è¡Œã„ã¾ã™\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª\n",
    "dataset_file = 'dpo_dataset.jsonl'\n",
    "\n",
    "if os.path.exists(dataset_file):\n",
    "    print(f\"âœ… Dataset file found: {dataset_file}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è©³ç´°åˆ†æ\n",
    "    with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print(f\"ğŸ“Š Total samples: {len(lines)}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®æ¤œè¨¼\n",
    "    valid_samples = 0\n",
    "    invalid_samples = 0\n",
    "    sample_lengths = {'prompt': [], 'chosen': [], 'rejected': []}\n",
    "    \n",
    "    for i, line in enumerate(lines[:10]):  # æœ€åˆã®10ã‚µãƒ³ãƒ—ãƒ«ã‚’æ¤œè¨¼\n",
    "        try:\n",
    "            data = json.loads(line.strip())\n",
    "            if 'prompt' in data and 'chosen' in data and 'rejected' in data:\n",
    "                valid_samples += 1\n",
    "                sample_lengths['prompt'].append(len(data['prompt']))\n",
    "                sample_lengths['chosen'].append(len(data['chosen']))\n",
    "                sample_lengths['rejected'].append(len(data['rejected']))\n",
    "            else:\n",
    "                invalid_samples += 1\n",
    "                print(f\"âš ï¸ Sample {i+1} missing required fields\")\n",
    "        except json.JSONDecodeError:\n",
    "            invalid_samples += 1\n",
    "            print(f\"âŒ Sample {i+1} invalid JSON format\")\n",
    "    \n",
    "    print(f\"âœ… Valid samples (first 10): {valid_samples}\")\n",
    "    print(f\"âŒ Invalid samples (first 10): {invalid_samples}\")\n",
    "    \n",
    "    if valid_samples > 0:\n",
    "        # å¹³å‡æ–‡å­—æ•°ã‚’è¨ˆç®—\n",
    "        avg_lengths = {\n",
    "            'prompt': sum(sample_lengths['prompt']) / len(sample_lengths['prompt']),\n",
    "            'chosen': sum(sample_lengths['chosen']) / len(sample_lengths['chosen']),\n",
    "            'rejected': sum(sample_lengths['rejected']) / len(sample_lengths['rejected'])\n",
    "        }\n",
    "        \n",
    "        print(\"\\nğŸ“ Average text lengths:\")\n",
    "        for field, avg_len in avg_lengths.items():\n",
    "            print(f\"  {field}: {avg_len:.1f} characters\")\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º\n",
    "        print(\"\\nğŸ” Sample data preview:\")\n",
    "        sample = json.loads(lines[0])\n",
    "        print(f\"Prompt: {sample['prompt'][:100]}...\")\n",
    "        print(f\"Chosen: {sample['chosen'][:100]}...\")\n",
    "        print(f\"Rejected: {sample['rejected'][:100]}...\")\n",
    "        \n",
    "        print(\"\\nğŸ¯ Dataset is ready for DPO training!\")\n",
    "else:\n",
    "    print(f\"âŒ Dataset file not found: {dataset_file}\")\n",
    "    print(\"Please run one of the dataset preparation methods above first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ae089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã«å¿œã˜ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´\n",
    "\n",
    "import os\n",
    "\n",
    "if os.path.exists('dpo_dataset.jsonl'):\n",
    "    with open('dpo_dataset.jsonl', 'r', encoding='utf-8') as f:\n",
    "        num_samples = len(f.readlines())\n",
    "    \n",
    "    print(f\"ğŸ“Š Dataset size: {num_samples} samples\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã«å¿œã˜ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´\n",
    "    if num_samples >= 3000:\n",
    "        # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        training_config = {\n",
    "            'num_train_epochs': 3,\n",
    "            'per_device_train_batch_size': 2,\n",
    "            'gradient_accumulation_steps': 8,\n",
    "            'learning_rate': 5e-7,\n",
    "            'warmup_steps': 100,\n",
    "            'max_steps': 1000,\n",
    "            'eval_steps': 100,\n",
    "            'save_steps': 200,\n",
    "            'max_length': 512,\n",
    "            'max_prompt_length': 256\n",
    "        }\n",
    "        print(\"ğŸ¯ Using parameters optimized for large dataset (3000+ samples)\")\n",
    "    elif num_samples >= 1000:\n",
    "        # ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        training_config = {\n",
    "            'num_train_epochs': 5,\n",
    "            'per_device_train_batch_size': 4,\n",
    "            'gradient_accumulation_steps': 4,\n",
    "            'learning_rate': 1e-6,\n",
    "            'warmup_steps': 50,\n",
    "            'max_steps': 500,\n",
    "            'eval_steps': 50,\n",
    "            'save_steps': 100,\n",
    "            'max_length': 512,\n",
    "            'max_prompt_length': 256\n",
    "        }\n",
    "        print(\"ğŸ¯ Using parameters optimized for medium dataset (1000-3000 samples)\")\n",
    "    else:\n",
    "        # å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        training_config = {\n",
    "            'num_train_epochs': 10,\n",
    "            'per_device_train_batch_size': 8,\n",
    "            'gradient_accumulation_steps': 2,\n",
    "            'learning_rate': 2e-6,\n",
    "            'warmup_steps': 20,\n",
    "            'max_steps': 200,\n",
    "            'eval_steps': 20,\n",
    "            'save_steps': 50,\n",
    "            'max_length': 512,\n",
    "            'max_prompt_length': 256\n",
    "        }\n",
    "        print(\"ğŸ¯ Using parameters optimized for small dataset (<1000 samples)\")\n",
    "    \n",
    "    print(\"\\nğŸ“‹ Training Configuration:\")\n",
    "    for key, value in training_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # æ¨å®šãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“\n",
    "    estimated_time_minutes = (training_config['max_steps'] * training_config['per_device_train_batch_size'] * 0.5) / 60\n",
    "    print(f\"\\nâ° Estimated training time: {estimated_time_minutes:.1f} minutes\")\n",
    "    \n",
    "    # GPUä½¿ç”¨é‡ã®è­¦å‘Š\n",
    "    if training_config['per_device_train_batch_size'] >= 4:\n",
    "        print(\"\\nâš ï¸  Warning: Large batch size may cause GPU memory issues.\")\n",
    "        print(\"   If you encounter CUDA out of memory errors, reduce batch_size to 1-2.\")\n",
    "else:\n",
    "    print(\"âŒ Please prepare the dataset first using one of the methods above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¤– DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ\n",
    "# å®Ÿéš›ã®DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™\n",
    "\n",
    "RUN_DPO_TRAINING = False  # â† Trueã«å¤‰æ›´ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹\n",
    "\n",
    "if RUN_DPO_TRAINING:\n",
    "    import torch\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        TrainingArguments\n",
    "    )\n",
    "    from trl import DPOTrainer\n",
    "    from datasets import Dataset\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    # GPUç¢ºèª\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"ğŸ”§ Using device: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ\n",
    "    model_options = {\n",
    "        \"1\": {\n",
    "            \"name\": \"microsoft/DialoGPT-small\",\n",
    "            \"description\": \"Small conversational model (117M params) - Fast training\"\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"name\": \"rinna/japanese-gpt2-small\",\n",
    "            \"description\": \"Japanese GPT-2 Small (110M params) - Japanese optimized\"\n",
    "        },\n",
    "        \"3\": {\n",
    "            \"name\": \"elyza/ELYZA-japanese-Llama-2-7b-fast-instruct\",\n",
    "            \"description\": \"Japanese Llama-2 7B - High quality but requires more GPU memory\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nğŸ¤– Available base models:\")\n",
    "    for key, model in model_options.items():\n",
    "        print(f\"  {key}. {model['name']}\")\n",
    "        print(f\"     {model['description']}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§GPT-2 Smallã‚’ä½¿ç”¨ï¼ˆGPU ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼‰\n",
    "    selected_model = model_options[\"2\"][\"name\"]\n",
    "    print(f\"\\nâœ… Selected model: {selected_model}\")\n",
    "    \n",
    "    try:\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "        print(\"ğŸ“¥ Loading tokenizer and model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(selected_model)\n",
    "        \n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            selected_model,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Model and tokenizer loaded successfully\")\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
    "        print(\"ğŸ“Š Loading dataset...\")\n",
    "        dataset_list = []\n",
    "        \n",
    "        with open('dpo_dataset.jsonl', 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                dataset_list.append({\n",
    "                    'prompt': data['prompt'],\n",
    "                    'chosen': data['chosen'],\n",
    "                    'rejected': data['rejected']\n",
    "                })\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\n",
    "        dataset = Dataset.from_list(dataset_list)\n",
    "        \n",
    "        # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²\n",
    "        split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        train_dataset = split_dataset['train']\n",
    "        eval_dataset = split_dataset['test']\n",
    "        \n",
    "        print(f\"âœ… Dataset loaded: {len(train_dataset)} train, {len(eval_dataset)} eval samples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model or dataset: {str(e)}\")\n",
    "        print(\"Please check your model selection and dataset file.\")\n",
    "        RUN_DPO_TRAINING = False\n",
    "else:\n",
    "    print(\"ğŸ“ DPO training not enabled. Set RUN_DPO_TRAINING = True to start training.\")\n",
    "    print(\"\\nâš ï¸  Before starting training:\")\n",
    "    print(\"   1. Ensure your dataset is prepared and validated\")\n",
    "    print(\"   2. Check GPU memory availability\")\n",
    "    print(\"   3. Review training parameters above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œã®ç¶šã\n",
    "# å‰ã®ã‚»ãƒ«ã§ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå ´åˆã«ã®ã¿å®Ÿè¡Œ\n",
    "\n",
    "if RUN_DPO_TRAINING and 'model' in locals() and 'train_dataset' in locals():\n",
    "    try:\n",
    "        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã®è¨­å®š\n",
    "        print(\"âš™ï¸ Setting up training arguments...\")\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./dpo_output\",\n",
    "            num_train_epochs=training_config['num_train_epochs'],\n",
    "            per_device_train_batch_size=training_config['per_device_train_batch_size'],\n",
    "            per_device_eval_batch_size=training_config['per_device_train_batch_size'],\n",
    "            gradient_accumulation_steps=training_config['gradient_accumulation_steps'],\n",
    "            learning_rate=training_config['learning_rate'],\n",
    "            warmup_steps=training_config['warmup_steps'],\n",
    "            max_steps=training_config['max_steps'],\n",
    "            eval_steps=training_config['eval_steps'],\n",
    "            save_steps=training_config['save_steps'],\n",
    "            logging_steps=20,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            report_to=None,  # Wandbãªã©ã®ãƒ­ã‚®ãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–\n",
    "            remove_unused_columns=False,\n",
    "            dataloader_pin_memory=False,\n",
    "            fp16=torch.cuda.is_available(),  # GPUä½¿ç”¨æ™‚ã®ã¿fp16ã‚’æœ‰åŠ¹åŒ–\n",
    "        )\n",
    "        \n",
    "        # DPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–\n",
    "        print(\"ğŸ¯ Initializing DPO trainer...\")\n",
    "        \n",
    "        dpo_trainer = DPOTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=training_config['max_length'],\n",
    "            max_prompt_length=training_config['max_prompt_length'],\n",
    "            beta=0.1,  # DPOæ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… DPO trainer initialized successfully\")\n",
    "        \n",
    "        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹\n",
    "        print(\"\\nğŸš€ Starting DPO training...\")\n",
    "        print(f\"   Training samples: {len(train_dataset)}\")\n",
    "        print(f\"   Validation samples: {len(eval_dataset)}\")\n",
    "        print(f\"   Max steps: {training_config['max_steps']}\")\n",
    "        print(f\"   Estimated time: {estimated_time_minutes:.1f} minutes\")\n",
    "        print(\"\\n   Training will start now. You can monitor progress below.\")\n",
    "        \n",
    "        # å®Ÿéš›ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
    "        dpo_trainer.train()\n",
    "        \n",
    "        print(\"\\nğŸ‰ DPO training completed successfully!\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "        print(\"ğŸ’¾ Saving the trained model...\")\n",
    "        dpo_trainer.save_model(\"./dpo_final_model\")\n",
    "        tokenizer.save_pretrained(\"./dpo_final_model\")\n",
    "        \n",
    "        print(\"âœ… Model saved to: ./dpo_final_model\")\n",
    "        \n",
    "        # è¨“ç·´çµ±è¨ˆ\n",
    "        if hasattr(dpo_trainer.state, 'log_history'):\n",
    "            logs = dpo_trainer.state.log_history\n",
    "            if logs:\n",
    "                final_train_loss = None\n",
    "                final_eval_loss = None\n",
    "                \n",
    "                for log in reversed(logs):\n",
    "                    if 'train_loss' in log and final_train_loss is None:\n",
    "                        final_train_loss = log['train_loss']\n",
    "                    if 'eval_loss' in log and final_eval_loss is None:\n",
    "                        final_eval_loss = log['eval_loss']\n",
    "                    if final_train_loss is not None and final_eval_loss is not None:\n",
    "                        break\n",
    "                \n",
    "                print(\"\\nğŸ“Š Training Results:\")\n",
    "                if final_train_loss:\n",
    "                    print(f\"   Final training loss: {final_train_loss:.4f}\")\n",
    "                if final_eval_loss:\n",
    "                    print(f\"   Final validation loss: {final_eval_loss:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training failed: {str(e)}\")\n",
    "        print(\"\\nPossible solutions:\")\n",
    "        print(\"   1. Reduce batch_size in training configuration\")\n",
    "        print(\"   2. Reduce max_length or max_prompt_length\")\n",
    "        print(\"   3. Try a smaller model\")\n",
    "        print(\"   4. Restart runtime and try again\")\n",
    "elif RUN_DPO_TRAINING:\n",
    "    print(\"âŒ Cannot start training: Model or dataset not properly loaded\")\n",
    "    print(\"Please run the previous cells first.\")\n",
    "else:\n",
    "    print(\"ğŸ“ Training not started. Set RUN_DPO_TRAINING = True in the previous cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ\n",
    "# DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†å¾Œã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™\n",
    "\n",
    "TEST_TRAINED_MODEL = False  # â† Trueã«å¤‰æ›´ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "if TEST_TRAINED_MODEL:\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import os\n",
    "    \n",
    "    model_path = \"./dpo_final_model\"\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        print(\"ğŸ¤– Loading trained DPO model...\")\n",
    "        \n",
    "        try:\n",
    "            # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "            )\n",
    "            \n",
    "            print(\"âœ… Model loaded successfully\")\n",
    "            \n",
    "            # ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆåºƒå‘ŠæŠ€è¡“åˆ†é‡ï¼‰\n",
    "            test_prompts = [\n",
    "                \"ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®ä¸»è¦ãªãƒ¡ãƒªãƒƒãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
    "                \"RTBï¼ˆReal-Time Biddingï¼‰ã®ä»•çµ„ã¿ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n",
    "                \"DMPã¨CDPã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
    "                \"åºƒå‘Šé…ä¿¡ã«ãŠã‘ã‚‹æ©Ÿæ¢°å­¦ç¿’ã®æ´»ç”¨æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\",\n",
    "                \"Cookieå»ƒæ­¢ãŒåºƒå‘Šæ¥­ç•Œã«ä¸ãˆã‚‹å½±éŸ¿ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\"\n",
    "            ]\n",
    "            \n",
    "            print(\"\\nğŸ§ª Testing the trained model:\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            for i, prompt in enumerate(test_prompts, 1):\n",
    "                print(f\"\\nğŸ” Test {i}: {prompt}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
    "                inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        inputs,\n",
    "                        max_length=inputs.shape[1] + 150,\n",
    "                        num_return_sequences=1,\n",
    "                        temperature=0.7,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        no_repeat_ngram_size=2\n",
    "                    )\n",
    "                \n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                response = generated_text[len(prompt):].strip()\n",
    "                \n",
    "                print(f\"ğŸ“ Response: {response}\")\n",
    "                \n",
    "                if i < len(test_prompts):  # æœ€å¾Œã§ãªã„å ´åˆ\n",
    "                    print()\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ğŸ‰ Model testing completed!\")\n",
    "            print(\"\\nğŸ’¡ Tips for evaluation:\")\n",
    "            print(\"   â€¢ Compare responses with original model\")\n",
    "            print(\"   â€¢ Check for relevant technical terminology\")\n",
    "            print(\"   â€¢ Assess factual accuracy and coherence\")\n",
    "            print(\"   â€¢ Look for improved domain-specific knowledge\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading or testing model: {str(e)}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"âŒ Trained model not found at {model_path}\")\n",
    "        print(\"Please complete the DPO training first.\")\n",
    "else:\n",
    "    print(\"ğŸ“ Model testing not enabled. Set TEST_TRAINED_MODEL = True to test the model.\")\n",
    "    print(\"\\nğŸ“‹ To test the model:\")\n",
    "    print(\"   1. Complete DPO training first\")\n",
    "    print(\"   2. Set TEST_TRAINED_MODEL = True\")\n",
    "    print(\"   3. Run this cell to see model responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa32925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¿å­˜ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰\n",
    "\n",
    "SAVE_PROJECT = False  # â† Trueã«å¤‰æ›´ã—ã¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜\n",
    "\n",
    "if SAVE_PROJECT:\n",
    "    from google.colab import files\n",
    "    import zipfile\n",
    "    import os\n",
    "    \n",
    "    print(\"ğŸ’¾ Preparing project files for download...\")\n",
    "    \n",
    "    # ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ\n",
    "    files_to_save = []\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«\n",
    "    if os.path.exists('dpo_dataset.jsonl'):\n",
    "        files_to_save.append('dpo_dataset.jsonl')\n",
    "    \n",
    "    # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«\n",
    "    if os.path.exists('./dpo_final_model'):\n",
    "        model_files = []\n",
    "        for root, dirs, files_list in os.walk('./dpo_final_model'):\n",
    "            for file in files_list:\n",
    "                file_path = os.path.join(root, file)\n",
    "                model_files.append(file_path)\n",
    "        files_to_save.extend(model_files)\n",
    "    \n",
    "    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ­ã‚°\n",
    "    if os.path.exists('./dpo_output'):\n",
    "        for root, dirs, files_list in os.walk('./dpo_output'):\n",
    "            for file in files_list:\n",
    "                if file.endswith(('.json', '.log', '.txt')):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    files_to_save.append(file_path)\n",
    "    \n",
    "    if files_to_save:\n",
    "        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\n",
    "        zip_filename = 'dpo_training_project.zip'\n",
    "        \n",
    "        with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "            for file_path in files_to_save:\n",
    "                if os.path.exists(file_path):\n",
    "                    # ZIPãƒ•ã‚¡ã‚¤ãƒ«å†…ã§ã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’ä¿æŒ\n",
    "                    arcname = file_path.replace('./', '')\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    print(f\"  âœ… Added: {file_path}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“¦ Created archive: {zip_filename}\")\n",
    "        print(f\"   Total files: {len(files_to_save)}\")\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "        print(\"\\nâ¬‡ï¸ Downloading project archive...\")\n",
    "        files.download(zip_filename)\n",
    "        \n",
    "        print(\"âœ… Download started! Check your Downloads folder.\")\n",
    "    else:\n",
    "        print(\"âŒ No project files found to save.\")\n",
    "        print(\"Please complete the training process first.\")\n",
    "else:\n",
    "    print(\"ğŸ“ Project saving not enabled. Set SAVE_PROJECT = True to download files.\")\n",
    "\n",
    "# æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ“‹ å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯:\")\n",
    "if os.path.exists('dpo_dataset.jsonl'):\n",
    "    print(\"   âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\")\n",
    "else:\n",
    "    print(\"   âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\")\n",
    "\n",
    "if os.path.exists('./dpo_final_model'):\n",
    "    print(\"   âœ… DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ\")\n",
    "else:\n",
    "    print(\"   âŒ DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ\")\n",
    "\n",
    "if 'dpo_trainer' in locals():\n",
    "    print(\"   âœ… ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ\")\n",
    "else:\n",
    "    print(\"   âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ\")\n",
    "\n",
    "print(\"\\nğŸš€ æ¨å¥¨ã•ã‚Œã‚‹æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:\")\n",
    "print(\"\\n1. ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®æ”¹å–„:\")\n",
    "print(\"   â€¢ ã‚ˆã‚Šå¤šæ§˜ãªãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®è©•ä¾¡\")\n",
    "print(\"   â€¢ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ\")\n",
    "print(\"   â€¢ äººé–“ã«ã‚ˆã‚‹å“è³ªè©•ä¾¡\")\n",
    "\n",
    "print(\"\\n2. ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–:\")\n",
    "print(\"   â€¢ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\")\n",
    "print(\"   â€¢ ã‚ˆã‚Šå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\")\n",
    "print(\"   â€¢ ç•°ãªã‚‹ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã®å®Ÿé¨“\")\n",
    "\n",
    "print(\"\\n3. ğŸŒ æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹:\")\n",
    "print(\"   â€¢ ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ãƒ³ã‚°APIã®æ§‹ç¯‰\")\n",
    "print(\"   â€¢ æ¨è«–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æœ€é©åŒ–\")\n",
    "print(\"   â€¢ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã®è€ƒæ…®\")\n",
    "\n",
    "print(\"\\n4. ğŸ“ˆ ç¶™ç¶šçš„æ”¹å–„:\")\n",
    "print(\"   â€¢ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®åé›†\")\n",
    "print(\"   â€¢ è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\")\n",
    "print(\"   â€¢ ãƒ¢ãƒ‡ãƒ«å“è³ªã®ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ Google Colabã§ã®DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œäº†!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cade523",
   "metadata": {},
   "source": [
    "## ğŸš¨ ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã®ã‚¯ã‚¤ãƒƒã‚¯ãƒ•ã‚£ãƒƒã‚¯ã‚¹\n",
    "\n",
    "Google Colabã§ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆï¼š\n",
    "- `fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1`\n",
    "- `gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0`\n",
    "\n",
    "**è§£æ±ºæ‰‹é †ï¼š**\n",
    "1. **Runtime â†’ Restart runtime** ã‚’ã‚¯ãƒªãƒƒã‚¯\n",
    "2. ä¸‹è¨˜ã®ã‚¯ã‚¤ãƒƒã‚¯ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚»ãƒ«ã‚’å®Ÿè¡Œ\n",
    "3. å†åº¦ãƒ¡ã‚¤ãƒ³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ãƒ«ã‚’å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš¨ ç·Šæ€¥ã‚¯ã‚¤ãƒƒã‚¯ãƒ•ã‚£ãƒƒã‚¯ã‚¹ (ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼å°‚ç”¨)\n",
    "# ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã®ã¿å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "\n",
    "QUICK_FIX = False  # â† Trueã«å¤‰æ›´ã—ã¦å®Ÿè¡Œ\n",
    "\n",
    "if QUICK_FIX:\n",
    "    print(\"ğŸš¨ Running emergency dependency fix...\")\n",
    "    \n",
    "    # å•é¡Œã®ã‚ã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’å¼·åˆ¶å‰Šé™¤\n",
    "    !pip uninstall -y fastai torch gcsfs fsspec --quiet\n",
    "    \n",
    "    # é©åˆ‡ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    !pip install 'torch>=2.0,<2.7' --index-url https://download.pytorch.org/whl/cu118\n",
    "    !pip install fsspec==2025.3.2 gcsfs==2025.3.2\n",
    "    !pip install 'fastai>=2.7.0'\n",
    "    \n",
    "    print(\"âœ… Quick fix completed. Now run the main setup cell below.\")\n",
    "else:\n",
    "    print(\"ğŸ“ Quick fix not enabled. Set QUICK_FIX = True if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c48f00f",
   "metadata": {},
   "source": [
    "# ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Google Colabã§ä½¿ç”¨ã™ã‚‹æ–¹æ³•\n",
    "\n",
    "æ—¢å­˜ã®é«˜å“è³ªãªDPOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ3,566ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã‚’Google Colabã§ä½¿ç”¨ã™ã‚‹ãŸã‚ã®3ã¤ã®æ–¹æ³•ã‚’æä¾›ã—ã¾ã™ï¼š\n",
    "\n",
    "## æ–¹æ³•1: GitHubçµŒç”±ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—\n",
    "## æ–¹æ³•2: Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½¿ç”¨\n",
    "## æ–¹æ³•3: ç›´æ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚³ãƒ”ãƒ¼&ãƒšãƒ¼ã‚¹ãƒˆ\n",
    "\n",
    "ä»¥ä¸‹ã®ã‚»ãƒ«ã‹ã‚‰é©åˆ‡ãªæ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56ccac",
   "metadata": {},
   "source": [
    "# ğŸš€ DPO Training on Google Colab - ç›´æ¥é¸å¥½æœ€é©åŒ–\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/dpo-colab)\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Google Colabã§DPOï¼ˆDirect Preference Optimizationï¼‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "\n",
    "## âœ¨ ç‰¹å¾´\n",
    "- ğŸ”¥ GPUåŠ é€Ÿãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ç›£è¦–\n",
    "- ğŸ’¾ è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜\n",
    "- ğŸ¯ åŠ¹ç‡çš„ãªLoRAå¾®èª¿æ•´\n",
    "- ğŸ“ˆ çµæœã®å¯è¦–åŒ–\n",
    "\n",
    "## âš™ï¸ æ¨å¥¨ç’°å¢ƒ\n",
    "- Google Colab Pro (GPU: T4/V100)\n",
    "- ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚¿ã‚¤ãƒ—: GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3952e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆä¾å­˜é–¢ä¿‚ã®ç«¶åˆã‚’å›é¿ï¼‰\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Colabç’°å¢ƒãƒã‚§ãƒƒã‚¯\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"ğŸƒâ€â™‚ï¸ Running in Google Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“¦ Installing required packages with dependency resolution...\")\n",
    "    \n",
    "    # äº‹å‰ã«å•é¡Œã®ã‚ã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    print(\"Step 1: Cleaning existing conflicting packages...\")\n",
    "    !pip uninstall -y fastai gcsfs fsspec torch -q\n",
    "    \n",
    "    print(\"Step 2: Installing core dependencies with specific versions...\")\n",
    "    !pip install -q --upgrade pip setuptools wheel\n",
    "    \n",
    "    # PyTorchã‚’äº’æ›æ€§ã®ã‚ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å›ºå®š\n",
    "    print(\"Step 3: Installing PyTorch with version constraint...\")\n",
    "    !pip install 'torch<2.7,>=2.0' torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    \n",
    "    # fsspecã¨gcsfsã‚’æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§çµ±ä¸€\n",
    "    print(\"Step 4: Installing file system packages...\")\n",
    "    !pip install -q fsspec==2025.3.2 gcsfs==2025.3.2\n",
    "    \n",
    "    print(\"Step 5: Installing transformers ecosystem...\")\n",
    "    !pip install -q transformers datasets accelerate tokenizers\n",
    "    \n",
    "    print(\"Step 6: Installing TRL and PEFT...\")\n",
    "    !pip install -q trl peft\n",
    "    \n",
    "    print(\"Step 7: Installing additional ML packages...\")\n",
    "    !pip install -q evaluate bitsandbytes matplotlib pandas numpy\n",
    "    \n",
    "    # fastaiã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆé©åˆ‡ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ï¼‰\n",
    "    print(\"Step 8: Reinstalling fastai with correct torch version...\")\n",
    "    !pip install -q 'fastai>=2.7.0'\n",
    "    \n",
    "    print(\"âœ… Installation completed with dependency resolution!\")\n",
    "    \n",
    "    # è­¦å‘Šã¨ã‚¨ãƒ©ãƒ¼ã‚’æœ€å°åŒ–\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # æœ€çµ‚çš„ãªä¾å­˜é–¢ä¿‚ç¢ºèª\n",
    "    print(\"\\nğŸ” Verifying critical package versions:\")\n",
    "    import pkg_resources\n",
    "    critical_packages = ['torch', 'transformers', 'trl', 'peft', 'fastai']\n",
    "    for pkg in critical_packages:\n",
    "        try:\n",
    "            version = pkg_resources.get_distribution(pkg).version\n",
    "            print(f\"  âœ… {pkg}: {version}\")\n",
    "        except:\n",
    "            print(f\"  âŒ {pkg}: Not properly installed\")\n",
    "\n",
    "# GPUç¢ºèª\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\nğŸš€ GPU Available: {gpu_name}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"ğŸ”¥ PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No GPU available. Please enable GPU in Runtime > Change runtime type\")\n",
    "    print(\"ğŸ’¡ Go to Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b554cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°: è©³ç´°ãªä¾å­˜é–¢ä¿‚ç¢ºèª\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ” Comprehensive package version check...\")\n",
    "    \n",
    "    import subprocess\n",
    "    import pkg_resources\n",
    "    \n",
    "    # é‡è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®è©³ç´°ç¢ºèª\n",
    "    packages_to_check = {\n",
    "        'torch': 'PyTorch (Should be <2.7, >=2.0)',\n",
    "        'transformers': 'Hugging Face Transformers',\n",
    "        'trl': 'Transformer Reinforcement Learning',\n",
    "        'peft': 'Parameter Efficient Fine-tuning',\n",
    "        'datasets': 'Hugging Face Datasets',\n",
    "        'accelerate': 'Hugging Face Accelerate',\n",
    "        'fsspec': 'File System Specification (Should be 2025.3.2)',\n",
    "        'gcsfs': 'Google Cloud Storage File System (Should be 2025.3.2)',\n",
    "        'fastai': 'FastAI (Should work with current torch)'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nğŸ“Š Package Status:\")\n",
    "    conflicts_found = False\n",
    "    \n",
    "    for package, description in packages_to_check.items():\n",
    "        try:\n",
    "            version = pkg_resources.get_distribution(package).version\n",
    "            status = \"âœ…\"\n",
    "            \n",
    "            # ç‰¹å®šã®ç«¶åˆã‚’ãƒã‚§ãƒƒã‚¯\n",
    "            if package == 'torch' and version >= '2.7':\n",
    "                status = \"âš ï¸ \"\n",
    "                conflicts_found = True\n",
    "            elif package in ['fsspec', 'gcsfs'] and not version.startswith('2025.3'):\n",
    "                status = \"âš ï¸ \"\n",
    "                conflicts_found = True\n",
    "                \n",
    "            print(f\"  {status} {package}: {version} - {description}\")\n",
    "        except pkg_resources.DistributionNotFound:\n",
    "            print(f\"  âŒ {package}: Not installed - {description}\")\n",
    "            conflicts_found = True\n",
    "    \n",
    "    if conflicts_found:\n",
    "        print(\"\\nğŸ”§ Dependency conflicts detected! To fix:\")\n",
    "        print(\"   1. Runtime â†’ Restart runtime\")\n",
    "        print(\"   2. Re-run the installation cell above\")\n",
    "        print(\"   3. If issues persist, enable ALTERNATIVE_INSTALL in the next cell\")\n",
    "    else:\n",
    "        print(\"\\nğŸ‰ All dependencies are compatible!\")\n",
    "\n",
    "    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±\n",
    "    print(\"\\nğŸ–¥ï¸ System Information:\")\n",
    "    print(f\"  Python: {sys.version.split()[0]}\")\n",
    "    print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"  GPU Count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”„ ä»£æ›¿ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹æ³• (ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆ)\n",
    "\n",
    "# ä¸Šè¨˜ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ fastai ã‚„ fsspec ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ã“ã¡ã‚‰ã‚’å®Ÿè¡Œ\n",
    "# âš ï¸ ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã€ã—ã¦ãã ã•ã„\n",
    "\n",
    "ALTERNATIVE_INSTALL = False  # â† Trueã«å¤‰æ›´ã—ã¦å®Ÿè¡Œ\n",
    "\n",
    "if ALTERNATIVE_INSTALL and IN_COLAB:\n",
    "    print(\"ğŸ”„ Running alternative installation with strict version management...\")\n",
    "    \n",
    "    # å®Œå…¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "    print(\"Step 1: Complete package cleanup...\")\n",
    "    !pip uninstall -y fastai fastbook torch torchvision torchaudio gcsfs fsspec transformers trl peft -q\n",
    "    \n",
    "    # åŸºç›¤ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰é †ç•ªã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    print(\"Step 2: Installing PyTorch with fastai compatibility...\")\n",
    "    !pip install 'torch>=1.10,<2.7' 'torchvision' 'torchaudio' --index-url https://download.pytorch.org/whl/cu118\n",
    "    \n",
    "    print(\"Step 3: Installing file system packages with exact versions...\")\n",
    "    !pip install fsspec==2025.3.2 gcsfs==2025.3.2\n",
    "    \n",
    "    print(\"Step 4: Installing fastai first (to avoid conflicts)...\")\n",
    "    !pip install 'fastai>=2.7.0,<2.8'\n",
    "    \n",
    "    print(\"Step 5: Installing transformers ecosystem...\")\n",
    "    !pip install transformers datasets accelerate tokenizers\n",
    "    \n",
    "    print(\"Step 6: Installing DPO training packages...\")\n",
    "    !pip install trl peft evaluate\n",
    "    \n",
    "    print(\"Step 7: Installing additional utilities...\")\n",
    "    !pip install matplotlib pandas numpy bitsandbytes\n",
    "    \n",
    "    print(\"\\nâœ… Alternative installation completed!\")\n",
    "    print(\"ğŸ“ Verifying installation...\")\n",
    "    \n",
    "    # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª\n",
    "    try:\n",
    "        import torch\n",
    "        import transformers\n",
    "        import trl\n",
    "        import peft\n",
    "        import fastai\n",
    "        print(\"ğŸ‰ All critical packages imported successfully!\")\n",
    "        print(f\"   PyTorch: {torch.__version__}\")\n",
    "        print(f\"   Transformers: {transformers.__version__}\")\n",
    "        print(f\"   FastAI: {fastai.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ Import error: {e}\")\n",
    "        print(\"ğŸ’¡ Please restart runtime and try again\")\n",
    "\n",
    "elif ALTERNATIVE_INSTALL:\n",
    "    print(\"ğŸ“ This alternative method is only for Google Colab.\")\n",
    "else:\n",
    "    print(\"ğŸ“ Alternative installation not enabled.\")\n",
    "    print(\"     Set ALTERNATIVE_INSTALL = True if you encounter dependency conflicts.\")\n",
    "    print(\"     Remember to restart runtime first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50383597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š DPOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "def create_dpo_dataset(size: int = 1000) -> List[Dict]:\n",
    "    \"\"\"é«˜å“è³ªãªDPOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ\"\"\"\n",
    "    \n",
    "    templates = [\n",
    "        {\n",
    "            \"prompt\": \"ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„: {question}\",\n",
    "            \"chosen\": \"{topic}ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚{detail}ã“ã‚Œã«ã‚ˆã‚Šã€{benefit}ãŒæœŸå¾…ã§ãã¾ã™ã€‚\",\n",
    "            \"rejected\": \"ãã®è³ªå•ã¯è¤‡é›‘ã§ã™ã­ã€‚æ§˜ã€…ãªè¦å› ãŒã‚ã‚Šã¾ã™ã€‚\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"æ¬¡ã®æ–‡ç« ã‚’è¦ç´„ã—ã¦ãã ã•ã„: {text}\",\n",
    "            \"chosen\": \"ã“ã®æ–‡ç« ã®ãƒã‚¤ãƒ³ãƒˆã¯{point}ã§ã™ã€‚å…·ä½“çš„ã«ã¯{detail}ã«ã¤ã„ã¦è¿°ã¹ã¦ã„ã¾ã™ã€‚\",\n",
    "            \"rejected\": \"æ–‡ç« ãŒé•·ãã¦è¤‡é›‘ãªãŸã‚ã€è¦ç´„ã¯å›°é›£ã§ã™ã€‚\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„: {topic}\",\n",
    "            \"chosen\": \"{topic}ã¯{definition}ã§ã™ã€‚ä¸»ãªç‰¹å¾´ã¨ã—ã¦{feature}ãŒã‚ã‚Šã€{usage}ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\",\n",
    "            \"rejected\": \"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯æŠ€è¡“çš„ã§èª¬æ˜ãŒé›£ã—ã„ã§ã™ã€‚\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    topics = [\"æ©Ÿæ¢°å­¦ç¿’\", \"ã‚¦ã‚§ãƒ–é–‹ç™º\", \"ãƒ‡ãƒ¼ã‚¿åˆ†æ\", \"äººå·¥çŸ¥èƒ½\", \"ã‚¯ãƒ©ã‚¦ãƒ‰\"]\n",
    "    questions = [\"AIã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\", \"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åŸºæœ¬ã¯ï¼Ÿ\", \"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã¨ã¯ï¼Ÿ\"]\n",
    "    \n",
    "    dataset = []\n",
    "    for i in range(size):\n",
    "        template = random.choice(templates)\n",
    "        topic = random.choice(topics)\n",
    "        question = random.choice(questions)\n",
    "        \n",
    "        if \"{question}\" in template[\"prompt\"]:\n",
    "            prompt = template[\"prompt\"].format(question=question)\n",
    "            chosen = template[\"chosen\"].format(\n",
    "                topic=topic,\n",
    "                detail=f\"{topic}ã®é‡è¦ãªæ¦‚å¿µ\",\n",
    "                benefit=\"åŠ¹ç‡çš„ãªå•é¡Œè§£æ±º\"\n",
    "            )\n",
    "        elif \"{text}\" in template[\"prompt\"]:\n",
    "            prompt = template[\"prompt\"].format(text=f\"{topic}ã«é–¢ã™ã‚‹è©³ç´°ãªèª¬æ˜\")\n",
    "            chosen = template[\"chosen\"].format(\n",
    "                point=f\"{topic}ã®æ´»ç”¨\",\n",
    "                detail=\"å®Ÿç”¨çš„ãªå¿œç”¨ä¾‹\"\n",
    "            )\n",
    "        else:\n",
    "            prompt = template[\"prompt\"].format(topic=topic)\n",
    "            chosen = template[\"chosen\"].format(\n",
    "                topic=topic,\n",
    "                definition=\"é‡è¦ãªæŠ€è¡“åˆ†é‡\",\n",
    "                feature=\"é«˜ã„åŠ¹ç‡æ€§\",\n",
    "                usage=\"æ§˜ã€…ãªæ¥­ç•Œ\"\n",
    "            )\n",
    "        \n",
    "        dataset.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": chosen,\n",
    "            \"rejected\": template[\"rejected\"]\n",
    "        })\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ\n",
    "print(\"ğŸ”„ Generating DPO dataset...\")\n",
    "data = create_dpo_dataset(1000)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"âœ… Generated {len(df)} samples\")\n",
    "print(f\"ğŸ“Š Columns: {list(df.columns)}\")\n",
    "print(\"\\nğŸ” Sample data:\")\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70450792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è¨­å®š\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Colab GPUç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸè»½é‡ãƒ¢ãƒ‡ãƒ«\n",
    "model_name = \"microsoft/DialoGPT-small\"\n",
    "\n",
    "print(f\"ğŸ“¥ Loading model: {model_name}\")\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"âœ… Model and tokenizer loaded\")\n",
    "print(f\"ğŸ“Š Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca373a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ LoRAè¨­å®šï¼ˆåŠ¹ç‡çš„ãªå¾®èª¿æ•´ï¼‰\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,  # Low rank for efficiency\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# LoRAãƒ¢ãƒ‡ãƒ«ä½œæˆ\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ”¥ GPU memory usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†\n",
    "from datasets import Dataset\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "# Datasetå¤‰æ›\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset_split['train']\n",
    "eval_dataset = dataset_split['test']\n",
    "\n",
    "print(f\"ğŸ¯ Training samples: {len(train_dataset)}\")\n",
    "print(f\"ğŸ¯ Evaluation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª\n",
    "print(\"\\nğŸ“ Sample training data:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdeaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã¨å®Ÿè¡Œ\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"./dpo_results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-6,\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    max_length=128,\n",
    "    max_prompt_length=64,\n",
    "    report_to=[\"none\"]  # Disable wandb\n",
    ")\n",
    "\n",
    "# DPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    beta=0.1\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸ DPO Trainer initialized\")\n",
    "print(\"ğŸ”¥ Starting training... This may take 10-15 minutes\")\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
    "train_result = dpo_trainer.train()\n",
    "\n",
    "print(\"\\nğŸ‰ Training completed!\")\n",
    "print(f\"ğŸ“Š Final training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ çµæœã®è©•ä¾¡ã¨å¯è¦–åŒ–\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# æœ€çµ‚è©•ä¾¡\n",
    "eval_results = dpo_trainer.evaluate()\n",
    "print(\"ğŸ“Š Final Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å±¥æ­´ã®å¯è¦–åŒ–\n",
    "if hasattr(dpo_trainer.state, 'log_history'):\n",
    "    log_history = dpo_trainer.state.log_history\n",
    "    \n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    steps = []\n",
    "    \n",
    "    for log in log_history:\n",
    "        if 'loss' in log:\n",
    "            train_losses.append(log['loss'])\n",
    "            steps.append(log.get('step', len(train_losses)))\n",
    "        if 'eval_loss' in log:\n",
    "            eval_losses.append(log['eval_loss'])\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(steps, train_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "    plt.title('Training Loss Progress', fontsize=14)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if eval_losses:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        eval_steps = np.linspace(0, max(steps), len(eval_losses))\n",
    "        plt.plot(eval_steps, eval_losses, 'r-', linewidth=2, label='Evaluation Loss')\n",
    "        plt.title('Evaluation Loss Progress', fontsize=14)\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965113a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ\n",
    "def generate_response(prompt, max_length=100):\n",
    "    \"\"\"DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»\n",
    "    if response.startswith(prompt):\n",
    "        response = response[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "test_prompts = [\n",
    "    \"ä»¥ä¸‹ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„: æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"æ¬¡ã®æ–‡ç« ã‚’è¦ç´„ã—ã¦ãã ã•ã„: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«é–¢ã™ã‚‹è©³ç´°ãªèª¬æ˜\",\n",
    "    \"ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„: Python\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ”¬ Testing DPO-trained model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"\\nğŸ“ Test {i}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fccb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"ğŸ’¾ Saving trained model...\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "dpo_trainer.save_model(\"./final_dpo_model\")\n",
    "tokenizer.save_pretrained(\"./final_dpo_model\")\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª\n",
    "import os\n",
    "def get_folder_size(folder_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size\n",
    "\n",
    "model_size = get_folder_size('./final_dpo_model')\n",
    "print(f\"ğŸ“Š Model size: {model_size / 1e6:.1f} MB\")\n",
    "\n",
    "# Google Driveã«ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    import shutil\n",
    "    drive_path = '/content/drive/MyDrive/dpo_trained_model'\n",
    "    shutil.copytree('./final_dpo_model', drive_path, dirs_exist_ok=True)\n",
    "    print(f\"â˜ï¸ Model saved to Google Drive: {drive_path}\")\n",
    "\n",
    "# ZIPå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æº–å‚™\n",
    "!zip -r dpo_model.zip ./final_dpo_model\n",
    "\n",
    "print(\"\\nğŸ‰ DPO Training Successfully Completed!\")\n",
    "print(\"\\nğŸ“‹ Summary:\")\n",
    "print(f\"   â€¢ Model: {model_name}\")\n",
    "print(f\"   â€¢ Training samples: {len(train_dataset)}\")\n",
    "print(f\"   â€¢ Evaluation samples: {len(eval_dataset)}\")\n",
    "print(f\"   â€¢ Final training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   â€¢ Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   â€¢ Peak GPU memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\nğŸ“¥ To download the model:\")\n",
    "print(\"   1. Download 'dpo_model.zip' from the file browser\")\n",
    "print(\"   2. Or access from Google Drive if mounted\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
