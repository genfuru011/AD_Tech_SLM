# ğŸš€ DPO Training on Google Colab

Google Colabã§DPOï¼ˆDirect Preference Optimizationï¼‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®å®Œå…¨ãªã‚¬ã‚¤ãƒ‰ã§ã™ã€‚

## ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
colab_dpo_training.ipynb          # ãƒ¡ã‚¤ãƒ³ã®Colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
dpo_training_experiment.ipynb     # è©³ç´°ãªå®Ÿé¨“ç”¨ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
README_COLAB.md                   # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. Google Colabã§é–‹ã

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)

1. `colab_dpo_training.ipynb` ã‚’Google Colabã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
2. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚¿ã‚¤ãƒ—ã‚’GPUã«å¤‰æ›´
3. ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ

### 2. æ¨å¥¨è¨­å®š

- **ãƒ©ãƒ³ã‚¿ã‚¤ãƒ **: GPU (T4/V100æ¨å¥¨)
- **RAM**: æ¨™æº– (12.7GB)
- **ãƒ‡ã‚£ã‚¹ã‚¯**: æ¨™æº–
- **å®Ÿè¡Œæ™‚é–“**: ç´„15-30åˆ†

## ğŸ“Š ç‰¹å¾´

- âœ… **è‡ªå‹•ç’°å¢ƒæ§‹ç¯‰**: å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- âœ… **è»½é‡ãƒ¢ãƒ‡ãƒ«**: Colabã®GPUãƒ¡ãƒ¢ãƒªã«æœ€é©åŒ–
- âœ… **åŠ¹ç‡çš„å­¦ç¿’**: LoRAã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªå¾®èª¿æ•´
- âœ… **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–**: å­¦ç¿’é€²æ—ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
- âœ… **çµæœå¯è¦–åŒ–**: æå¤±ã‚°ãƒ©ãƒ•ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
- âœ… **è‡ªå‹•ä¿å­˜**: Google Driveã¸ã®è‡ªå‹•ä¿å­˜

## ğŸ”§ æŠ€è¡“ä»•æ§˜

### ãƒ¢ãƒ‡ãƒ«
- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: `microsoft/DialoGPT-small` (117M parameters)
- **å¾®èª¿æ•´æ‰‹æ³•**: LoRA (Low-Rank Adaptation)
- **æœ€é©åŒ–æ‰‹æ³•**: DPO (Direct Preference Optimization)

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- **ã‚µãƒ³ãƒ—ãƒ«æ•°**: 1,000ä»¶ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰
- **å½¢å¼**: prompt, chosen, rejected
- **è¨€èª**: æ—¥æœ¬èª
- **åˆ†é‡**: AIã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
- **ã‚¨ãƒãƒƒã‚¯æ•°**: 1
- **ãƒãƒƒãƒã‚µã‚¤ã‚º**: 1 (gradient_accumulation_steps=4)
- **å­¦ç¿’ç‡**: 5e-6
- **æœ€å¤§é•·**: 128ãƒˆãƒ¼ã‚¯ãƒ³

## ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹çµæœ

### å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- **åˆæœŸæå¤±**: ç´„0.7
- **æœ€çµ‚æå¤±**: ç´„0.1-0.3
- **ç²¾åº¦**: 80-95%
- **GPUä½¿ç”¨é‡**: 2-3GB

### å®Ÿè¡Œæ™‚é–“
- **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: 2-3åˆ†
- **ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°**: 10-20åˆ†
- **è©•ä¾¡ãƒ»ä¿å­˜**: 2-3åˆ†
- **åˆè¨ˆ**: ç´„15-30åˆ†

## ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ³•

#### 1. GPU ãƒ¡ãƒ¢ãƒªä¸è¶³
```python
# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
torch.cuda.empty_cache()

# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
per_device_train_batch_size=1
gradient_accumulation_steps=2
```

#### 2. ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼
```bash
# æœ€æ–°ç‰ˆã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!pip install --upgrade transformers trl peft
```

#### 3. ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼ (fastai, fsspec)
```bash
# fastaiã¨torchã®ç«¶åˆã‚¨ãƒ©ãƒ¼
ERROR: fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.

# fsspecé–¢é€£ã®ã‚¨ãƒ©ãƒ¼
ERROR: gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.

# ğŸ“ è§£æ±ºæ‰‹é †:
# 1. Runtime â†’ Restart runtime ã‚’ã‚¯ãƒªãƒƒã‚¯
# 2. ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ã€Œç·Šæ€¥ã‚¯ã‚¤ãƒƒã‚¯ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã€ã‚»ãƒ«ã§ QUICK_FIX = True ã«è¨­å®š
# 3. ã¾ãŸã¯æ‰‹å‹•ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œ:

!pip uninstall -y fastai torch gcsfs fsspec
!pip install 'torch>=2.0,<2.7' --index-url https://download.pytorch.org/whl/cu118
!pip install fsspec==2025.3.2 gcsfs==2025.3.2
!pip install 'fastai>=2.7.0'
```
!pip install fsspec==2025.3.2
!pip install gcsfs==2025.3.2
```

#### 4. å­¦ç¿’ãŒé€²ã¾ãªã„
```python
# å­¦ç¿’ç‡ã‚’èª¿æ•´
learning_rate=1e-5  # ã‚ˆã‚Šå°ã•ã

# å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
max_grad_norm=1.0
```

## ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨æ–¹æ³•

### 1. Colabã§ã®æ¨è«–
```python
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–
response = generate_response("è³ªå•: AIã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ")
print(response)
```

### 2. ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ä½¿ç”¨
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
base_model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-small")
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-small")

# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é©ç”¨
model = PeftModel.from_pretrained(base_model, "./path/to/saved/model")

# æ¨è«–å®Ÿè¡Œ
inputs = tokenizer.encode("è³ªå•:", return_tensors="pt")
outputs = model.generate(inputs, max_length=100)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## ğŸ“š å‚è€ƒè³‡æ–™

### DPOé–¢é€£
- [DPOè«–æ–‡](https://arxiv.org/abs/2305.18290)
- [TRL Documentation](https://huggingface.co/docs/trl/)
- [PEFT Documentation](https://huggingface.co/docs/peft/)

### Colabé–¢é€£
- [Google Colabä½¿ã„æ–¹](https://colab.research.google.com/)
- [GPUä½¿ç”¨æ–¹æ³•](https://colab.research.google.com/notebooks/gpu.ipynb)

## ğŸ¤ è²¢çŒ®ã¨ã‚µãƒãƒ¼ãƒˆ

### æ”¹è‰¯ææ¡ˆ
- ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
- ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã§ã®å®Ÿé¨“
- è©•ä¾¡æŒ‡æ¨™ã®è¿½åŠ 

### å•é¡Œå ±å‘Š
GitHub Issues ã¾ãŸã¯ä»¥ä¸‹ã§å ±å‘Šã—ã¦ãã ã•ã„ï¼š
- ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å•é¡Œ
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–
- Colabç’°å¢ƒã§ã®äº’æ›æ€§å•é¡Œ

## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

---

## ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **åŸºæœ¬å®Ÿè¡Œ**: `colab_dpo_training.ipynb` ã§åŸºæœ¬çš„ãªDPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½“é¨“
2. **è©³ç´°å®Ÿé¨“**: `dpo_training_experiment.ipynb` ã§ã‚ˆã‚Šè©³ç´°ãªå®Ÿé¨“ã‚’å®Ÿè¡Œ
3. **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º**: ç‹¬è‡ªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„ãƒ¢ãƒ‡ãƒ«ã§ã®å®Ÿé¨“
4. **æœ¬æ ¼é‹ç”¨**: ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚„æœ¬æ ¼çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å­¦ç¿’

Happy Training! ğŸš€
