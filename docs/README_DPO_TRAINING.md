# DPO Training Environment Setup Guide

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€MacBook Air M2 8GBç’°å¢ƒã§AD_Tech_SLMã®DPOï¼ˆDirect Preference Optimizationï¼‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ“‹ å¿…è¦ç’°å¢ƒ

- **ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢**: MacBook Air M2 8GB
- **OS**: macOS (Apple Siliconå¯¾å¿œ)
- **Python**: 3.8ä»¥ä¸Š
- **VSCode**: æ¨å¥¨IDE
- **GPU**: M2ãƒãƒƒãƒ— (Metal Performance Shaderså¯¾å¿œ)

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/genfuru011/AD_Tech_SLM.git
cd AD_Tech_SLM

# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
./setup.sh
```

### 2. ä»®æƒ³ç’°å¢ƒã®æœ‰åŠ¹åŒ–

```bash
source venv/bin/activate
```

### 3. ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª

```bash
# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¤œè¨¼
python scripts/validate_data.py data/sample_dpo_dataset.jsonl
```

### 4. DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ

```bash
# åŸºæœ¬çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
python scripts/train_dpo.py

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
python scripts/train_dpo.py --config configs/dpo_config.yaml
```

### 5. æ¨è«–ãƒ†ã‚¹ãƒˆ

```bash
# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ¨è«–
python scripts/inference.py
```

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
AD_Tech_SLM/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ dpo_config.yaml          # DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_dpo_dataset.jsonl # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_dpo.py            # DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ validate_data.py        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â””â”€â”€ inference.py            # æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ dpo_training_experiment.ipynb # Jupyterå®Ÿé¨“ãƒãƒ¼ãƒˆ
â”œâ”€â”€ requirements.txt            # Pythonä¾å­˜é–¢ä¿‚
â”œâ”€â”€ setup.sh                   # ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â””â”€â”€ README_DPO_TRAINING.md     # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

## âš™ï¸ è¨­å®šè©³ç´°

### DPOè¨­å®š (`configs/dpo_config.yaml`)

M2 8GBç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šï¼š

```yaml
# ãƒ¢ãƒ‡ãƒ«è¨­å®š
model_name: "google/gemma-2b-it"  # è»½é‡ãƒ¢ãƒ‡ãƒ«
max_length: 512
max_prompt_length: 256

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
num_train_epochs: 3
per_device_train_batch_size: 1    # ãƒ¡ãƒ¢ãƒªç¯€ç´„
gradient_accumulation_steps: 4    # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º = 4
learning_rate: 5e-6
beta: 0.1  # DPO beta ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

# LoRAè¨­å®š
use_lora: true
lora_r: 16
lora_alpha: 32
target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

# ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¨­å®š
device: "mps"  # M2 GPU
fp16: true     # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
```

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå½¢å¼

JSONLãƒ•ã‚¡ã‚¤ãƒ«ã§ã€å„è¡Œã¯ä»¥ä¸‹ã®å½¢å¼ï¼š

```json
{
  "prompt": "ã€ãƒ†ãƒ¼ãƒã€‘ã‚¢ãƒ—ãƒªã®ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³æ–‡ã‚’æ›¸ã„ã¦ãã ã•ã„",
  "chosen": "é­…åŠ›çš„ã§åŠ¹æœçš„ãªåºƒå‘Šæ–‡",
  "rejected": "å¹³å‡¡ã§åŠ¹æœã®ä½ã„åºƒå‘Šæ–‡"
}
```

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™

### 1. ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ

```bash
# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
python scripts/validate_data.py --create-sample data/my_dataset.jsonl 100

# ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
python scripts/validate_data.py data/my_dataset.jsonl
```

### 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¦ä»¶

- **prompt**: åºƒå‘Šä½œæˆã®ãŠé¡Œã‚„æ¡ä»¶
- **chosen**: CTRãŒé«˜ã„ã€ã¾ãŸã¯è©•ä¾¡ã®é«˜ã„åºƒå‘Šæ–‡
- **rejected**: CTRãŒä½ã„ã€ã¾ãŸã¯è©•ä¾¡ã®ä½ã„åºƒå‘Šæ–‡

å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯æ—¥æœ¬èªã§ã€æ„Ÿæƒ…ã‚„é­…åŠ›ã‚’è¡¨ç¾ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚

## ğŸ¯ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### M2 8GBç’°å¢ƒã§ã®æœ€é©åŒ–

1. **ãƒãƒƒãƒã‚µã‚¤ã‚º**: 1ã«è¨­å®šã—ã€gradient_accumulationã§èª¿æ•´
2. **æ··åˆç²¾åº¦**: fp16ã‚’æœ‰åŠ¹ã«ã—ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›
3. **LoRA**: ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ãªãLoRAã‚’ä½¿ç”¨
4. **ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ã•ã‚‰ã«å‰Šæ¸›

### æ¨å¥¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹é †

```bash
# 1. ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
python scripts/validate_data.py data/your_dataset.jsonl

# 2. å°è¦æ¨¡ãƒ†ã‚¹ãƒˆï¼ˆ1ã‚¨ãƒãƒƒã‚¯ï¼‰
# configs/dpo_config.yaml ã§ num_train_epochs: 1 ã«è¨­å®š
python scripts/train_dpo.py

# 3. çµæœç¢ºèª
python scripts/inference.py

# 4. æœ¬æ ¼ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
# configs/dpo_config.yaml ã§ num_train_epochs: 3-5 ã«è¨­å®š
python scripts/train_dpo.py
```

## ğŸ“ˆ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨è©•ä¾¡

### TensorBoard ã§ã®ç›£è¦–

```bash
# TensorBoardã®èµ·å‹•
tensorboard --logdir outputs/logs

# ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:6006 ã«ã‚¢ã‚¯ã‚»ã‚¹
```

### è©•ä¾¡æŒ‡æ¨™

- **Training Loss**: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æå¤±ã®æ¸›å°‘
- **Evaluation Loss**: æ¤œè¨¼æå¤±ã®ç›£è¦–
- **Preference Accuracy**: DPOã® preference accuracy

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

#### 1. ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’1ã«ã€gradient_accumulation_stepsã‚’æ¸›ã‚‰ã™
# configs/dpo_config.yaml ã§ä»¥ä¸‹ã‚’èª¿æ•´:
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
```

#### 2. MPS ãŒåˆ©ç”¨ã§ããªã„

```bash
# CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ç¢ºèª
python -c "import torch; print(torch.backends.mps.is_available())"

# CPUãƒ¢ãƒ¼ãƒ‰ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
# configs/dpo_config.yaml ã§ device: "cpu" ã«å¤‰æ›´
```

#### 3. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼

```bash
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªã‚¢
rm -rf ~/.cache/huggingface/transformers/

# å†å®Ÿè¡Œ
python scripts/train_dpo.py
```

## ğŸ“š å‚è€ƒè³‡æ–™

- [DPO (Direct Preference Optimization) è«–æ–‡](https://arxiv.org/abs/2305.18290)
- [TRL (Transformer Reinforcement Learning) ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/trl/index)
- [PEFT (Parameter-Efficient Fine-tuning) ã‚¬ã‚¤ãƒ‰](https://huggingface.co/docs/peft/index)
- [Metal Performance Shaders (MPS) ã‚¬ã‚¤ãƒ‰](https://developer.apple.com/metal/pytorch/)

## ğŸ¤ ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³

ãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½è¦æ±‚ã¯ã€GitHubã®Issueã§ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚
ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚‚æ­“è¿ã—ã¾ã™ï¼

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯Apache License 2.0ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚