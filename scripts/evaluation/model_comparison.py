#!/usr/bin/env python3
"""
ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒè©•ä¾¡
"""

import os
import json
import logging
import torch
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def compare_models():
    """ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ"""
    
    base_model_name = "SakanaAI/TinySwallow-1.5B-Instruct"
    trained_model_path = "./outputs/tiny_swallow_dpo/final_model"
    device = "cpu"
    
    # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    test_prompts = [
        "RTBã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®åˆ©ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
    ]
    
    logger.info("ğŸ” ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒé–‹å§‹")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    results = {"base_model": [], "dpo_model": []}
    
    # 1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ
    logger.info("ğŸ“¥ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float32,
        trust_remote_code=True
    ).to(device)
    base_model.eval()
    
    logger.info("ğŸ§ª ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è©•ä¾¡...")
    for prompt in test_prompts:
        try:
            inputs = tokenizer(prompt, return_tensors="pt", max_length=256, truncation=True)
            
            with torch.no_grad():
                outputs = base_model.generate(
                    inputs.input_ids,
                    max_new_tokens=30,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            results["base_model"].append({"prompt": prompt, "response": response.strip()})
            print(f"\nğŸ”µ ãƒ™ãƒ¼ã‚¹ - {prompt}")
            print(f"   å¿œç­”: {response.strip()}")
            
        except Exception as e:
            logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
            results["base_model"].append({"prompt": prompt, "response": f"ã‚¨ãƒ©ãƒ¼: {e}"})
    
    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
    del base_model
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    # 2. DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ
    logger.info("ğŸ“¥ DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿...")
    dpo_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float32,
        trust_remote_code=True
    )
    
    if os.path.exists(trained_model_path):
        dpo_model = PeftModel.from_pretrained(dpo_model, trained_model_path)
        dpo_model = dpo_model.merge_and_unload()
    
    dpo_model.to(device)
    dpo_model.eval()
    
    logger.info("ğŸ§ª DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«è©•ä¾¡...")
    for prompt in test_prompts:
        try:
            inputs = tokenizer(prompt, return_tensors="pt", max_length=256, truncation=True)
            
            with torch.no_grad():
                outputs = dpo_model.generate(
                    inputs.input_ids,
                    max_new_tokens=30,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            results["dpo_model"].append({"prompt": prompt, "response": response.strip()})
            print(f"\nğŸŸ¢ DPO - {prompt}")
            print(f"   å¿œç­”: {response.strip()}")
            
        except Exception as e:
            logger.error(f"DPOãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
            results["dpo_model"].append({"prompt": prompt, "response": f"ã‚¨ãƒ©ãƒ¼: {e}"})
    
    # çµæœä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"outputs/evaluation/model_comparison_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ’¾ æ¯”è¼ƒçµæœä¿å­˜: {output_file}")
    logger.info("ğŸ‰ æ¯”è¼ƒè©•ä¾¡å®Œäº†ï¼")

if __name__ == "__main__":
    compare_models()
