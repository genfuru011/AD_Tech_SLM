#!/usr/bin/e    print("ğŸ¤– DPOä¸­é–“ãƒ¢ãƒ‡ãƒ«ï¼ˆStep 300ï¼‰ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
    model_name = "gpt2"
    checkpoint_path = "./outputs/checkpoint-300"
    
    print(f"ğŸ“¥ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_name}")
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = GPT2LMHeadModel.from_pretrained(model_name)
    
    print(f"ğŸ”§ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿: {checkpoint_path}")
DPOä¸­é–“ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ200ã®ãƒ¢ãƒ‡ãƒ«å“è³ªã‚’ç¢ºèª
"""

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from peft import PeftModel
import json

def test_intermediate_model():
    print("ğŸ¤– DPOä¸­é–“ãƒ¢ãƒ‡ãƒ«ï¼ˆStep 200ï¼‰ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
    model_name = "gpt2"
    checkpoint_path = "./outputs/checkpoint-300"
    
    print(f"ğŸ“¥ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_name}")
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = GPT2LMHeadModel.from_pretrained(model_name)
    
    print(f"ğŸ”§ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿: {checkpoint_path}")
    model = PeftModel.from_pretrained(base_model, checkpoint_path)
    model.eval()
    
    # MPSä½¿ç”¨å¯èƒ½ãƒã‚§ãƒƒã‚¯
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    model = model.to(device)
    print(f"ğŸ–¥ï¸  ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    test_prompts = [
        "æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "å¥åº·çš„ãªé£Ÿäº‹ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
        "åŠ¹æœçš„ãªå­¦ç¿’æ–¹æ³•ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
        "ç’°å¢ƒä¿è­·ã®é‡è¦æ€§ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å­¦ç¿’ã®ã‚³ãƒ„ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
    ]
    
    print("\n" + "="*60)
    print("ğŸ§ª DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    print("="*60)
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nã€ãƒ†ã‚¹ãƒˆ {i}ã€‘")
        print(f"ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
        print("ğŸ“ ç”Ÿæˆçµæœ:")
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        inputs = tokenizer.encode(prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + 100,
                num_return_sequences=1,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=2
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = generated_text[len(prompt):].strip()
        
        # å“è³ªæŒ‡æ¨™
        print(f"   {response}")
        print(f"ğŸ“Š ç”Ÿæˆé•·: {len(response)} æ–‡å­—")
        print("-" * 40)
    
    print("\nâœ… ä¸­é–“ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆå®Œäº†!")
    print(f"ğŸ“ˆ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€²æ—: 200/2139 ã‚¹ãƒ†ãƒƒãƒ— (9.3%)")
    print(f"ğŸ¯ Loss: 0.031 (å„ªç§€ãªæ”¹å–„)")
    print(f"â­ ç²¾åº¦: 100%")

if __name__ == "__main__":
    test_intermediate_model()
