#!/usr/bin/env python3
"""
DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã®ä¸­é–“ãƒ†ã‚¹ãƒˆ
checkpoint-100ã§ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ç¢ºèª
"""

import torch
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import os

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def quick_test():
    """ç°¡å˜ãªãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    
    base_model_name = "SakanaAI/TinySwallow-1.5B-Instruct"
    checkpoint_path = "./outputs/tiny_swallow_dpo/checkpoint-100"
    
    if not os.path.exists(checkpoint_path):
        logger.error(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {checkpoint_path}")
        return
    
    logger.info("ğŸ” checkpoint-100ã§ã®ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    logger.info(f"ğŸ”§ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    try:
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
        logger.info("ğŸ“¥ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        logger.info("ğŸ“¥ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿...")
        model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float32 if device == "mps" else torch.float16,
            trust_remote_code=True
        )
        
        # DPOã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿
        logger.info("ğŸ“¥ DPOã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿...")
        model = PeftModel.from_pretrained(model, checkpoint_path)
        model = model.merge_and_unload()
        
        model.to(device)
        model.eval()
        
        # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        test_prompts = [
            "RTBï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å…¥æœ­ï¼‰ã®ä»•çµ„ã¿ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®ä¸»ãªåˆ©ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "CPMã¨CPCã®é•ã„ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
        ]
        
        logger.info("ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ...")
        
        for i, prompt in enumerate(test_prompts):
            logger.info(f"\n--- ãƒ†ã‚¹ãƒˆ {i+1} ---")
            logger.info(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = tokenizer(
                prompt,
                return_tensors="pt",
                max_length=256,
                truncation=True
            ).to(device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    max_new_tokens=100,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            response = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            )
            
            logger.info(f"å¿œç­”: {response.strip()}")
        
        logger.info("\nâœ… ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Œäº†")
        
    except Exception as e:
        logger.error(f"âŒ ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    quick_test()
