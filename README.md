# AD_Tech_SLM

広告特化型 SLM（Small Language Model）開発・運用プロジェクト

## プロジェクト概要

本プロジェクトは、“広告コピーの品質向上”と“CTR（クリック率）改善”を目指す、広告業界向けの特化型言語モデルを開発・運用するものです。  
データ収集からモデル開発・評価・デプロイ・運用まで、一貫したワークフローを設計し、効率的かつ安定的な価値提供を目指します。

---

## ロードマップ

### 短期（2ヶ月）：PoC → 本格トレーニング
### 中期（4ヶ月）：デプロイ＋運用パイプライン構築
### 長期（6ヶ月以降）：継続的改善・新機能展開

---

## チューニング手法

### 人間のフィードバックを活用（RLHF, DPO手法）
- 人間のフィードバックをもとに、より最適なコピーを生成するようモデルを学習させる
- 感情制御、要約生成、対話タスクなど、様々なタスクでRLHFと同等かそれ以上の人間選好一致性を示すことが報告されています。特に、トーンやスタイル、特定のコンテンツ指向など、主観的な要素が重要となる場合に効果を発揮します。
- 特に、トーンやスタイル、特定のコンテンツ指向など、主観的な要素が重要となる場合に効果を発揮します。
- 人間が好む出力と好まない出力のペアデータが不可欠です。このデータの質や量、多様性がモデルの性能に大きく影響します
- 
<img width="633" alt="Image" src="https://github.com/user-attachments/assets/a0c9bf1c-a7e9-4ead-a18d-dc47a0b874f1" />
<img width="566" alt="Image" src="https://github.com/user-attachments/assets/9c941693-71b1-4da6-b043-42a4deb54954" />

要するに人間がLLMの人格、性格話し方、トーン、表現の仕方などをチューニングできると言うことです。
以上の特徴から広告表現特化SLMにはDPOが最適と判断しました。

### 必要データアセットJSONL
```json
{"prompt": "【テーマ】雨の日でもワクワクするニュースアプリを紹介してください", 
 "chosen": "雨が降っても最新トレンドをスマホでサクッとチェック！天気と話題を同時にキャッチして、移動中も退屈知らず♪", 
 "rejected": "ニュースが見られるよ！便利！"}

{"prompt": "【テーマ】忙しい会社員が移動中に英語を学べるアプリを紹介してください", 
 "chosen": "通勤電車で 3 分！AI レッスンがあなたの発音を即フィードバック💡スキマ時間で着実にスキルアップ！", 
 "rejected": "英語学習ができます。おすすめです。"}

{"prompt": "【テーマ】節電をサポートする家計簿アプリを紹介してください", 
 "chosen": "家電ごとの電気代を自動計算！グラフでムダを一目で発見して、月末の請求額にもうドキドキしません✨", 
 "rejected": "節約に役立つアプリです。ぜひ使ってください。"}
```
- prompt：コピーを書かせたいお題や前提条件をまとめます。
- chosen：CTR が高かった “良い広告テキスト” を入れます。
- rejected：CTR が低かった、または不採用になった文を入れます。

## フェーズ詳細

### 1. 要件定義・プロジェクト設計（1ヶ月）
- ビジネスゴール整理（例：CTR向上・広告テキスト品質評価の定量目標設定）
- ステークホルダー（マーケ／エンジニア／デザイナー）との合意形成
- 技術要件（モデル指標：Perplexity, Preference Accuracy, CTRシミュレーション等）
- インフラ要件（GPU台数、ストレージ容量、CI/CD設計）
- チーム体制整備・マイルストーン設定

### 2. データ収集・準備（2ヶ月）
- 広告コピーデータ（自社／外部CTR実績、A/Bテスト、ユーザー調査ログなど）収集
- アノテーション基準設計（“chosen”／“rejected”の基準化、JSONLフォーマット定義）
- データ前処理パイプライン（テキストクリーニング、トークナイザー設定、不均衡データ対策）

### 3. 開発環境構築・PoC（3ヶ月）
- Ollama等連携
- ベースモデル選定（例：Gemma3-1b）
- LoRA＋DPOによるトレーニングPoC
- 初期評価（Preference Accuracy、サンプル広告文の品質評価）

## 🚀 DPO Training Environment

MacBook Air M2 8GB対応のDPOトレーニング環境が利用可能です！

### クイックスタート

```bash
# 環境セットアップ
./quick_start.sh

# DPOトレーニング実行
source venv/bin/activate
python scripts/train_dpo.py

# 推論テスト
python scripts/inference.py
```

詳細なセットアップガイド: [README_DPO_TRAINING.md](README_DPO_TRAINING.md)

### 4. モデル本格チューニング・評価（4ヶ月）
- 大規模DPOトレーニング（サンプル数1k程度を想定）
- 学習率／バッチサイズのハイパーパラ最適化
- 自動評価（Preference Accuracy、Perplexity、CTR予測スコア）
- ユーザーテスト（A/Bテスト事前シミュレーション）
- 反復改善（エラー分析、データセット強化）

### 5. デプロイメント＆CI/CD整備（5ヶ月）
- モデルパッケージ化→Dockerイメージ化
- 推論API（FastAPI＋Uvicorn等）構築
- 継続学習パイプライン（新規広告データ収集・DPO再学習スクリプト）
- GitHub Actionsによる定期ジョブ、Lint／テスト自動化
- GitHub Copilotレビュー体制導入

### 6. モニタリング・運用・改善サイクル（継続）
- 推論ログ収集（プロンプト・出力・ユーザーフィードバック）
- 性能劣化・ドリフト検知、定期再評価
- 四半期ごとのデータ拡充・再学習
- 新機能追加（カスタムスタイル制御、A/Bテスト自動化等）

---

## 貢献・開発ガイド

- 本リポジトリは随時コントリビューターを募集しています。
- PR作成時はCopilotレビュー完了チェックリストを活用し、Lint・ユニットテストを通過してください。

---

## まとめ

このロードマップに沿って、PoCから本格トレーニング、運用・改善まで一貫して推進することで、  
安定的かつ進化的に「広告特化SLM」をリリース・運用できます。  
プロジェクト進行に応じて、各フェーズのスコープや期間は柔軟に調整します。

---
