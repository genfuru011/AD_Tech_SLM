#!/opt/miniconda3/envs/dpo_training/bin/python
"""
TinySwallow-1.5B-Instruct ãƒ¢ãƒ‡ãƒ«å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆ
ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã¨åŸºæœ¬çš„ãªç”Ÿæˆãƒ†ã‚¹ãƒˆ
"""

import torch
import sys
import os

def test_tinyswallow():
    """TinySwallowãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ¦† TinySwallow-1.5B-Instruct å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆ")
    print("="*50)
    
    # PyTorchç’°å¢ƒç¢ºèª
    print(f"ğŸ”§ PyTorch ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.__version__}")
    if torch.backends.mps.is_available():
        device = "mps"
        print("âœ… MPS (Metal Performance Shaders) åˆ©ç”¨å¯èƒ½")
    elif torch.cuda.is_available():
        device = "cuda"
        print("âœ… CUDA GPU åˆ©ç”¨å¯èƒ½")
    else:
        device = "cpu"
        print("âš ï¸  CPU ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        print("âœ… Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model_name = "SakanaAI/TinySwallow-1.5B-Instruct"
    print(f"\nğŸ“¥ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {model_name}")
    
    try:
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿æˆåŠŸ")
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            print("ğŸ”§ ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®šå®Œäº†")
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device != "cpu" else torch.float32,
            trust_remote_code=True
        )
        print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
        
        # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        if device == "mps":
            model = model.to(device)
            print("âœ… MPS ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•å®Œäº†")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ç¢ºèª
        param_count = sum(p.numel() for p in model.parameters())
        print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {param_count:,}")
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    # ãƒ†ã‚¹ãƒˆç”Ÿæˆ
    test_prompts = [
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "RTBã®ä»•çµ„ã¿ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
        "æ—¥æœ¬ã®ãƒ‡ã‚¸ã‚¿ãƒ«ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã«ã¤ã„ã¦"
    ]
    
    print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆç”Ÿæˆé–‹å§‹ (ãƒ‡ãƒã‚¤ã‚¹: {device})")
    print("-" * 50)
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nğŸ” ãƒ†ã‚¹ãƒˆ {i}: {prompt}")
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = tokenizer.encode(prompt, return_tensors="pt")
            if device == "mps":
                inputs = inputs.to(device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_new_tokens=50,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    no_repeat_ngram_size=2
                )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = generated_text[len(prompt):].strip()
            
            print(f"ğŸ“ ç”Ÿæˆçµæœ: {response}")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    print("\n" + "="*50)
    print("ğŸ‰ TinySwallow-1.5B-Instruct å‹•ä½œç¢ºèªå®Œäº†ï¼")
    print("âœ… DPO ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æº–å‚™ãŒã§ãã¦ã„ã¾ã™ã€‚")
    
    return True

if __name__ == "__main__":
    success = test_tinyswallow()
    if not success:
        sys.exit(1)
