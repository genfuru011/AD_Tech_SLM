#!/usr/bin/env python3
"""
DPOä¸­é–“ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ - Step 300
ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ300ã®ãƒ¢ãƒ‡ãƒ«å“è³ªã‚’ç¢ºèª
"""

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from peft import PeftModel
import json

def test_checkpoint_300():
    print("ğŸ¤– DPOä¸­é–“ãƒ¢ãƒ‡ãƒ«ï¼ˆStep 300ï¼‰ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
    model_name = "gpt2"
    checkpoint_path = "./outputs/checkpoint-300"
    
    print(f"ğŸ“¥ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_name}")
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = GPT2LMHeadModel.from_pretrained(model_name)
    
    print(f"ğŸ”§ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿: {checkpoint_path}")
    model = PeftModel.from_pretrained(base_model, checkpoint_path)
    model.eval()
    
    # MPSä½¿ç”¨å¯èƒ½ãƒã‚§ãƒƒã‚¯
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    model = model.to(device)
    print(f"ğŸ–¥ï¸  ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚ˆã‚ŠçŸ­ã„ã€ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ãªè³ªå•ï¼‰
    test_prompts = [
        "What is machine learning?",
        "How to study effectively?",
        "Benefits of exercise:",
        "Python programming tips:",
        "Healthy food choices:"
    ]
    
    print("\n" + "="*60)
    print("ğŸ§ª DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆãƒ†ã‚¹ãƒˆ (Step 300)")
    print("="*60)
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nã€ãƒ†ã‚¹ãƒˆ {i}ã€‘")
        print(f"ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
        print("ğŸ“ ç”Ÿæˆçµæœ:")
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        inputs = tokenizer.encode(prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + 80,
                num_return_sequences=1,
                temperature=0.8,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=2,
                early_stopping=True
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = generated_text[len(prompt):].strip()
        
        # å“è³ªæŒ‡æ¨™
        print(f"   {response}")
        print(f"ğŸ“Š ç”Ÿæˆé•·: {len(response)} æ–‡å­—")
        print("-" * 40)
    
    print("\nâœ… Step 300 ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆå®Œäº†!")
    print("ğŸ“ˆ é€²æ—çŠ¶æ³:")
    print(f"   ğŸ¯ ç¾åœ¨ã‚¹ãƒ†ãƒƒãƒ—: 300/2139 (14.0%)")
    print(f"   ğŸ“‰ Eval Loss: 0.009 (å„ªç§€ãªæ”¹å–„)")
    print(f"   â­ Eval ç²¾åº¦: 100%")
    print(f"   ğŸ“Š Eval Margin: 5.23 (å¼·ã„é¸å¥½å­¦ç¿’)")

if __name__ == "__main__":
    test_checkpoint_300()
