#!/opt/miniconda3/envs/dpo_training/bin/python
"""
TinySwallow-1.5B-Instruct DPO Training Script
SakanaAIè£½ã®æ—¥æœ¬èªç‰¹åŒ–è»½é‡ãƒ¢ãƒ‡ãƒ«ç”¨DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

Model: SakanaAI/TinySwallow-1.5B-Instruct
Target: åºƒå‘ŠæŠ€è¡“åˆ†é‡ã®å°‚é–€çŸ¥è­˜å‘ä¸Š
Hardware: MacBook Air M2 8GB (MPSæœ€é©åŒ–)
"""

import os
import sys
import yaml
import json
import torch
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# è¿½åŠ ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        TrainingArguments,
        BitsAndBytesConfig
    )
    from trl import DPOTrainer
    from datasets import Dataset
    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
    import accelerate
except ImportError as e:
    print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    print("conda activate dpo_training && pip install -r requirements.txt ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

class TinySwallowDPOTrainer:
    """TinySwallow-1.5B-Instructç”¨DPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼"""
    
    def __init__(self, config_path: str = "configs/tiny_swallow_config.yaml"):
        self.config_path = config_path
        self.config = self._load_config()
        self.setup_logging()
        self.device = self._setup_device()
        
        self.tokenizer = None
        self.model = None
        self.dataset = None
        self.trainer = None
        
    def _load_config(self) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {self.config_path}")
            return config
        except FileNotFoundError:
            print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.config_path}")
            sys.exit(1)
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        os.makedirs("outputs/logs", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = f"outputs/logs/tinyswallow_dpo_{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info("ğŸ¦† TinySwallow DPO Training Started")
    
    def _setup_device(self) -> str:
        """ãƒ‡ãƒã‚¤ã‚¹è¨­å®šï¼ˆMPSæœ€é©åŒ–ï¼‰"""
        if torch.backends.mps.is_available() and self.config['hardware']['use_mps']:
            device = "mps"
            self.logger.info("ğŸ”§ MPS (Metal Performance Shaders) ã‚’ä½¿ç”¨")
        elif torch.cuda.is_available():
            device = "cuda"
            self.logger.info("ğŸ”§ CUDA GPU ã‚’ä½¿ç”¨")
        else:
            device = "cpu"
            self.logger.info("ğŸ”§ CPU ã‚’ä½¿ç”¨")
        
        return device
    
    def load_model_and_tokenizer(self):
        """TinySwallowãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿"""
        model_name = self.config['model']['name']
        self.logger.info(f"ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_name}")
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=self.config['model']['parameters']['trust_remote_code']
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.logger.info("ğŸ”§ ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š")
            
            # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
            model_kwargs = {
                'trust_remote_code': self.config['model']['parameters']['trust_remote_code'],
                'torch_dtype': torch.float16 if self.device != "cpu" else torch.float32,
            }
            
            if self.device == "mps":
                # MPSç”¨ã®æœ€é©åŒ–è¨­å®š
                model_kwargs['device_map'] = None  # MPSã§ã¯è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹ãƒãƒƒãƒ—ã‚’ç„¡åŠ¹åŒ–
            else:
                model_kwargs['device_map'] = self.config['model']['parameters']['device_map']
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                **model_kwargs
            )
            
            # MPSãƒ‡ãƒã‚¤ã‚¹ã«æ‰‹å‹•ã§ç§»å‹•
            if self.device == "mps":
                self.model = self.model.to(self.device)
            
            # LoRAè¨­å®šã®é©ç”¨
            self._setup_lora()
            
            self.logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_name}")
            self.logger.info(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ
            self._try_backup_models()
    
    def _setup_lora(self):
        """LoRAè¨­å®šã®é©ç”¨"""
        lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªè¨­å®š
        if self.config['hardware']['memory_efficient']:
            self.model = prepare_model_for_kbit_training(self.model)
        
        self.model = get_peft_model(self.model, lora_config)
        self.logger.info("ğŸ”§ LoRAè¨­å®šã‚’é©ç”¨")
    
    def _try_backup_models(self):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã®è©¦è¡Œ"""
        backup_models = self.config['model']['backup_models']
        
        for backup_model in backup_models:
            try:
                self.logger.info(f"ğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ: {backup_model}")
                
                self.tokenizer = AutoTokenizer.from_pretrained(backup_model)
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    backup_model,
                    torch_dtype=torch.float16 if self.device != "cpu" else torch.float32,
                    trust_remote_code=True
                )
                
                if self.device == "mps":
                    self.model = self.model.to(self.device)
                
                self._setup_lora()
                self.logger.info(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {backup_model}")
                return
                
            except Exception as e:
                self.logger.warning(f"âš ï¸  ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«å¤±æ•—: {backup_model} - {e}")
                continue
        
        self.logger.error("âŒ å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    def load_dataset(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
        dataset_file = self.config['dataset']['train_file']
        self.logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­: {dataset_file}")
        
        if not os.path.exists(dataset_file):
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dataset_file}")
            sys.exit(1)
        
        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        dataset_list = []
        required_keys = self.config['dataset']['required_keys']
        
        with open(dataset_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    data = json.loads(line.strip())
                    
                    # å¿…è¦ãªã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
                    if all(key in data for key in required_keys):
                        dataset_list.append({
                            'prompt': data['prompt'],
                            'chosen': data['chosen'],
                            'rejected': data['rejected']
                        })
                    else:
                        self.logger.warning(f"âš ï¸  Line {line_num}: å¿…è¦ãªã‚­ãƒ¼ãŒä¸è¶³")
                        
                except json.JSONDecodeError as e:
                    self.logger.warning(f"âš ï¸  Line {line_num}: JSONå½¢å¼ã‚¨ãƒ©ãƒ¼ - {e}")
                    continue
        
        if not dataset_list:
            self.logger.error("âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            sys.exit(1)
        
        # Datasetã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
        dataset = Dataset.from_list(dataset_list)
        
        # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
        split_dataset = dataset.train_test_split(
            test_size=self.config['dataset']['test_size'],
            seed=self.config['dataset']['seed']
        )
        
        self.train_dataset = split_dataset['train']
        self.eval_dataset = split_dataset['test']
        
        self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†")
        self.logger.info(f"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(self.train_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
        self.logger.info(f"ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(self.eval_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    
    def setup_training(self):
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        output_dir = self.config['training']['output_dir']
        os.makedirs(output_dir, exist_ok=True)
        
        # ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        training_config = self.config['training'].copy()
        if self.config['hardware']['memory_efficient']:
            fallback = self.config['hardware']['fallback']
            self.logger.info("ğŸ”§ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ã‚’é©ç”¨")
            training_config.update(fallback)
        
        # TrainingArgumentsã®è¨­å®š
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=training_config['num_train_epochs'],
            max_steps=training_config['max_steps'],
            per_device_train_batch_size=training_config['per_device_train_batch_size'],
            per_device_eval_batch_size=training_config['per_device_eval_batch_size'],
            gradient_accumulation_steps=training_config['gradient_accumulation_steps'],
            learning_rate=training_config['learning_rate'],
            warmup_steps=training_config['warmup_steps'],
            eval_steps=training_config['eval_steps'],
            save_steps=training_config['save_steps'],
            logging_steps=training_config['logging_steps'],
            eval_strategy=training_config['evaluation_strategy'],
            save_strategy=training_config['save_strategy'],
            save_total_limit=training_config['save_total_limit'],
            load_best_model_at_end=training_config['load_best_model_at_end'],
            metric_for_best_model=training_config['metric_for_best_model'],
            greater_is_better=training_config['greater_is_better'],
            fp16=training_config['fp16'] and self.device != "cpu",
            dataloader_pin_memory=training_config['dataloader_pin_memory'],
            remove_unused_columns=training_config['remove_unused_columns'],
            report_to=training_config['report_to'],
        )
        
        # DPOTrainerã®åˆæœŸåŒ–
        dpo_config = self.config['dpo']
        
        # TRL 0.18.1ç”¨ã®æœ€å°é™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§DPOTraineråˆæœŸåŒ–
        try:
            # æœ€å°é™ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨
            self.trainer = DPOTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.train_dataset,
                eval_dataset=self.eval_dataset,
                beta=dpo_config['beta'],
            )
            self.logger.info("âœ… DPOTraineråˆæœŸåŒ–ï¼ˆæœ€å°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰")
        except Exception as e:
            self.logger.error(f"âŒ DPOTraineråˆæœŸåŒ–å¤±æ•—: {e}")
            self.logger.info("DPOTrainerã®ä»£æ›¿æ¡ˆã‚’è©¦è¡Œ...")
            
            # ä»£æ›¿æ¡ˆ: åŸºæœ¬çš„ãªDPOå®Ÿè£…ã‚’ä½¿ç”¨
            try:
                from transformers import Trainer
                self.trainer = Trainer(
                    model=self.model,
                    args=training_args,
                    train_dataset=self.train_dataset,
                    eval_dataset=self.eval_dataset,
                )
                self.logger.info("âœ… åŸºæœ¬Trainerã§ä»£æ›¿åˆæœŸåŒ–å®Œäº†")
            except Exception as e2:
                self.logger.error(f"âŒ ä»£æ›¿åˆæœŸåŒ–ã‚‚å¤±æ•—: {e2}")
                raise
        
        self.logger.info("âœ… DPOãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–å®Œäº†")
    
    def train(self):
        """DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ"""
        self.logger.info("ğŸš€ TinySwallow DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
        
        try:
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
            self.trainer.train()
            
            self.logger.info("âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
            
            # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
            final_output_dir = os.path.join(self.config['training']['output_dir'], "final_model")
            self.trainer.save_model(final_output_dir)
            self.tokenizer.save_pretrained(final_output_dir)
            
            self.logger.info(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {final_output_dir}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def test_model(self):
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
        if not self.model or not self.tokenizer:
            self.logger.error("âŒ ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        test_prompts = self.config['testing']['test_prompts']
        generation_config = self.config['testing']['generation_config']
        
        self.logger.info("ğŸ§ª å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("\n" + "="*80)
        print("ğŸ¦† TinySwallow-1.5B-Instruct ãƒ†ã‚¹ãƒˆçµæœ")
        print("="*80)
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\nğŸ” ãƒ†ã‚¹ãƒˆ {i}: {prompt}")
            print("-" * 60)
            
            try:
                # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                inputs = self.tokenizer.encode(prompt, return_tensors="pt")
                if self.device == "mps":
                    inputs = inputs.to(self.device)
                
                # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs,
                        max_new_tokens=generation_config['max_new_tokens'],
                        temperature=generation_config['temperature'],
                        do_sample=generation_config['do_sample'],
                        no_repeat_ngram_size=generation_config['no_repeat_ngram_size'],
                        pad_token_id=self.tokenizer.eos_token_id,
                    )
                
                # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ã‚³ãƒ¼ãƒ‰
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                response = generated_text[len(prompt):].strip()
                
                print(f"ğŸ“ å›ç­”: {response}")
                
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        print("\n" + "="*80)
        print("ğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¦† TinySwallow-1.5B-Instruct DPO Training")
    print("="*50)
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
    trainer = TinySwallowDPOTrainer()
    
    # ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
    try:
        print("\nğŸ“‹ Step 1: ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿")
        trainer.load_model_and_tokenizer()
        
        print("\nğŸ“‹ Step 2: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿")
        trainer.load_dataset()
        
        print("\nğŸ“‹ Step 3: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š")
        trainer.setup_training()
        
        print("\nğŸ“‹ Step 4: DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ")
        success = trainer.train()
        
        if success:
            print("\nğŸ“‹ Step 5: ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ")
            trainer.test_model()
            
            print("\nğŸ‰ TinySwallow DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        else:
            print("\nâŒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
