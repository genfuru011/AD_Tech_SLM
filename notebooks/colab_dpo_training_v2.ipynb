{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876fdb64",
   "metadata": {},
   "source": [
    "# ğŸ¦† TinySwallow-1.5B DPO Training on Google Colab\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/AD_Tech_SLM/blob/main/notebooks/colab_dpo_training_v2.ipynb)\n",
    "\n",
    "**Direct Preference Optimization (DPO) Training for Japanese Language Model**\n",
    "\n",
    "- **Model**: SakanaAI/TinySwallow-1.5B-Instruct\n",
    "- **Target**: åºƒå‘ŠæŠ€è¡“åˆ†é‡ã®å°‚é–€çŸ¥è­˜å‘ä¸Š\n",
    "- **Framework**: TRL 0.18.1 + Transformers\n",
    "- **Hardware**: Google Colab GPU (T4/V100/A100)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8cd66",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "### GPUç¢ºèªã¨å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a539f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUç¢ºèª\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"ğŸ”§ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"ğŸš€ CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ“± GPU Device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df32ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€æ–°ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers==4.52.4\n",
    "!pip install -q trl==0.18.1\n",
    "!pip install -q datasets==3.1.0\n",
    "!pip install -q peft==0.15.0\n",
    "!pip install -q bitsandbytes==0.44.1\n",
    "!pip install -q accelerate==1.2.1\n",
    "!pip install -q wandb\n",
    "!pip install -q tensorboard\n",
    "\n",
    "# Colabç‰¹æœ‰ã®å•é¡Œè§£æ±º\n",
    "!pip install -q tf-keras\n",
    "\n",
    "print(\"âœ… ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791b200",
   "metadata": {},
   "source": [
    "### å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1de4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Transformers & TRL\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Colabç”¨è¨­å®š\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"ğŸ“¦ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\")\n",
    "print(f\"ğŸ”§ TRL Version: {trl.__version__ if 'trl' in globals() else 'Not found'}\")\n",
    "print(f\"ğŸ¤– Transformers Version: {transformers.__version__ if 'transformers' in globals() else 'Not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef4ad81",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. è¨­å®šã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b7371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\n",
    "CONFIG = {\n",
    "    # ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "    \"model_name\": \"SakanaAI/TinySwallow-1.5B-Instruct\",\n",
    "    \"backup_models\": [\n",
    "        \"tokyotech-llm/Swallow-1.5b-instruct-hf\",\n",
    "        \"rinna/japanese-gpt-neox-3.6b-instruction-sft\"\n",
    "    ],\n",
    "    \n",
    "    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šï¼ˆColab GPUæœ€é©åŒ–ï¼‰\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"max_steps\": 500,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 1e-6,\n",
    "    \"warmup_steps\": 50,\n",
    "    \"eval_steps\": 50,\n",
    "    \"save_steps\": 100,\n",
    "    \"logging_steps\": 10,\n",
    "    \n",
    "    # DPOè¨­å®š\n",
    "    \"beta\": 0.1,\n",
    "    \"max_length\": 1024,\n",
    "    \"max_prompt_length\": 512,\n",
    "    \n",
    "    # LoRAè¨­å®š\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š\n",
    "    \"test_size\": 0.1,\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # å‡ºåŠ›è¨­å®š\n",
    "    \"output_dir\": \"./outputs/colab_dpo_training\",\n",
    "}\n",
    "\n",
    "print(\"âœ… è¨­å®šå®Œäº†\")\n",
    "print(f\"ğŸ“‹ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {CONFIG['model_name']}\")\n",
    "print(f\"ğŸ¯ æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°: {CONFIG['max_steps']}\")\n",
    "print(f\"ğŸ“Š ãƒãƒƒãƒã‚µã‚¤ã‚º: {CONFIG['per_device_train_batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c19aed4",
   "metadata": {},
   "source": [
    "### ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\n",
    "\n",
    "åºƒå‘ŠæŠ€è¡“åˆ†é‡ã«ç‰¹åŒ–ã—ãŸDPOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b7b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åºƒå‘ŠæŠ€è¡“åˆ†é‡ã®ã‚µãƒ³ãƒ—ãƒ«DPOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "sample_dpo_data = [\n",
    "    {\n",
    "        \"prompt\": \"ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®RTBã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n",
    "        \"chosen\": \"RTBï¼ˆReal-Time Biddingï¼‰ã¯ã€åºƒå‘Šæ ã®å£²è²·ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³å½¢å¼ã§è¡Œã†ä»•çµ„ã¿ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒWebãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ãŸç¬é–“ã«ã€ãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å±æ€§ã‚„é–²è¦§å±¥æ­´ã«åŸºã¥ã„ã¦ã€åºƒå‘Šä¸»ãŒè‡ªå‹•çš„ã«å…¥æœ­ã‚’è¡Œã„ã¾ã™ã€‚æœ€ã‚‚é«˜ã„é‡‘é¡ã‚’æç¤ºã—ãŸåºƒå‘Šä¸»ã®åºƒå‘ŠãŒè¡¨ç¤ºã•ã‚Œã‚‹ä»•çµ„ã¿ã§ã€åŠ¹ç‡çš„ãªã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°ã¨è²»ç”¨å¯¾åŠ¹æœã®å‘ä¸Šã‚’å®Ÿç¾ã—ã¾ã™ã€‚\",\n",
    "        \"rejected\": \"RTBã¯åºƒå‘Šã‚’è¡¨ç¤ºã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"DSPã¨SSPã®é•ã„ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\",\n",
    "        \"chosen\": \"DSPï¼ˆDemand-Side Platformï¼‰ã¯åºƒå‘Šä¸»å´ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã€åºƒå‘Šæ ã®è³¼å…¥ã‚’è‡ªå‹•åŒ–ã—ã€ã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°ã‚„å…¥æœ­æˆ¦ç•¥ã®æœ€é©åŒ–ã‚’è¡Œã„ã¾ã™ã€‚ä¸€æ–¹ã€SSPï¼ˆSupply-Side Platformï¼‰ã¯ãƒ¡ãƒ‡ã‚£ã‚¢å´ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã€åºƒå‘Šæ ã®è²©å£²ã‚’è‡ªå‹•åŒ–ã—ã€åç›Šã®æœ€å¤§åŒ–ã‚’å›³ã‚Šã¾ã™ã€‚DSPã¯è²·ã„æ‰‹ã€SSPã¯å£²ã‚Šæ‰‹ã®ç«‹å ´ã§ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®å–å¼•ã‚’æ”¯æ´ã—ã¾ã™ã€‚\",\n",
    "        \"rejected\": \"DSPã¨SSPã¯ä¸¡æ–¹ã¨ã‚‚åºƒå‘Šé–¢é€£ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"ã‚¯ãƒƒã‚­ãƒ¼ãƒ¬ã‚¹æ™‚ä»£ã®åºƒå‘Šã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°æ‰‹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n",
    "        \"chosen\": \"ã‚¯ãƒƒã‚­ãƒ¼ãƒ¬ã‚¹æ™‚ä»£ã§ã¯ã€ä»¥ä¸‹ã®æ‰‹æ³•ãŒé‡è¦ã¨ãªã‚Šã¾ã™ï¼š1) ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒ‘ãƒ¼ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨ï¼ˆè‡ªç¤¾ã®é¡§å®¢ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰ã€2) ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåºƒå‘Šï¼ˆWebãƒšãƒ¼ã‚¸ã®å†…å®¹ã«åŸºã¥ã„ãŸã‚¿ãƒ¼ã‚²ãƒ†ã‚£ãƒ³ã‚°ï¼‰ã€3) ã‚³ãƒ›ãƒ¼ãƒˆåˆ†æï¼ˆé¡ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã§ã®åˆ†æï¼‰ã€4) ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹æŠ€è¡“ï¼ˆGoogleãŒææ¡ˆã™ã‚‹æ–°æŠ€è¡“ç¾¤ï¼‰ã€5) IDã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ™ãƒ¼ã‚¹ã®è­˜åˆ¥å­ï¼‰ãªã©ãŒã‚ã‚Šã¾ã™ã€‚\",\n",
    "        \"rejected\": \"ã‚¯ãƒƒã‚­ãƒ¼ãŒä½¿ãˆãªããªã‚‹ã®ã§ã€æ–°ã—ã„æ–¹æ³•ã‚’è€ƒãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"ã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æã®é‡è¦æ€§ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚\",\n",
    "        \"chosen\": \"ã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æã¯ã€ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«è‡³ã‚‹ã¾ã§ã®å„ã‚¿ãƒƒãƒãƒã‚¤ãƒ³ãƒˆã®è²¢çŒ®åº¦ã‚’æ¸¬å®šã™ã‚‹åˆ†ææ‰‹æ³•ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³¼è²·è¡Œå‹•ã¯è¤‡é›‘ã§ã€è¤‡æ•°ã®åºƒå‘Šæ¥è§¦ã‚’çµŒã¦ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«è‡³ã‚‹ãŸã‚ã€ãƒ©ã‚¹ãƒˆã‚¯ãƒªãƒƒã‚¯ä»¥å¤–ã®æ¥è§¦ç‚¹ã®ä¾¡å€¤ã‚‚é©åˆ‡ã«è©•ä¾¡ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°äºˆç®—ã®æœ€é©é…åˆ†ã€ãƒãƒ£ãƒãƒ«é–“ã®ç›¸äº’ä½œç”¨ã®ç†è§£ã€ROIã®æ­£ç¢ºãªæ¸¬å®šãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚\",\n",
    "        \"rejected\": \"ã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æã¯åºƒå‘Šã®åŠ¹æœã‚’æ¸¬å®šã™ã‚‹æ–¹æ³•ã§ã™ã€‚\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ“ãƒ‡ã‚£ãƒ³ã‚°ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
    "        \"chosen\": \"ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ“ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆHeader Biddingï¼‰ã¯ã€è¤‡æ•°ã®SSPã‚„ã‚¢ãƒ‰ã‚¨ã‚¯ã‚¹ãƒã‚§ãƒ³ã‚¸ãŒåŒæ™‚ã«åºƒå‘Šæ ã«å…¥æœ­ã§ãã‚‹ä»•çµ„ã¿ã§ã™ã€‚å¾“æ¥ã®ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«æ–¹å¼ã¨ã¯ç•°ãªã‚Šã€ã™ã¹ã¦ã®éœ€è¦æºãŒåŒã˜ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ç«¶åˆã§ãã‚‹ãŸã‚ã€ã‚ˆã‚Šå…¬å¹³ã§é€æ˜æ€§ã®é«˜ã„ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚çµæœã¨ã—ã¦ã€ãƒ‘ãƒ–ãƒªãƒƒã‚·ãƒ£ãƒ¼ã®åç›Šå‘ä¸Šã¨åºƒå‘Šä¸»ã®åŠ¹ç‡çš„ãªé…ä¿¡ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚\",\n",
    "        \"rejected\": \"ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ“ãƒ‡ã‚£ãƒ³ã‚°ã¯åºƒå‘Šæ ã‚’å£²ã‚‹æ–¹æ³•ã®ä¸€ã¤ã§ã™ã€‚\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ã‚ˆã‚Šå¤šãã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰\n",
    "extended_data = []\n",
    "for i in range(50):  # 50å›ç¹°ã‚Šè¿”ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æ‹¡å¼µ\n",
    "    for base_data in sample_dpo_data:\n",
    "        extended_data.append({\n",
    "            \"prompt\": base_data[\"prompt\"],\n",
    "            \"chosen\": base_data[\"chosen\"],\n",
    "            \"rejected\": base_data[\"rejected\"]\n",
    "        })\n",
    "\n",
    "print(f\"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(extended_data)} ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "print(f\"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ä¾‹:\")\n",
    "print(f\"  Prompt: {sample_dpo_data[0]['prompt'][:50]}...\")\n",
    "print(f\"  Chosen: {sample_dpo_data[0]['chosen'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4833f0ef",
   "metadata": {},
   "source": [
    "## ğŸ¤– 3. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
    "def load_tokenizer(model_name):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            logger.info(\"ğŸ”§ ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š\")\n",
    "        \n",
    "        return tokenizer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ\n",
    "tokenizer = load_tokenizer(CONFIG[\"model_name\"])\n",
    "\n",
    "# å¤±æ•—ã—ãŸå ´åˆã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ\n",
    "if tokenizer is None:\n",
    "    for backup_model in CONFIG[\"backup_models\"]:\n",
    "        logger.info(f\"ğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ: {backup_model}\")\n",
    "        tokenizer = load_tokenizer(backup_model)\n",
    "        if tokenizer is not None:\n",
    "            CONFIG[\"model_name\"] = backup_model\n",
    "            break\n",
    "\n",
    "if tokenizer is None:\n",
    "    raise ValueError(\"âŒ ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "\n",
    "print(f\"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†: {CONFIG['model_name']}\")\n",
    "print(f\"ğŸ“ èªå½™ã‚µã‚¤ã‚º: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡å­åŒ–è¨­å®šï¼ˆGPU ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰\n",
    "def create_bnb_config():\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "def load_model(model_name, use_quantization=True):\n",
    "    try:\n",
    "        model_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"device_map\": \"auto\",\n",
    "        }\n",
    "        \n",
    "        if use_quantization:\n",
    "            model_kwargs[\"quantization_config\"] = create_bnb_config()\n",
    "            logger.info(\"ğŸ”§ 4bité‡å­åŒ–ã‚’ä½¿ç”¨\")\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ\n",
    "model = load_model(CONFIG[\"model_name\"])\n",
    "\n",
    "if model is None:\n",
    "    raise ValueError(\"âŒ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "\n",
    "print(f\"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {CONFIG['model_name']}\")\n",
    "print(f\"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda99653",
   "metadata": {},
   "source": [
    "### LoRAè¨­å®šã®é©ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f266c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRAè¨­å®š\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¢ºèª\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percentage = 100 * trainable_params / total_params\n",
    "\n",
    "print(f\"ğŸ”§ LoRAè¨­å®šé©ç”¨å®Œäº†\")\n",
    "print(f\"ğŸ“ˆ è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_params:,} ({trainable_percentage:.2f}%)\")\n",
    "print(f\"ğŸ“Š ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8bee95",
   "metadata": {},
   "source": [
    "## ğŸ“Š 4. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f281cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆã¨åˆ†å‰²\n",
    "dataset = Dataset.from_list(extended_data)\n",
    "\n",
    "# è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²\n",
    "split_dataset = dataset.train_test_split(\n",
    "    test_size=CONFIG[\"test_size\"],\n",
    "    seed=CONFIG[\"seed\"]\n",
    ")\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†\")\n",
    "print(f\"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "print(f\"ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(eval_dataset)} ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª\n",
    "print(f\"\\nğŸ“ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"  Prompt: {sample['prompt'][:100]}...\")\n",
    "print(f\"  Chosen: {sample['chosen'][:100]}...\")\n",
    "print(f\"  Rejected: {sample['rejected'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1dcf1c",
   "metadata": {},
   "source": [
    "## ğŸš€ 5. DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c36860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# DPOConfigè¨­å®šï¼ˆTRL 0.18.1å¯¾å¿œï¼‰\n",
    "training_args = DPOConfig(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    max_steps=CONFIG[\"max_steps\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,  # GPUç”¨\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[\"tensorboard\"],  # Colabç”¨\n",
    "    # DPOå›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    beta=CONFIG[\"beta\"],\n",
    "    max_length=CONFIG[\"max_length\"],\n",
    "    max_prompt_length=CONFIG[\"max_prompt_length\"],\n",
    ")\n",
    "\n",
    "print(f\"âœ… DPOConfigè¨­å®šå®Œäº†\")\n",
    "print(f\"ğŸ“‹ æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°: {CONFIG['max_steps']}\")\n",
    "print(f\"ğŸ¯ å­¦ç¿’ç‡: {CONFIG['learning_rate']}\")\n",
    "print(f\"ğŸ”¥ DPO Beta: {CONFIG['beta']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPOTrainer ã®åˆæœŸåŒ–ï¼ˆTRL 0.18.1å¯¾å¿œï¼‰\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,  # æ–°ã—ã„API\n",
    ")\n",
    "\n",
    "print(f\"âœ… DPOTraineråˆæœŸåŒ–å®Œäº†\")\n",
    "print(f\"ğŸ”§ ä½¿ç”¨API: TRL 0.18.1\")\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€çµ‚ç¢ºèª\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒª (å‰²ã‚Šå½“ã¦æ¸ˆã¿): {allocated:.2f} GB\")\n",
    "    print(f\"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒª (äºˆç´„æ¸ˆã¿): {reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2285cb9",
   "metadata": {},
   "source": [
    "## ğŸƒâ€â™‚ï¸ 6. DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
    "\n",
    "**æ³¨æ„**: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯30åˆ†ã€œ1æ™‚é–“ç¨‹åº¦ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d3ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹æ™‚åˆ»ã®è¨˜éŒ²\n",
    "start_time = datetime.now()\n",
    "print(f\"ğŸš€ DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ“Š äºˆæƒ³æ‰€è¦æ™‚é–“: 30-60åˆ†\")\n",
    "print(f\"ğŸ“ˆ é€²æ—ã¯ä¸‹è¨˜ã§ç¢ºèªã§ãã¾ã™:\")\n",
    "\n",
    "try:\n",
    "    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ\n",
    "    trainer.train()\n",
    "    \n",
    "    # å®Œäº†æ™‚åˆ»ã®è¨ˆç®—\n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nâœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†!\")\n",
    "    print(f\"â±ï¸  æ‰€è¦æ™‚é–“: {duration}\")\n",
    "    print(f\"ğŸ¯ æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—: {trainer.state.global_step}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®è©³ç´°è¡¨ç¤º\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc649ad",
   "metadata": {},
   "source": [
    "## ğŸ’¾ 7. ãƒ¢ãƒ‡ãƒ«ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "final_output_dir = os.path.join(CONFIG[\"output_dir\"], \"final_model\")\n",
    "os.makedirs(final_output_dir, exist_ok=True)\n",
    "\n",
    "# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ä¿å­˜\n",
    "trainer.save_model(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "\n",
    "print(f\"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {final_output_dir}\")\n",
    "\n",
    "# ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "saved_files = os.listdir(final_output_dir)\n",
    "print(f\"ğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "for file in saved_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# Hugging Face Hubã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "print(f\"\\nğŸš€ Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ:\")\n",
    "print(f\"  trainer.push_to_hub('your-username/tiny-swallow-dpo-adtech')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7027f1a",
   "metadata": {},
   "source": [
    "## ğŸ§ª 8. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad92a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "test_prompts = [\n",
    "    \"ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®ãƒ¡ãƒªãƒƒãƒˆã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\",\n",
    "    \"DSPã®ä¸»è¦ãªæ©Ÿèƒ½ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\",\n",
    "    \"ã‚¯ãƒƒã‚­ãƒ¼ãƒ¬ã‚¹æ™‚ä»£ã®å¯¾ç­–ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚\",\n",
    "    \"åºƒå‘Šã®ã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ“ãƒ‡ã‚£ãƒ³ã‚°ã®ä»•çµ„ã¿ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆé–‹å§‹\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nğŸ” ãƒ†ã‚¹ãƒˆ {i}: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"ğŸ“ å›ç­”: {response}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef67b3df",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ 9. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc5f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoardã®èµ·å‹•\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir={CONFIG[\"output_dir\"]}/runs\n",
    "\n",
    "print(\"ğŸ“ˆ TensorBoardèµ·å‹•å®Œäº†\")\n",
    "print(\"ğŸ“Š ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¸Šè¨˜ã§ç¢ºèªã§ãã¾ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ccf4c4",
   "metadata": {},
   "source": [
    "## ğŸ“¥ 10. ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46269ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ZIPãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
    "zip_filename = \"tiny_swallow_dpo_model\"\n",
    "shutil.make_archive(zip_filename, 'zip', final_output_dir)\n",
    "\n",
    "print(f\"ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {zip_filename}.zip\")\n",
    "print(f\"ğŸ“ ã‚µã‚¤ã‚º: {os.path.getsize(f'{zip_filename}.zip') / 1024**2:.1f} MB\")\n",
    "\n",
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "# files.download(f\"{zip_filename}.zip\")\n",
    "print(\"ğŸ’¾ ä¸Šè¨˜ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’è§£é™¤ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df0cfb6",
   "metadata": {},
   "source": [
    "## ğŸ‰ å®Œäº†ï¼\n",
    "\n",
    "### âœ… é”æˆé …ç›®\n",
    "- âœ… TinySwallow-1.5B ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "- âœ… åºƒå‘ŠæŠ€è¡“åˆ†é‡ã®DPOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n",
    "- âœ… LoRA + 4bité‡å­åŒ–ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- âœ… TRL 0.18.1 æœ€æ–°APIã‚’ä½¿ç”¨ã—ãŸDPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- âœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ\n",
    "- âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®å¯è¦–åŒ–\n",
    "\n",
    "### ğŸ“š æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "1. **å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§ã®è¨“ç·´**: ã‚ˆã‚Šå¤§è¦æ¨¡ã§å¤šæ§˜ãªåºƒå‘ŠæŠ€è¡“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨\n",
    "2. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**: å­¦ç¿’ç‡ã€betaãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–\n",
    "3. **è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: ã‚ˆã‚Šè©³ç´°ãªè©•ä¾¡æŒ‡æ¨™ã®å°å…¥\n",
    "4. **ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å±•é–‹**: APIåŒ–ã‚„Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦ã®å±•é–‹\n",
    "\n",
    "### ğŸ”— å‚è€ƒãƒªãƒ³ã‚¯\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl/)\n",
    "- [DPO Paper](https://arxiv.org/abs/2305.18290)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "\n",
    "---\n",
    "**ğŸ¦† TinySwallow DPO Training Completed Successfully! ğŸŠ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
