{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cade523",
   "metadata": {},
   "source": [
    "## 🚨 依存関係エラーが出た場合のクイックフィックス\n",
    "\n",
    "Google Colabで以下のようなエラーが出た場合：\n",
    "- `fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1`\n",
    "- `gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0`\n",
    "\n",
    "**解決手順：**\n",
    "1. **Runtime → Restart runtime** をクリック\n",
    "2. 下記のクイックフィックスセルを実行\n",
    "3. 再度メインのセットアップセルを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚨 緊急クイックフィックス (依存関係エラー専用)\n",
    "# エラーが出た場合のみ実行してください\n",
    "\n",
    "QUICK_FIX = False  # ← Trueに変更して実行\n",
    "\n",
    "if QUICK_FIX:\n",
    "    print(\"🚨 Running emergency dependency fix...\")\n",
    "    \n",
    "    # 問題のあるパッケージを強制削除\n",
    "    !pip uninstall -y fastai torch gcsfs fsspec --quiet\n",
    "    \n",
    "    # 適切なバージョンで再インストール\n",
    "    !pip install 'torch>=2.0,<2.7' --index-url https://download.pytorch.org/whl/cu118\n",
    "    !pip install fsspec==2025.3.2 gcsfs==2025.3.2\n",
    "    !pip install 'fastai>=2.7.0'\n",
    "    \n",
    "    print(\"✅ Quick fix completed. Now run the main setup cell below.\")\n",
    "else:\n",
    "    print(\"📝 Quick fix not enabled. Set QUICK_FIX = True if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56ccac",
   "metadata": {},
   "source": [
    "# 🚀 DPO Training on Google Colab - 直接選好最適化\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/dpo-colab)\n",
    "\n",
    "このノートブックでは、Google ColabでDPO（Direct Preference Optimization）トレーニングを実行します。\n",
    "\n",
    "## ✨ 特徴\n",
    "- 🔥 GPU加速トレーニング\n",
    "- 📊 リアルタイム進捗監視\n",
    "- 💾 自動チェックポイント保存\n",
    "- 🎯 効率的なLoRA微調整\n",
    "- 📈 結果の可視化\n",
    "\n",
    "## ⚙️ 推奨環境\n",
    "- Google Colab Pro (GPU: T4/V100)\n",
    "- ランタイムタイプ: GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3952e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 環境セットアップ（依存関係の競合を回避）\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Colab環境チェック\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"🏃‍♂️ Running in Google Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"📦 Installing required packages with dependency resolution...\")\n",
    "    \n",
    "    # 事前に問題のあるパッケージをアンインストール\n",
    "    print(\"Step 1: Cleaning existing conflicting packages...\")\n",
    "    !pip uninstall -y fastai gcsfs fsspec torch -q\n",
    "    \n",
    "    print(\"Step 2: Installing core dependencies with specific versions...\")\n",
    "    !pip install -q --upgrade pip setuptools wheel\n",
    "    \n",
    "    # PyTorchを互換性のあるバージョンで固定\n",
    "    print(\"Step 3: Installing PyTorch with version constraint...\")\n",
    "    !pip install 'torch<2.7,>=2.0' torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    \n",
    "    # fsspecとgcsfsを最新バージョンで統一\n",
    "    print(\"Step 4: Installing file system packages...\")\n",
    "    !pip install -q fsspec==2025.3.2 gcsfs==2025.3.2\n",
    "    \n",
    "    print(\"Step 5: Installing transformers ecosystem...\")\n",
    "    !pip install -q transformers datasets accelerate tokenizers\n",
    "    \n",
    "    print(\"Step 6: Installing TRL and PEFT...\")\n",
    "    !pip install -q trl peft\n",
    "    \n",
    "    print(\"Step 7: Installing additional ML packages...\")\n",
    "    !pip install -q evaluate bitsandbytes matplotlib pandas numpy\n",
    "    \n",
    "    # fastaiの再インストール（適切なバージョンで）\n",
    "    print(\"Step 8: Reinstalling fastai with correct torch version...\")\n",
    "    !pip install -q 'fastai>=2.7.0'\n",
    "    \n",
    "    print(\"✅ Installation completed with dependency resolution!\")\n",
    "    \n",
    "    # 警告とエラーを最小化\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # 最終的な依存関係確認\n",
    "    print(\"\\n🔍 Verifying critical package versions:\")\n",
    "    import pkg_resources\n",
    "    critical_packages = ['torch', 'transformers', 'trl', 'peft', 'fastai']\n",
    "    for pkg in critical_packages:\n",
    "        try:\n",
    "            version = pkg_resources.get_distribution(pkg).version\n",
    "            print(f\"  ✅ {pkg}: {version}\")\n",
    "        except:\n",
    "            print(f\"  ❌ {pkg}: Not properly installed\")\n",
    "\n",
    "# GPU確認\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\n🚀 GPU Available: {gpu_name}\")\n",
    "    print(f\"💾 GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"🔥 PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"\\n⚠️ No GPU available. Please enable GPU in Runtime > Change runtime type\")\n",
    "    print(\"💡 Go to Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b554cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 トラブルシューティング: 詳細な依存関係確認\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"🔍 Comprehensive package version check...\")\n",
    "    \n",
    "    import subprocess\n",
    "    import pkg_resources\n",
    "    \n",
    "    # 重要なパッケージの詳細確認\n",
    "    packages_to_check = {\n",
    "        'torch': 'PyTorch (Should be <2.7, >=2.0)',\n",
    "        'transformers': 'Hugging Face Transformers',\n",
    "        'trl': 'Transformer Reinforcement Learning',\n",
    "        'peft': 'Parameter Efficient Fine-tuning',\n",
    "        'datasets': 'Hugging Face Datasets',\n",
    "        'accelerate': 'Hugging Face Accelerate',\n",
    "        'fsspec': 'File System Specification (Should be 2025.3.2)',\n",
    "        'gcsfs': 'Google Cloud Storage File System (Should be 2025.3.2)',\n",
    "        'fastai': 'FastAI (Should work with current torch)'\n",
    "    }\n",
    "    \n",
    "    print(\"\\n📊 Package Status:\")\n",
    "    conflicts_found = False\n",
    "    \n",
    "    for package, description in packages_to_check.items():\n",
    "        try:\n",
    "            version = pkg_resources.get_distribution(package).version\n",
    "            status = \"✅\"\n",
    "            \n",
    "            # 特定の競合をチェック\n",
    "            if package == 'torch' and version >= '2.7':\n",
    "                status = \"⚠️ \"\n",
    "                conflicts_found = True\n",
    "            elif package in ['fsspec', 'gcsfs'] and not version.startswith('2025.3'):\n",
    "                status = \"⚠️ \"\n",
    "                conflicts_found = True\n",
    "                \n",
    "            print(f\"  {status} {package}: {version} - {description}\")\n",
    "        except pkg_resources.DistributionNotFound:\n",
    "            print(f\"  ❌ {package}: Not installed - {description}\")\n",
    "            conflicts_found = True\n",
    "    \n",
    "    if conflicts_found:\n",
    "        print(\"\\n🔧 Dependency conflicts detected! To fix:\")\n",
    "        print(\"   1. Runtime → Restart runtime\")\n",
    "        print(\"   2. Re-run the installation cell above\")\n",
    "        print(\"   3. If issues persist, enable ALTERNATIVE_INSTALL in the next cell\")\n",
    "    else:\n",
    "        print(\"\\n🎉 All dependencies are compatible!\")\n",
    "\n",
    "    # システム情報\n",
    "    print(\"\\n🖥️ System Information:\")\n",
    "    print(f\"  Python: {sys.version.split()[0]}\")\n",
    "    print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"  GPU Count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 代替インストール手法 (依存関係エラーが出た場合)\n",
    "\n",
    "# 上記のインストールで fastai や fsspec エラーが出た場合はこちらを実行\n",
    "# ⚠️ このセルを実行する前に「ランタイム → ランタイムを再起動」してください\n",
    "\n",
    "ALTERNATIVE_INSTALL = False  # ← Trueに変更して実行\n",
    "\n",
    "if ALTERNATIVE_INSTALL and IN_COLAB:\n",
    "    print(\"🔄 Running alternative installation with strict version management...\")\n",
    "    \n",
    "    # 完全クリーンアップ\n",
    "    print(\"Step 1: Complete package cleanup...\")\n",
    "    !pip uninstall -y fastai fastbook torch torchvision torchaudio gcsfs fsspec transformers trl peft -q\n",
    "    \n",
    "    # 基盤パッケージから順番にインストール\n",
    "    print(\"Step 2: Installing PyTorch with fastai compatibility...\")\n",
    "    !pip install 'torch>=1.10,<2.7' 'torchvision' 'torchaudio' --index-url https://download.pytorch.org/whl/cu118\n",
    "    \n",
    "    print(\"Step 3: Installing file system packages with exact versions...\")\n",
    "    !pip install fsspec==2025.3.2 gcsfs==2025.3.2\n",
    "    \n",
    "    print(\"Step 4: Installing fastai first (to avoid conflicts)...\")\n",
    "    !pip install 'fastai>=2.7.0,<2.8'\n",
    "    \n",
    "    print(\"Step 5: Installing transformers ecosystem...\")\n",
    "    !pip install transformers datasets accelerate tokenizers\n",
    "    \n",
    "    print(\"Step 6: Installing DPO training packages...\")\n",
    "    !pip install trl peft evaluate\n",
    "    \n",
    "    print(\"Step 7: Installing additional utilities...\")\n",
    "    !pip install matplotlib pandas numpy bitsandbytes\n",
    "    \n",
    "    print(\"\\n✅ Alternative installation completed!\")\n",
    "    print(\"📝 Verifying installation...\")\n",
    "    \n",
    "    # インストール確認\n",
    "    try:\n",
    "        import torch\n",
    "        import transformers\n",
    "        import trl\n",
    "        import peft\n",
    "        import fastai\n",
    "        print(\"🎉 All critical packages imported successfully!\")\n",
    "        print(f\"   PyTorch: {torch.__version__}\")\n",
    "        print(f\"   Transformers: {transformers.__version__}\")\n",
    "        print(f\"   FastAI: {fastai.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"❌ Import error: {e}\")\n",
    "        print(\"💡 Please restart runtime and try again\")\n",
    "\n",
    "elif ALTERNATIVE_INSTALL:\n",
    "    print(\"📝 This alternative method is only for Google Colab.\")\n",
    "else:\n",
    "    print(\"📝 Alternative installation not enabled.\")\n",
    "    print(\"     Set ALTERNATIVE_INSTALL = True if you encounter dependency conflicts.\")\n",
    "    print(\"     Remember to restart runtime first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50383597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 DPOデータセット生成\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "def create_dpo_dataset(size: int = 1000) -> List[Dict]:\n",
    "    \"\"\"高品質なDPOデータセットを生成\"\"\"\n",
    "    \n",
    "    templates = [\n",
    "        {\n",
    "            \"prompt\": \"以下の質問に答えてください: {question}\",\n",
    "            \"chosen\": \"{topic}について説明します。{detail}これにより、{benefit}が期待できます。\",\n",
    "            \"rejected\": \"その質問は複雑ですね。様々な要因があります。\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"次の文章を要約してください: {text}\",\n",
    "            \"chosen\": \"この文章のポイントは{point}です。具体的には{detail}について述べています。\",\n",
    "            \"rejected\": \"文章が長くて複雑なため、要約は困難です。\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"プログラミングについて説明してください: {topic}\",\n",
    "            \"chosen\": \"{topic}は{definition}です。主な特徴として{feature}があり、{usage}に使用されます。\",\n",
    "            \"rejected\": \"プログラミングは技術的で説明が難しいです。\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    topics = [\"機械学習\", \"ウェブ開発\", \"データ分析\", \"人工知能\", \"クラウド\"]\n",
    "    questions = [\"AIとは何ですか？\", \"プログラミングの基本は？\", \"データサイエンスとは？\"]\n",
    "    \n",
    "    dataset = []\n",
    "    for i in range(size):\n",
    "        template = random.choice(templates)\n",
    "        topic = random.choice(topics)\n",
    "        question = random.choice(questions)\n",
    "        \n",
    "        if \"{question}\" in template[\"prompt\"]:\n",
    "            prompt = template[\"prompt\"].format(question=question)\n",
    "            chosen = template[\"chosen\"].format(\n",
    "                topic=topic,\n",
    "                detail=f\"{topic}の重要な概念\",\n",
    "                benefit=\"効率的な問題解決\"\n",
    "            )\n",
    "        elif \"{text}\" in template[\"prompt\"]:\n",
    "            prompt = template[\"prompt\"].format(text=f\"{topic}に関する詳細な説明\")\n",
    "            chosen = template[\"chosen\"].format(\n",
    "                point=f\"{topic}の活用\",\n",
    "                detail=\"実用的な応用例\"\n",
    "            )\n",
    "        else:\n",
    "            prompt = template[\"prompt\"].format(topic=topic)\n",
    "            chosen = template[\"chosen\"].format(\n",
    "                topic=topic,\n",
    "                definition=\"重要な技術分野\",\n",
    "                feature=\"高い効率性\",\n",
    "                usage=\"様々な業界\"\n",
    "            )\n",
    "        \n",
    "        dataset.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": chosen,\n",
    "            \"rejected\": template[\"rejected\"]\n",
    "        })\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# データセット生成\n",
    "print(\"🔄 Generating DPO dataset...\")\n",
    "data = create_dpo_dataset(1000)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"✅ Generated {len(df)} samples\")\n",
    "print(f\"📊 Columns: {list(df.columns)}\")\n",
    "print(\"\\n🔍 Sample data:\")\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70450792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🤖 モデルとトークナイザー設定\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Colab GPU環境に最適化された軽量モデル\n",
    "model_name = \"microsoft/DialoGPT-small\"\n",
    "\n",
    "print(f\"📥 Loading model: {model_name}\")\n",
    "\n",
    "# トークナイザー読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# モデル読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"✅ Model and tokenizer loaded\")\n",
    "print(f\"📊 Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca373a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚡ LoRA設定（効率的な微調整）\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,  # Low rank for efficiency\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# LoRAモデル作成\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🔥 GPU memory usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 データ前処理\n",
    "from datasets import Dataset\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "# Dataset変換\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset_split['train']\n",
    "eval_dataset = dataset_split['test']\n",
    "\n",
    "print(f\"🎯 Training samples: {len(train_dataset)}\")\n",
    "print(f\"🎯 Evaluation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# サンプル確認\n",
    "print(\"\\n📝 Sample training data:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdeaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 DPOトレーニング設定と実行\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"./dpo_results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-6,\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    max_length=128,\n",
    "    max_prompt_length=64,\n",
    "    report_to=[\"none\"]  # Disable wandb\n",
    ")\n",
    "\n",
    "# DPOトレーナー初期化\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    beta=0.1\n",
    ")\n",
    "\n",
    "print(\"⚙️ DPO Trainer initialized\")\n",
    "print(\"🔥 Starting training... This may take 10-15 minutes\")\n",
    "\n",
    "# トレーニング実行\n",
    "train_result = dpo_trainer.train()\n",
    "\n",
    "print(\"\\n🎉 Training completed!\")\n",
    "print(f\"📊 Final training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 結果の評価と可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 最終評価\n",
    "eval_results = dpo_trainer.evaluate()\n",
    "print(\"📊 Final Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# トレーニング履歴の可視化\n",
    "if hasattr(dpo_trainer.state, 'log_history'):\n",
    "    log_history = dpo_trainer.state.log_history\n",
    "    \n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    steps = []\n",
    "    \n",
    "    for log in log_history:\n",
    "        if 'loss' in log:\n",
    "            train_losses.append(log['loss'])\n",
    "            steps.append(log.get('step', len(train_losses)))\n",
    "        if 'eval_loss' in log:\n",
    "            eval_losses.append(log['eval_loss'])\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(steps, train_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "    plt.title('Training Loss Progress', fontsize=14)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if eval_losses:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        eval_steps = np.linspace(0, max(steps), len(eval_losses))\n",
    "        plt.plot(eval_steps, eval_losses, 'r-', linewidth=2, label='Evaluation Loss')\n",
    "        plt.title('Evaluation Loss Progress', fontsize=14)\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965113a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 トレーニング済みモデルのテスト\n",
    "def generate_response(prompt, max_length=100):\n",
    "    \"\"\"DPOトレーニング済みモデルでレスポンス生成\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # プロンプト部分を除去\n",
    "    if response.startswith(prompt):\n",
    "        response = response[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# テスト実行\n",
    "test_prompts = [\n",
    "    \"以下の質問に答えてください: 機械学習とは何ですか？\",\n",
    "    \"次の文章を要約してください: プログラミングに関する詳細な説明\",\n",
    "    \"プログラミングについて説明してください: Python\"\n",
    "]\n",
    "\n",
    "print(\"🔬 Testing DPO-trained model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"\\n📝 Test {i}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fccb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 モデル保存とダウンロード\n",
    "print(\"💾 Saving trained model...\")\n",
    "\n",
    "# モデル保存\n",
    "dpo_trainer.save_model(\"./final_dpo_model\")\n",
    "tokenizer.save_pretrained(\"./final_dpo_model\")\n",
    "\n",
    "# ファイルサイズ確認\n",
    "import os\n",
    "def get_folder_size(folder_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size\n",
    "\n",
    "model_size = get_folder_size('./final_dpo_model')\n",
    "print(f\"📊 Model size: {model_size / 1e6:.1f} MB\")\n",
    "\n",
    "# Google Driveに保存（オプション）\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    import shutil\n",
    "    drive_path = '/content/drive/MyDrive/dpo_trained_model'\n",
    "    shutil.copytree('./final_dpo_model', drive_path, dirs_exist_ok=True)\n",
    "    print(f\"☁️ Model saved to Google Drive: {drive_path}\")\n",
    "\n",
    "# ZIP形式でダウンロード準備\n",
    "!zip -r dpo_model.zip ./final_dpo_model\n",
    "\n",
    "print(\"\\n🎉 DPO Training Successfully Completed!\")\n",
    "print(\"\\n📋 Summary:\")\n",
    "print(f\"   • Model: {model_name}\")\n",
    "print(f\"   • Training samples: {len(train_dataset)}\")\n",
    "print(f\"   • Evaluation samples: {len(eval_dataset)}\")\n",
    "print(f\"   • Final training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   • Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   • Peak GPU memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\n📥 To download the model:\")\n",
    "print(\"   1. Download 'dpo_model.zip' from the file browser\")\n",
    "print(\"   2. Or access from Google Drive if mounted\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
