#!/opt/miniconda3/bin/python
"""
Quick model test
"""

import sys
import torch

print("Testing basic imports...")

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    print("‚úÖ Transformers imported successfully")
except ImportError as e:
    print(f"‚ùå Failed to import transformers: {e}")
    sys.exit(1)

print(f"PyTorch version: {torch.__version__}")
print(f"MPS available: {torch.backends.mps.is_available()}")

# Test model loading
model_name = "microsoft/Phi-3-mini-4k-instruct"
print(f"\nTesting model: {model_name}")

try:
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    print("‚úÖ Tokenizer loaded")
    
    print("Loading model...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        trust_remote_code=True,
        device_map="auto"
    )
    print("‚úÖ Model loaded")
    print(f"Model parameters: {model.num_parameters():,}")
    
except Exception as e:
    print(f"‚ùå Error loading model: {e}")
    
    # Try alternative model
    print("\nTrying alternative model: google/gemma-2-2b-it")
    try:
        tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b-it")
        model = AutoModelForCausalLM.from_pretrained(
            "google/gemma-2-2b-it",
            torch_dtype=torch.float16,
            device_map="auto"
        )
        print("‚úÖ Alternative model loaded successfully")
        print(f"Model parameters: {model.num_parameters():,}")
        
        # Update config recommendation
        print("\nüìù Recommendation: Use google/gemma-2-2b-it model")
        
    except Exception as e2:
        print(f"‚ùå Alternative model also failed: {e2}")
        
        # Try Qwen2.5
        print("\nTrying Qwen2.5-1.5B-Instruct...")
        try:
            tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
            model = AutoModelForCausalLM.from_pretrained(
                "Qwen/Qwen2.5-1.5B-Instruct",
                torch_dtype=torch.float16,
                device_map="auto"
            )
            print("‚úÖ Qwen2.5 model loaded successfully")
            print(f"Model parameters: {model.num_parameters():,}")
            print("\nüìù Recommendation: Use Qwen/Qwen2.5-1.5B-Instruct model")
            
        except Exception as e3:
            print(f"‚ùå Qwen2.5 also failed: {e3}")
            print("\nüí° Consider using a more basic model like 'distilgpt2' for testing")

print("\nTest completed!")
