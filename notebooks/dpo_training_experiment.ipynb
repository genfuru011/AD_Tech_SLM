{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AD_Tech_SLM - DPO Training Notebook\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€åºƒå‘Šç‰¹åŒ–å‹SLMã®DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªç’°å¢ƒã§ã™ã€‚\n",
    "\n",
    "## ç’°å¢ƒ\n",
    "- MacBook Air M2 8GB\n",
    "- Metal Performance Shaders (MPS) GPU\n",
    "- DPO (Direct Preference Optimization) æ‰‹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ \n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"ğŸš€ Metal Performance Shaders (MPS) is available!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"âš ï¸ MPS not available, using CPU\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿\n",
    "dataset_path = project_root / \"data\" / \"sample_dpo_dataset.jsonl\"\n",
    "\n",
    "data = []\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(df)}\")\n",
    "print(f\"ã‚«ãƒ©ãƒ : {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±\n",
    "print(\"=== ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ ===\")\n",
    "print(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¹³å‡æ–‡å­—æ•°: {df['prompt'].str.len().mean():.1f}\")\n",
    "print(f\"chosenã®å¹³å‡æ–‡å­—æ•°: {df['chosen'].str.len().mean():.1f}\")\n",
    "print(f\"rejectedã®å¹³å‡æ–‡å­—æ•°: {df['rejected'].str.len().mean():.1f}\")\n",
    "\n",
    "print(\"\\n=== ã‚µãƒ³ãƒ—ãƒ«ä¾‹ ===\")\n",
    "sample = df.iloc[0]\n",
    "print(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {sample['prompt']}\")\n",
    "print(f\"Chosen: {sample['chosen']}\")\n",
    "print(f\"Rejected: {sample['rejected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«åï¼ˆM2 8GBã«é©ã—ãŸã‚µã‚¤ã‚ºï¼‰\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_name}\")\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if torch.backends.mps.is_available() else None,\n",
    ")\n",
    "\n",
    "print(\"âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LoRAè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRAè¨­å®š\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "# LoRAãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"âœ… LoRAè¨­å®šå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# HuggingFace Datasetã«å¤‰æ›\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "eval_dataset = dataset.select(range(train_size, len(dataset)))\n",
    "\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "print(f\"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(eval_dataset)} ã‚µãƒ³ãƒ—ãƒ«\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ç°¡å˜ãªæ¨è«–ãƒ†ã‚¹ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ad_copy(prompt, max_length=150):\n",
    "    \"\"\"åºƒå‘Šã‚³ãƒ”ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.backends.mps.is_available():\n",
    "        inputs = inputs.to(\"mps\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»\n",
    "    if generated_text.startswith(prompt):\n",
    "        generated_text = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "test_prompt = \"ã€ãƒ†ãƒ¼ãƒã€‘é›¨ã®æ—¥ã§ã‚‚ãƒ¯ã‚¯ãƒ¯ã‚¯ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ãƒ—ãƒªã‚’ç´¹ä»‹ã—ã¦ãã ã•ã„\"\n",
    "result = generate_ad_copy(test_prompt)\n",
    "\n",
    "print(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {test_prompt}\")\n",
    "print(f\"ç”Ÿæˆçµæœ: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DPO ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æº–å‚™\n",
    "\n",
    "å®Ÿéš›ã®DPOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ `scripts/train_dpo.py` ã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "\n",
    "ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š\n",
    "\n",
    "```bash\n",
    "cd /path/to/AD_Tech_SLM\n",
    "python scripts/train_dpo.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡\n",
    "\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§è©•ä¾¡ã™ã‚‹å ´åˆ\n",
    "# ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†å¾Œã«å®Ÿè¡Œ\n",
    "\n",
    "test_prompts = [\n",
    "    \"ã€ãƒ†ãƒ¼ãƒã€‘å¥åº·ç®¡ç†ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ã‚¢ãƒ—ãƒªã‚’ç´¹ä»‹ã—ã¦ãã ã•ã„\",\n",
    "    \"ã€ãƒ†ãƒ¼ãƒã€‘æ–™ç†åˆå¿ƒè€…å‘ã‘ã®ãƒ¬ã‚·ãƒ”ã‚¢ãƒ—ãƒªã‚’ç´¹ä»‹ã—ã¦ãã ã•ã„\",\n",
    "    \"ã€ãƒ†ãƒ¼ãƒã€‘èª­æ›¸å¥½ãã®ãŸã‚ã®é›»å­æ›¸ç±ã‚¢ãƒ—ãƒªã‚’ç´¹ä»‹ã—ã¦ãã ã•ã„\",\n",
    "]\n",
    "\n",
    "print(\"=== ç”Ÿæˆãƒ†ã‚¹ãƒˆ ===\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    result = generate_ad_copy(prompt)\n",
    "    print(f\"\\nãƒ†ã‚¹ãƒˆ {i}:\")\n",
    "    print(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\")\n",
    "    print(f\"ç”Ÿæˆçµæœ: {result}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}