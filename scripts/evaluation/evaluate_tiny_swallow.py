#!/usr/bin/env python3
"""
TinySwallow DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
åºƒå‘ŠæŠ€è¡“ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã®æ€§èƒ½è©•ä¾¡
"""

import os
import json
import logging
import torch
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import pandas as pd
from typing import List, Dict, Any

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class TinySwallowEvaluator:
    """TinySwallow DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_model_name: str, trained_model_path: str, device: str = "auto"):
        """
        Args:
            base_model_name: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å
            trained_model_path: DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
            device: ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹
        """
        self.base_model_name = base_model_name
        self.trained_model_path = trained_model_path
        self.device = device
        
        # M1/M2 Macå¯¾å¿œ
        if device == "auto":
            if torch.backends.mps.is_available():
                self.device = "mps"
                logger.info("ğŸ”§ MPS (Metal Performance Shaders) ã‚’ä½¿ç”¨")
            elif torch.cuda.is_available():
                self.device = "cuda"
                logger.info("ğŸ”§ CUDA ã‚’ä½¿ç”¨")
            else:
                self.device = "cpu"
                logger.info("ğŸ”§ CPU ã‚’ä½¿ç”¨")
        
        self.tokenizer = None
        self.model = None
        
    def load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿"""
        logger.info(f"ğŸ“¥ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {self.base_model_name}")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_name,
            trust_remote_code=True
        )
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        model_kwargs = {
            "trust_remote_code": True,
            "torch_dtype": torch.float16 if self.device != "mps" else torch.float32,
        }
        
        if self.device == "mps":
            # MPSç’°å¢ƒã§ã¯ç‰¹åˆ¥ãªè¨­å®š
            model_kwargs["torch_dtype"] = torch.float32
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            **model_kwargs
        )
        
        # DPOè¨“ç·´æ¸ˆã¿ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’èª­ã¿è¾¼ã¿
        if os.path.exists(self.trained_model_path):
            logger.info(f"ğŸ“¥ DPOè¨“ç·´æ¸ˆã¿ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­: {self.trained_model_path}")
            self.model = PeftModel.from_pretrained(self.model, self.trained_model_path)
            self.model = self.model.merge_and_unload()  # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒãƒ¼ã‚¸
        else:
            logger.warning(f"âš ï¸ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.trained_model_path}")
            logger.info("ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦è©•ä¾¡ã‚’ç¶šè¡Œã—ã¾ã™")
        
        # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        self.model.to(self.device)
        self.model.eval()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¡¨ç¤º
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"ğŸ“Š ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
        
        logger.info("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    
    def generate_response(self, prompt: str, max_length: int = 512, **kwargs) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆ"""
        if not self.model or not self.tokenizer:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆè¨­å®š
        generation_kwargs = {
            "max_length": max_length,
            "num_return_sequences": 1,
            "do_sample": True,
            "temperature": 0.7,
            "top_p": 0.9,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            **kwargs
        }
        
        # å¿œç­”ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                **generation_kwargs
            )
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        response = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        return response.strip()
    
    def evaluate_on_test_prompts(self, test_prompts: List[str]) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®è©•ä¾¡"""
        results = []
        
        logger.info(f"ğŸ“Š {len(test_prompts)}å€‹ã®ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§è©•ä¾¡é–‹å§‹")
        
        for i, prompt in enumerate(test_prompts):
            logger.info(f"ğŸ”„ è©•ä¾¡ä¸­ ({i+1}/{len(test_prompts)}): {prompt[:50]}...")
            
            try:
                response = self.generate_response(prompt)
                results.append({
                    "prompt": prompt,
                    "response": response,
                    "success": True,
                    "error": None
                })
            except Exception as e:
                logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
                results.append({
                    "prompt": prompt,
                    "response": "",
                    "success": False,
                    "error": str(e)
                })
        
        logger.info("âœ… è©•ä¾¡å®Œäº†")
        return results
    
    def save_evaluation_results(self, results: List[Dict[str, Any]], output_path: str):
        """è©•ä¾¡çµæœã‚’ä¿å­˜"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # JSONå½¢å¼ã§ä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ è©•ä¾¡çµæœã‚’ä¿å­˜: {output_path}")
        
        # çµ±è¨ˆæƒ…å ±ã‚‚ä¿å­˜
        stats_path = output_path.replace('.json', '_stats.txt')
        with open(stats_path, 'w', encoding='utf-8') as f:
            total = len(results)
            success = sum(1 for r in results if r['success'])
            failure = total - success
            
            f.write(f"=== TinySwallow DPO è©•ä¾¡çµ±è¨ˆ ===\n")
            f.write(f"è©•ä¾¡æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ç·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {total}\n")
            f.write(f"æˆåŠŸ: {success} ({success/total*100:.1f}%)\n")
            f.write(f"å¤±æ•—: {failure} ({failure/total*100:.1f}%)\n")
            f.write(f"\n=== ã‚µãƒ³ãƒ—ãƒ«å¿œç­” ===\n")
            
            for i, result in enumerate(results[:3]):  # æœ€åˆã®3ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º
                if result['success']:
                    f.write(f"\n--- ã‚µãƒ³ãƒ—ãƒ« {i+1} ---\n")
                    f.write(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {result['prompt']}\n")
                    f.write(f"å¿œç­”: {result['response']}\n")
        
        logger.info(f"ğŸ“Š çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜: {stats_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³è©•ä¾¡é–¢æ•°"""
    # è¨­å®š
    base_model_name = "SakanaAI/TinySwallow-1.5B-Instruct"
    trained_model_path = "./outputs/tiny_swallow_dpo"  # æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æŒ‡å®š
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
    checkpoint_dirs = []
    if os.path.exists(trained_model_path):
        for item in os.listdir(trained_model_path):
            if item.startswith("checkpoint-"):
                checkpoint_dirs.append(item)
    
    if checkpoint_dirs:
        # æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨
        latest_checkpoint = sorted(checkpoint_dirs, key=lambda x: int(x.split('-')[1]))[-1]
        trained_model_path = os.path.join(trained_model_path, latest_checkpoint)
        logger.info(f"ğŸ¯ æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨: {latest_checkpoint}")
    
    # åºƒå‘ŠæŠ€è¡“ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    test_prompts = [
        "ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³å˜ä¾¡(CPM)ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®æˆ¦ç•¥ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
        "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å…¥æœ­(RTB)ã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–æœ€é©åŒ–æ‰‹æ³•ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã«ãŠã‘ã‚‹ãƒ–ãƒ©ãƒ³ãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã®é‡è¦æ€§ã«ã¤ã„ã¦è¿°ã¹ã¦ãã ã•ã„ã€‚",
        "ã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æã‚’æ´»ç”¨ã—ãŸåºƒå‘ŠåŠ¹æœæ¸¬å®šã®æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
        "ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ“ãƒ‡ã‚£ãƒ³ã‚°ã¨ã‚¦ã‚©ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«æ–¹å¼ã®é•ã„ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "DSPã¨SSPã®å½¹å‰²ã¨é€£æºã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
        "ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ“ãƒªãƒ†ã‚£æ¸¬å®šã®èª²é¡Œã¨è§£æ±ºç­–ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "ãƒ•ãƒªãƒ¼ã‚¯ã‚¨ãƒ³ã‚·ãƒ¼ã‚­ãƒ£ãƒƒãƒ—ã®è¨­å®šæ–¹æ³•ã¨åŠ¹æœã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
        "Cookieãƒ¬ã‚¹Worldå¯¾å¿œã®ãŸã‚ã®æº–å‚™ã«ã¤ã„ã¦è¿°ã¹ã¦ãã ã•ã„ã€‚"
    ]
    
    # è©•ä¾¡å™¨åˆæœŸåŒ–
    evaluator = TinySwallowEvaluator(base_model_name, trained_model_path)
    
    try:
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        evaluator.load_model()
        
        # è©•ä¾¡å®Ÿè¡Œ
        results = evaluator.evaluate_on_test_prompts(test_prompts)
        
        # çµæœä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"./outputs/evaluation/tiny_swallow_evaluation_{timestamp}.json"
        evaluator.save_evaluation_results(results, output_path)
        
        logger.info("ğŸ‰ è©•ä¾¡ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        
    except Exception as e:
        logger.error(f"âŒ è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        raise


if __name__ == "__main__":
    main()
