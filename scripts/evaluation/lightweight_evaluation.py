#!/usr/bin/env python3
"""
TinySwallow DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è»½é‡è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
CPUç’°å¢ƒã§ã®å®‰å…¨ãªè©•ä¾¡
"""

import os
import json
import logging
import torch
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def evaluate_model():
    """è»½é‡ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
    
    base_model_name = "SakanaAI/TinySwallow-1.5B-Instruct"
    trained_model_path = "./outputs/tiny_swallow_dpo/final_model"
    
    # CPUå¼·åˆ¶ä½¿ç”¨
    device = "cpu"
    logger.info("ğŸ”§ CPUç’°å¢ƒã§è©•ä¾¡å®Ÿè¡Œ")
    
    try:
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
        logger.info("ğŸ“¥ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆCPUã€float32ï¼‰
        logger.info("ğŸ“¥ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿...")
        model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float32,
            trust_remote_code=True,
            device_map=None
        )
        
        # DPOã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿
        if os.path.exists(trained_model_path):
            logger.info("ğŸ“¥ DPOè¨“ç·´æ¸ˆã¿ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿...")
            model = PeftModel.from_pretrained(model, trained_model_path)
            model = model.merge_and_unload()
            logger.info("âœ… DPOè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨")
        else:
            logger.warning("âš ï¸ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
        
        model.to(device)
        model.eval()
        
        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        test_prompts = [
            "RTBã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯åºƒå‘Šã®åˆ©ç‚¹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
            "CPMã®æ„å‘³ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        ]
        
        logger.info("ğŸ§ª è©•ä¾¡é–‹å§‹...")
        results = []
        
        for i, prompt in enumerate(test_prompts):
            logger.info(f"ğŸ“ ãƒ†ã‚¹ãƒˆ {i+1}/3: {prompt}")
            
            try:
                # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                inputs = tokenizer(
                    prompt,
                    return_tensors="pt",
                    max_length=256,
                    truncation=True
                )
                
                # ç”Ÿæˆï¼ˆã‚°ãƒªãƒ¼ãƒ‡ã‚£ç”Ÿæˆã§å®‰å®šæ€§å‘ä¸Šï¼‰
                with torch.no_grad():
                    outputs = model.generate(
                        inputs.input_ids,
                        attention_mask=inputs.attention_mask,
                        max_new_tokens=50,  # çŸ­ã„å¿œç­”
                        do_sample=False,  # ã‚°ãƒªãƒ¼ãƒ‡ã‚£ç”Ÿæˆ
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                
                # ãƒ‡ã‚³ãƒ¼ãƒ‰
                response = tokenizer.decode(
                    outputs[0][inputs.input_ids.shape[1]:],
                    skip_special_tokens=True
                )
                
                result = {
                    "prompt": prompt,
                    "response": response.strip(),
                    "success": True
                }
                results.append(result)
                
                print(f"\nğŸ“‹ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
                print(f"ğŸ“ å¿œç­”: {response.strip()}")
                
            except Exception as e:
                logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                results.append({
                    "prompt": prompt,
                    "response": "",
                    "success": False,
                    "error": str(e)
                })
        
        # çµæœä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"outputs/evaluation/final_evaluation_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ è©•ä¾¡çµæœä¿å­˜: {output_file}")
        logger.info("ğŸ‰ è©•ä¾¡å®Œäº†ï¼")
        
        # çµ±è¨ˆ
        success_count = sum(1 for r in results if r['success'])
        print(f"\nğŸ“Š è©•ä¾¡çµ±è¨ˆ: {success_count}/{len(results)} æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"âŒ è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    evaluate_model()
